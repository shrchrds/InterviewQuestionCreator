{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/ML.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n \\n \\n \\n \\n \\n \\nMachine L earning Year ning is a   \\ndeeplear ning. ai pr oject. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n© 2018 Andrew Ng.  All Rights R eserv ed. \\n \\n  \\nPage 2 Machine Learning Yearning-Draft Andrew Ng Deeplearning.AI \\nTable of Contents \\n \\n1 Why Machine Learning Strategy  \\n2 How to use this book to help your team  \\n3 Prerequisites and Notation  \\n4 Scale drives machine learning progress  \\n5 Your development and test sets  \\n6 Your dev and test sets should come from the same distribution  \\n7 How large do the dev/test sets need to be?  \\n8 Establish a single-number evaluation metric for your team to optimize  \\n9 Optimizing and satisficing metrics  \\n10 Having a dev set and metric speeds up iterations  \\n11 When to change dev/test sets and metrics  \\n12 Takeaways: Setting up development and test sets  \\n13 Build your first system quickly, then iterate  \\n14 Error analysis: Look at dev set examples to evaluate ideas  \\n15 Evaluating multiple ideas in parallel during error analysis  \\n16 Cleaning up mislabeled dev and test set examples  \\n17 If you have a large dev set, split it into two subsets, only one of which you look at  \\n18 How big should the Eyeball and Blackbox dev sets be?  \\n19 Takeaways: Basic error analysis  \\n20 Bias and Variance: The two big sources of error  \\n21 Examples of Bias and Variance  \\n22 Comparing to the optimal error rate  \\n23 Addressing Bias and Variance  \\n24 Bias vs. Variance tradeoff  \\n25 Techniques for reducing avoidable bias  \\nPage 3 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n27 Techniques for reducing variance  \\n28 Diagnosing bias and variance: Learning curves  \\n29 Plotting training error  \\n30 Interpreting learning curves: High bias  \\n31 Interpreting learning curves: Other cases  \\n32 Plotting learning curves  \\n33 Why we compare to human-level performance  \\n34 How to define human-level performance  \\n35 Surpassing human-level performance  \\n36 When you should train and test on different distributions  \\n37 How to decide whether to use all your data  \\n38 How to decide whether to include inconsistent data  \\n39 Weighting data  \\n40 Generalizing from the training set to the dev set  \\n41 Identifying Bias, Variance, and Data Mismatch Errors  \\n42 Addressing data mismatch  \\n43 Artificial data synthesis  \\n44 The Optimization Verification test  \\n45 General form of Optimization Verification test  \\n46 Reinforcement learning example  \\n47 The rise of end-to-end learning  \\n48 More end-to-end learning examples  \\n49 Pros and cons of end-to-end learning  \\n50 Choosing pipeline components: Data availability  \\n51 Choosing pipeline components: Task simplicity  \\nPage 4 Machine Learning Yearning-Draft Andrew Ng  \\n52 Directly learning rich outputs  \\n53 Error analysis by parts  \\n54 Attributing error to one part  \\n55 General case of error attribution  \\n56 Error analysis by parts and comparison to human-level performance  \\n57 Spotting a flawed ML pipeline  \\n58 Building a superhero team - Get your teammates to read this  \\n \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 5 Machine Learning Yearning-Draft Andrew Ng  \\n1 Why Machine Learning Strategy  \\n \\nMachine learning is the foundation of countless important applications, including web  \\nsearch, email anti-spam, speech recognition, product recommendations, and more. I assume  \\nthat you or your team is working on a machine learning application, and that you want to  \\nmake rapid progress. This book will help you do so.   \\nExample:  Building a cat picture startup  \\nSay you’re building a startup that will provide an endless stream of cat pictures to cat lovers.  \\nYou use a neural network to build a computer vision system for detecting cats in pictures.  \\nBut tragically, your learning algorithm’s accuracy is not yet good enough. You are under  \\ntremendous pressure to improve your cat detector. What do you do?   \\nYour team has a lot of ideas, such as:  \\n•Get more data: Collect more pictures of cats.   \\n•Collect a more diverse training set. For example, pictures of cats in unusual positions; cats  \\nwith unusual coloration; pictures shot with a variety of camera settings; ….   \\n•Train the algorithm longer, by running more gradient descent iterations.  \\n•Try a bigger neural network, with more layers/hidden units/parameters.   \\nPage 6 Machine Learning Yearning-Draft Andrew Ng \\n \\n•Try a smaller neural network. \\n•Try adding regularization (such as L2 regularization).  \\n•Change the neural network architecture (activation function, number of hidden units, etc.)  \\n•…  \\nIf you choose well among these possible directions, you’ll build the leading cat picture  \\nplatform, and lead your company to success. If you choose poorly, you might waste months.  \\nHow do you proceed?   \\nThis book will tell you how. Most machine learning problems leave clues that tell you what’s  \\nuseful to try, and what’s not useful to try. Learning to read those clues will save you months  \\nor years of development time.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 7 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n2 How to use this book to help your team  \\n \\nAfter finishing this book, you will have a deep understanding of how to set technical  \\ndirection for a machine learning project.   \\nBut your teammates might not understand why you’re recommending a particular direction.  \\nPerhaps you want your team to define a single-number evaluation metric, but they aren’t  \\nconvinced. How do you persuade them?   \\nThat’s why I made the chapters short: So that you can print them out and get your  \\nteammates to read just the 1-2 pages you need them to know.   \\nA few changes in prioritization can have a huge effect on your team’s productivity. By helping  \\nyour team with a few such changes, I hope that you can become the superhero of your team!   \\n \\n \\n \\xa0\\nPage 8 Machine Learning Yearning-Draft Andrew Ng \\n \\n3 Prerequisites and Notation  \\n \\nIf you have taken a Machine Learning course such as my machine learning MOOC on  \\nCoursera, or if you have experience applying supervised learning, you will be able to  \\nunderstand this text.   \\nI assume you are familiar with \\u200bsupervised learning \\u200b: learning a function that maps from x  \\nto y, using labeled training examples (x,y). Supervised learning algorithms include linear  \\nregression, logistic regression, and neural networks. There are many forms of machine  \\nlearning, but the majority of Machine Learning’s practical value today comes from  \\nsupervised learning.   \\nI will frequently refer to neural networks (also known as “deep learning”). You’ll only need a  \\nbasic understanding of what they are to follow this text.   \\nIf you are not familiar with the concepts mentioned here, watch the first three weeks of  \\nvideos in the Machine Learning course on Coursera at \\u200bhttp://ml-class.org  \\n  \\nPage 9 Machine Learning Yearning-Draft Andrew Ng  \\n4 Scale drives machine learning progress  \\n \\nMany of the ideas of deep learning (neural networks) have been around for decades. Why are  \\nthese ideas taking off now?   \\nTwo of the biggest drivers of recent progress have been:  \\n•Data availability. \\u200b People are now spending more time on digital devices (laptops, mobile  \\ndevices). Their digital activities generate huge amounts of data that we can feed to our  \\nlearning algorithms.  \\n•Computational scale. \\u200bWe started just a few years ago to be able to train neural  \\nnetworks that are big enough to take advantage of the huge datasets we now have.   \\nIn detail, even as you accumulate more data, usually the performance of older learning  \\nalgorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens  \\nout,” and the algorithm stops improving even as you give it more data:   \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIt was as if the older algorithms didn’t know what to do with all the data we now have.   \\nIf you train a small neutral network (NN) on the same supervised learning task, you might  \\nget slightly better performance:   \\n \\n \\n \\nPage 10 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nHere, by “Small NN” we mean a neural network with only a small number of hidden  \\nunits/layers/parameters. Finally, if you train larger and larger neural networks, you can  \\nobtain even better performance:  1\\n \\nThus, you obtain the best performance when you (i) Train a very large neural network, so  \\nthat you are on the green curve above; (ii) Have a huge amount of data.   \\nMany other details such as neural network architecture are also important, and there has  \\nbeen much innovation here. But one of the more reliable ways to improve an algorithm’s  \\nperformance today is still to (i) train a bigger network and (ii) get more data.   \\n1 This diagram shows NNs doing better in the r egime of small datasets.  This effect is less consistent \\nthan the effect of NNs doing well in the r egime of huge datasets.  In the small data regime,  depending  \\non how the features ar e hand-engineer ed, traditional algor ithms may or  may not do better.  For \\nexample,  if you hav e 20 training examples,  it might not matter  much whether you use logist ic \\nregr ession or  a neur al network ; the hand-engineering of featur es will hav e a bigger effect than the  \\nchoice of algorithm.  But if you hav e 1 million examples,  I would fav or the neur al network .  \\nPage 11 Machine Learning Yearning-Draft Andrew Ng  \\nThe process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss  \\nthe details at length. We will start with general strategies that are useful for both traditional  \\nlearning algorithms and neural networks, and build up to the most modern strategies for  \\nbuilding deep learning systems.   \\nPage 12 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSetting up \\ndevelopment and \\ntest sets  \\nPage 13 Machine Learning Yearning-Draft Andrew Ng  \\n5 Your development and test sets  \\n \\nLet’s return to our earlier cat pictures example: You run a mobile app, and users are  \\nuploading pictures of many different things to your app. You want to automatically find the  \\ncat pictures.   \\nYour team gets a large training set by downloading pictures of cats (positive examples) and  \\nnon-cats (negative examples) off of different websites. They split the dataset 70%/30% into  \\ntraining and test sets. Using this data, they build a cat detector that works well on the  \\ntraining and test sets.   \\nBut when you deploy this classifier into the mobile app, you find that the performance is  \\nreally poor!   \\n         \\nWhat happened?  \\nYou figure out that the pictures users are uploading have a different look than the website  \\nimages that make up your training set: Users are uploading pictures taken with mobile  \\nphones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test  \\nsets were made of website images, your algorithm did not generalize well to the actual  \\ndistribution you care about: mobile phone pictures.   \\nBefore the modern era of big data, it was a common rule in machine learning to use a  \\nrandom 70%/30% split to form your training and test sets. This practice can work, but it’s a  \\nbad idea in more and more applications where the training distribution (website images in  \\nPage 14 Machine Learning Yearning-Draft Andrew Ng  \\nour example above) is different from the distribution you ultimately care about (mobile  \\nphone images).  \\nWe usually define:  \\n•Training set \\u200b — Which you run your learning algorithm on.  \\n•Dev (development) set \\u200b — Which you use to tune parameters, select features, and  \\nmake other decisions regarding the learning algorithm. Sometimes also called the  \\nhold-out cross validation set \\u200b.  \\n•Test set\\u200b — which you use to evaluate the performance of the algorithm, but not to make  \\nany decisions regarding what learning algorithm or parameters to use.   \\nOnce you define a dev set (development set) and test set, your team will try a lot of ideas,  \\nsuch as different learning algorithm parameters, to see what works best. The dev and test  \\nsets allow your team to quickly see how well your algorithm is doing.   \\nIn other words, \\u200bthe purpose of the dev and test sets are to direct your team toward  \\nthe most important changes to make to the machine learning system \\u200b.  \\nSo, you should do the following:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nIn other words, your test set should not simply be 30% of the available data, especially if you  \\nexpect your future data (mobile phone images) to be different in nature from your training  \\nset (website images).   \\nIf you have not yet launched your mobile app, you might not have any users yet, and thus  \\nmight not be able to get data that accurately reflects what you have to do well on in the  \\nfuture. But you might still try to approximate this. For example, ask your friends to take  \\nmobile phone pictures of cats and send them to you. Once your app is launched, you can  \\nupdate your dev/test sets using actual user data.   \\nIf you really don’t have any way of getting data that approximates what you expect to get in  \\nthe future, perhaps you can start by using website images. But you should be aware of the  \\nrisk of this leading to a system that doesn’t generalize well.   \\nIt requires judgment to decide how much to invest in developing great dev and test sets. But  \\ndon’t assume your training distribution is the same as your test distribution. Try to pick test  \\nPage 15 Machine Learning Yearning-Draft Andrew Ng  \\nexamples that reflect what you ultimately want to perform well on, rather than whatever data  \\nyou happen to have for training.   \\n \\n  \\nPage 16 Machine Learning Yearning-Draft Andrew Ng  \\n6 Your dev and test sets should come from the  \\nsame distribution  \\nYou have your cat app image data segmented into four regions, based on your largest  \\nmarkets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a dev set and a test  \\nset, say we put US and India in the dev set; China and Other in the test set. In other words,  \\nwe can randomly assign two of these segments to the dev set, and the other two to the test  \\nset, right?  \\nOnce you define the dev and test sets, your team will be focused on improving dev set  \\nperformance. Thus, the dev set should reflect the task you want to improve on the most: To  \\ndo well on all four geographies, and not only two.   \\nThere is a second problem with having different dev and test set distributions: There is a  \\nchance that your team will build something that works well on the dev set, only to find that it  \\ndoes poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid  \\nletting this happen to you.   \\nAs an example, suppose your team develops a system that works well on the dev set but not  \\nthe test set. If your dev and test sets had come from the same distribution, then you would  \\nhave a very clear diagnosis of what went wrong: You have overfit the dev set. The obvious  \\ncure is to get more dev set data.   \\nBut if the dev and test sets come from different distributions, then your options are less  \\nclear. Several things could have gone wrong:   \\n1.You had overfit to the dev set.   \\n2.The test set is harder than the dev set. So your algorithm might be doing as well as could  \\nbe expected, and no further significant improvement is possible.   \\nPage 17 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.The test set is not necessarily harder, but just different, from the dev set. So what works  \\nwell on the dev set just does not work well on the test set. In this case, a lot of your work  \\nto improve dev set performance might be wasted effort.   \\nWorking on machine learning applications is hard enough. Having mismatched dev and test  \\nsets introduces additional uncertainty about whether improving on the dev set distribution  \\nalso improves test set performance. Having mismatched dev and test sets makes it harder to  \\nfigure out what is and isn’t working, and thus makes it harder to prioritize what to work on.   \\nIf you are working on a 3rd party benchmark problem, their creator might have specified dev  \\nand test sets that come from different distributions. Luck, rather than skill, will have a  \\ngreater impact on your performance on such benchmarks compared to if the dev and test  \\nsets come from the same distribution. It is an important research problem to develop  \\nlearning algorithms that are trained on one distribution and generalize well to another. But if  \\nyour goal is to make progress on a specific machine learning application rather than make  \\nresearch progress, I  recommend trying to choose dev and test sets that are drawn from the  \\nsame distribution. This will make your team more efficient.   \\n \\n \\n \\n \\xa0\\nPage 18 Machine Learning Yearning-Draft Andrew Ng  \\n7 How large do the dev/test sets need to be?  \\n \\nThe dev set should be large enough to detect differences between algorithms that you are  \\ntrying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an  \\naccuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1%  \\ndifference. Compared to other machine learning problems I’ve seen, a 100 example dev set is  \\nsmall. Dev sets with sizes from 1,000 to 10,000 examples are common. With 10,000  \\nexamples, you will have a good chance of detecting an improvement of 0.1%.  2\\nFor mature and important applications—for example, advertising, web search, and product  \\nrecommendations—I have also seen teams that are highly motivated to eke out even a 0.01%  \\nimprovement, since it has a direct impact on the company’s profits. In this case, the dev set  \\ncould be much larger than 10,000, in order to detect even smaller improvements.   \\nHow about the size of the test set? It should be large enough to give high confidence in the  \\noverall performance of your system. One popular heuristic had been to use 30% of your data  \\nfor your test set. This works well when you have a modest number of examples—say 100 to  \\n10,000 examples. But in the era of big data where we now have machine learning problems  \\nwith sometimes more than a billion examples, the fraction of data allocated to dev/test sets  \\nhas been shrinking, even as the absolute number of examples in the dev/test sets has been  \\ngrowing. There is no need to have excessively large dev/test sets beyond what is needed to  \\nevaluate the performance of your algorithms.   \\n2 In theory,  one could also test if a change to an algorithm mak es a statistically significant difference \\non the dev  set. In pr actice,  most teams don’t bother  with this (unless they are publishing academic  \\nresearch papers),  and I usually do not find statistical significance tests useful for measuring inter im \\nprogr ess.  \\nPage 19 Machine Learning Yearning-Draft Andrew Ng  \\n8 Establish a single-number evaluation metric  \\nfor your team to optimize   \\n \\nClassification accuracy is an example of a \\u200bsingle-number evaluation metric \\u200b: You run \\nyour classifier on the dev set (or test set), and get back a single number about what fraction  \\nof examples it classified correctly. According to this metric, if classifier A obtains 97%  \\naccuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.  \\nIn contrast, Precision and Recall  is not a single-number evaluation metric: It gives two  3\\nnumbers for assessing your classifier. Having multiple-number evaluation metrics makes it  \\nharder to compare algorithms. Suppose your algorithms perform as follows:   \\n \\nClassifier \\xa0 Precision \\xa0 Recall\\xa0\\nA\\xa0 95% 90% \\nB\\xa0 98% 85% \\n \\nHere, neither classifier is obviously superior, so it doesn’t immediately guide you toward  \\npicking one. \\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\n \\nDuring development, your team will try a lot of ideas about algorithm architecture, model  \\nparameters, choice of features, etc. Having a \\u200bsingle-number evaluation metric \\u200b such as  \\naccuracy allows you to sort all your models according to their performance on this metric,  \\nand quickly decide what is working best.   \\nIf you really care about both Precision and Recall, I recommend using one of the standard  \\nways to combine them into a single number. For example, one could take the average of  \\nprecision and recall, to end up with a single number.  Alternatively, you can compute the “F1  \\n3 The Precision of a cat classifier  is the fraction of images in the dev  (or test) set it labeled as cats that \\nreally are cats.  Its Recall is the percentage of all cat images in the dev  (or test) set that it cor rectly  \\nlabeled as a cat.  There is often a tradeoff between hav ing high pr ecision and high recall.   \\nPage 20 Machine Learning Yearning-Draft Andrew Ng  \\nscore,” which is a modified way of computing their average, and works better than simply  \\ntaking the mean. 4\\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\nB\\xa0 98% 85% 91.0%  \\n \\nHaving a single-number evaluation metric speeds up your ability to make a decision when  \\nyou are selecting among a large number of classifiers. It gives a clear preference ranking  \\namong all of them, and therefore a clear direction for progress.   \\nAs a final example, suppose you are separately tracking the accuracy of your cat classifier in  \\nfour key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By  \\ntaking an average or weighted average of these four numbers, you end up with a single  \\nnumber metric. Taking an average or weighted average is one of the most common ways to  \\ncombine multiple metrics into one.   \\n \\n \\n \\xa0\\n4 If you want to learn more about the F1  scor e, see \\u200bhttps: //en. wikipedia. org/wik i/F1_score\\u200b.  It is the  \\n“harmonic mean” between Precision and R ecall,  and is calculated as 2 /((1/Pr ecision)+(1/R ecall)).   \\nPage 21 Machine Learning Yearning-Draft Andrew Ng  \\n9 Optimizing and satisficing metrics   \\n \\nHere’s another way to combine multiple evaluation metrics.   \\nSuppose you care about both the accuracy and the running time of a learning algorithm. You  \\nneed to choose from these three classifiers:   \\nClassifier \\xa0 Accuracy \\xa0 Running time\\xa0\\nA\\xa0 90%\\xa0 80ms\\xa0\\nB\\xa0 92% 95ms\\xa0\\nC\\xa0 95% 1,500ms \\xa0\\n \\nIt seems unnatural to derive a single metric by putting accuracy and running time into a  \\nsingle formula, such as:  \\nAccuracy - 0.5*RunningTime \\nHere’s what you can do instead: First, define what is an “acceptable” running time. Lets say  \\nanything that runs in 100ms is acceptable. Then, maximize accuracy, subject to your  \\nclassifier meeting the running time criteria. Here, running time is a “satisficing  \\nmetric”—your classifier just has to be “good enough” on this metric, in the sense that it  \\nshould take at most 100ms. Accuracy is the “optimizing metric.”  \\nIf you are trading off N different criteria, such as binary file size of the model (which is  \\nimportant for mobile apps, since users don’t want to download large apps), running time,  \\nand accuracy, you might consider setting N-1 of the criteria as “satisficing” metrics. I.e., you  \\nsimply require that they meet a certain value. Then define the final one as the “optimizing”  \\nmetric. For example, set a threshold for what is acceptable for binary file size and running  \\ntime, and try to optimize accuracy given those constraints.  \\nAs a final example, suppose you are building a hardware device that uses a microphone to  \\nlisten for the user saying a particular “wakeword,” that then causes the system to wake up.  \\nExamples include Amazon Echo listening for “Alexa”; Apple Siri listening for “Hey Siri”;  \\nAndroid listening for “Okay Google”; and Baidu apps listening for “Hello Baidu.” You care  \\nabout both the false positive rate—the frequency with which the system wakes up even when  \\nno one said the wakeword—as well as the false negative rate—how often it fails to wake up  \\nwhen someone says the wakeword. One reasonable goal for the performance of this system is  \\nPage 22 Machine Learning Yearning-Draft Andrew Ng  \\nto minimize the false negative rate (optimizing metric), subject to there being no more than  \\none false positive every 24 hours of operation (satisficing metric).   \\nOnce your team is aligned on the evaluation metric to optimize, they will be able to make  \\nfaster progress.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 23 Machine Learning Yearning-Draft Andrew Ng  \\n10 Having a dev set and metric speeds up  \\niterations \\n \\nIt is very difficult to know in advance what approach will work best for a new problem. Even  \\nexperienced machine learning researchers will usually try out many dozens of ideas before  \\nthey discover something satisfactory. When building a machine learning system, I will often:   \\n1.Start off with some \\u200bidea\\u200b on how to build the system.   \\n2.Implement the idea in \\u200bcode\\u200b.  \\n3.Carry out an \\u200bexperiment \\u200b which tells me how well the idea worked. (Usually my first few  \\nideas don’t work!) Based on these learnings, go back to generate more ideas, and keep on  \\niterating.  \\nThis is an iterative process. The faster you can go round this loop, the faster you will make  \\nprogress. This is why having dev/test sets and a metric are important: Each time you try an  \\nidea, measuring your idea’s performance on the dev set lets you quickly decide if you’re  \\nheading in the right direction.   \\nIn contrast, suppose you don’t have a specific dev set and metric. So each time your team  \\ndevelops a new cat classifier, you have to incorporate it into your app, and play with the app  \\nfor a few hours to get a sense of whether the new classifier is an improvement. This would be  \\nincredibly slow! Also, if your team improves the classifier’s accuracy from 95.0% to 95.1%,  \\nyou might not be able to detect that 0.1% improvement from playing with the app. Yet a lot  \\nof progress in your system will be made by gradually accumulating dozens of these 0.1%  \\nimprovements. Having a dev set and metric allows you to very quickly detect which ideas are  \\nsuccessfully giving you small (or large) improvements, and therefore lets you quickly decide  \\nwhat ideas to keep refining, and which ones to discard.   \\n \\xa0\\nPage 24 Machine Learning Yearning-Draft Andrew Ng \\n \\n11 When to change dev/test sets and metrics   \\n \\nWhen starting out on a new project, I try to quickly choose dev/test sets, since this gives the  \\nteam a well-defined target to aim for.   \\nI typically ask my teams to come up with an initial dev/test set and an initial metric in less  \\nthan one week—rarely longer. It is better to come up with something imperfect and get going  \\nquickly, rather than overthink this. But this one week timeline does not apply to mature  \\napplications. For example, anti-spam is a mature deep learning application. I have seen  \\nteams working on already-mature systems spend months to acquire even better dev/test  \\nsets.  \\nIf you later realize that your initial dev/test set or metric missed the mark, by all means  \\nchange them quickly. For example, if your dev set + metric ranks classifier A above classifier  \\nB, but your team thinks that classifier B is actually superior for your product, then this might  \\nbe a sign that you need to change your dev/test sets or your evaluation metric.   \\nThere are three main possible causes of the dev set/metric incorrectly rating classifier A  \\nhigher:  \\n1. The actual distribution you need to do well on is different from the dev/test sets.  \\nSuppose your initial dev/test set had mainly pictures of adult cats. You ship your cat app,  \\nand find that users are uploading a lot more kitten images than expected. So, the dev/test set  \\ndistribution is not representative of the actual distribution you need to do well on. In this  \\ncase, update your dev/test sets to be more representative.   \\n \\nPage 25 Machine Learning Yearning-Draft Andrew Ng \\n \\n2. You have overfit to the dev set.  \\nThe process of repeatedly evaluating ideas on the dev set causes your algorithm to gradually  \\n“overfit” to the dev set. When you are done developing, you will evaluate your system on the  \\ntest set. If you find that your dev set performance is much better than your test set  \\nperformance, it is a sign that you have overfit to the dev set. In this case, get a fresh dev set.   \\nIf you need to track your team’s progress, you can also evaluate your system regularly—say  \\nonce per week or once per month—on the test set. But do not use the test set to make any  \\ndecisions regarding the algorithm, including whether to roll back to the previous week’s  \\nsystem. If you do so, you will start to overfit to the test set, and can no longer count on it to  \\ngive a completely unbiased estimate of your system’s performance (which you would need if  \\nyou’re publishing research papers, or perhaps using this metric to make important business  \\ndecisions).  \\n3. The metric is measuring something other than what the project needs to optimize.  \\nSuppose that for your cat application, your metric is classification accuracy. This metric  \\ncurrently ranks classifier A as superior to classifier B. But suppose you try out both  \\nalgorithms, and find classifier A is allowing occasional pornographic images to slip through.  \\nEven though classifier A is more accurate, the bad impression left by the occasional  \\npornographic image means its performance is unacceptable. What do you do?   \\nHere, the metric is failing to identify the fact that Algorithm B is in fact better than  \\nAlgorithm A for your product. So, you can no longer trust the metric to pick the best  \\nalgorithm. It is time to change evaluation metrics. For example, you can change the metric to  \\nheavily penalize letting through pornographic images.  I would strongly recommend picking  \\na new metric and using the new metric to explicitly define a new goal for the team, rather  \\nthan proceeding for too long without a trusted metric and reverting to manually choosing  \\namong classifiers.  \\n \\nIt is quite common to change dev/test sets or evaluation metrics during a project. Having an  \\ninitial dev/test set and metric helps you iterate quickly. If you ever find that the dev/test sets  \\nor metric are no longer pointing your team in the right direction, it’s not a big deal! Just  \\nchange them and make sure your team knows about the new direction.   \\n \\n \\xa0\\nPage 26 Machine Learning Yearning-Draft Andrew Ng  \\n12 Takeaways: Setting up development and  \\ntest sets  \\n \\n•Choose dev and test sets from a distribution that reflects what data you expect to get in  \\nthe future and want to do well on. This may not be the same as your training data’s  \\ndistribution.   \\n•Choose dev and test sets from the same distribution if possible.   \\n•Choose a single-number evaluation metric for your team to optimize. If there are multiple  \\ngoals that you care about, consider combining them into a single formula (such as  \\naveraging multiple error metrics) or defining satisficing and optimizing metrics.   \\n•Machine learning is a highly iterative process: You may try many dozens of ideas before  \\nfinding one that you’re satisfied with.   \\n•Having dev/test sets and a single-number evaluation metric helps you quickly evaluate  \\nalgorithms, and therefore iterate faster.   \\n•When starting out on a brand new application, try to establish dev/test sets and a metric  \\nquickly, say in less than a week. It might be okay to take longer on mature applications.   \\n•The old heuristic of a 70%/30% train/test split does not apply for problems where you  \\nhave a lot of data; the dev and test sets can be much less than 30% of the data.   \\n•Your dev set should be large enough to detect meaningful changes in the accuracy of your  \\nalgorithm, but not necessarily much larger. Your test set should be big enough to give you  \\na confident estimate of the final performance of your system.   \\n•If your dev set and metric are no longer pointing your team in the right direction, quickly  \\nchange them: (i) If you had overfit the dev set, get more dev set data. (ii) If the actual  \\ndistribution you care about is different from the dev/test set distribution, get new  \\ndev/test set data. (iii) If your metric is no longer measuring what is most important to  \\nyou, change the metric.   \\n \\n \\n \\nPage 27 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\nBasic Error \\nAnalysis \\n\\xa0\\n\\xa0 \\xa0\\nPage 28 Machine Learning Yearning-Draft Andrew Ng  \\n13 Build your first system quickly, then iterate  \\n \\nYou want to build a new email anti-spam system. Your team has several ideas:   \\n•Collect a huge training set of spam email. For example, set up a “honeypot”: deliberately  \\nsend fake email addresses to known spammers, so that you can automatically harvest the  \\nspam messages they send to those addresses.  \\n•Develop features for understanding the text content of the email.   \\n•Develop features for understanding the email envelope/header features to show what set  \\nof internet servers the message went through.   \\n•and more.  \\nEven though I have worked extensively on anti-spam, I would still have a hard time picking  \\none of these directions. It is even harder if you are not an expert in the application area.   \\nSo don’t start off trying to design and build the perfect system. Instead, build and train a  \\nbasic system quickly—perhaps in just a few days.  Even if the basic system is far from the  5\\n“best” system you can build, it is valuable to examine how the basic system functions: you  \\nwill  quickly find clues that show you the most promising directions in which to invest your  \\ntime. These next few chapters will show you how to read these clues.  \\n \\n\\xa0\\n\\xa0 \\xa0\\n5 This adv ice is meant for readers wanting to build AI applications,  rather  than those whose goal is to  \\npublish academic papers.  I will later  return to the topic of doing research.   \\nPage 29 Machine Learning Yearning-Draft Andrew Ng  \\n14 Error analysis: Look at dev set examples to  \\nevaluate ideas  \\nWhen you play with your cat app, you notice several examples where it mistakes dogs for  \\ncats. Some dogs do look like cats!   \\nA team member proposes incorporating 3rd party software that will make the system do  \\nbetter on dog images. These changes will take a month, and the team member is  \\nenthusiastic. Should you ask them to go ahead?   \\nBefore investing a month on this task, I recommend that you first estimate how much it will  \\nactually improve the system’s accuracy. Then you can more rationally decide if this is worth  \\nthe month of development time, or if you’re better off using that time on other tasks.   \\nIn detail, here’s what you can do:   \\n1.Gather a sample of 100 dev set examples that your system \\u200bmisclassified \\u200b. I.e., examples  \\nthat your system made an error on.   \\n2.Look at these examples manually, and count what fraction of them are dog images.   \\nThe process of looking at misclassified examples is called \\u200berror analysis \\u200b. In this example, if \\nyou find that only 5% of the misclassified images are dogs, then no matter how much you  \\nimprove your algorithm’s performance on dog images, you won’t get rid of more than 5% of  \\nyour errors. In other words, 5% is a “ceiling” (meaning maximum possible amount) for how  \\nmuch the proposed project could help. Thus, if your overall system is currently 90% accurate  \\n(10% error), this improvement is likely to result in at best 90.5% accuracy (or 9.5% error,  \\nwhich is 5% less error than the original 10% error).   \\nPage 30 Machine Learning Yearning-Draft Andrew Ng \\n \\nIn contrast, if you find that 50% of the mistakes are dogs, then you can be more confident  \\nthat the proposed project will have a big impact. It could boost accuracy from 90% to 95% (a  \\n50% relative reduction in error, from 10% down to 5%).   \\nThis simple counting procedure of error analysis gives you a quick way to estimate the  \\npossible value of incorporating the 3rd party software for dog images. It provides a  \\nquantitative basis on which to decide whether to make this investment.   \\nError analysis can often help you figure out how promising different directions are. I’ve seen  \\nmany engineers reluctant to carry out error analysis. It often feels more exciting to just jump  \\nin and implement some idea, rather than question if the idea is worth the time investment.  \\nThis is a common mistake: It might result in your team spending a month only to realize  \\nafterward that it resulted in little benefit.   \\nManually examining 100 examples does not take long. Even if you take one minute per  \\nimage, you’d be done in under two hours. These two hours could save you a month of wasted  \\neffort.  \\nError Analysis \\u200b refers to the process of examining dev set examples that your algorithm  \\nmisclassified, so that you can understand the underlying causes of the errors. This can help  \\nyou prioritize projects—as in this example—and inspire new directions, which we will discuss  \\nnext. The next few chapters will also present best practices for carrying out error analyses.   \\n \\n \\n  \\nPage 31 Machine Learning Yearning-Draft Andrew Ng  \\n15 Evaluating multiple ideas in parallel during  \\nerror analysis \\n \\nYour team has several ideas for improving the cat detector:  \\n•Fix the problem of your algorithm recognizing \\u200bdogs\\u200b as cats. \\n•Fix the problem of your algorithm recognizing \\u200bgreat cats \\u200b (lions, panthers, etc.) as house  \\ncats (pets).   \\n•Improve the system’s performance on \\u200bblurry\\u200b images.  \\n•… \\nYou can efficiently evaluate all of these ideas in parallel. I usually create a spreadsheet and  \\nfill it out while looking through ~100 misclassified dev set images. I also jot down comments  \\nthat might help me remember specific examples. To illustrate this process, let’s look at a  \\nspreadsheet you might produce with a small dev set of four examples:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments \\xa0\\n1 ✔ \\xa0   Unusual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken at \\xa0\\nzoo on rainy day\\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n% of total \\xa0 25%\\xa0 50%\\xa0 50%\\xa0  \\n \\nImage #3 above has both the Great Cat and the Blurry columns checked. Furthermore,  \\nbecause it is possible for one example to be associated with multiple categories, the  \\npercentages at the bottom may not add up to 100%.   \\nAlthough you may first formulate the categories (Dog, Great cat, Blurry) then categorize the  \\nexamples by hand, in practice, once you start looking through examples, you will probably be  \\ninspired to propose new error categories. For example, say you go through a dozen images  \\nand realize a lot of mistakes occur with Instagram-filtered pictures. You can go back and add  \\na new “Instagram” column to the spreadsheet. Manually looking at examples that the  \\nalgorithm misclassified and asking how/whether you as a human could have labeled the  \\nPage 32 Machine Learning Yearning-Draft Andrew Ng  \\npicture correctly will often inspire you to come up with new categories of errors and  \\nsolutions.  \\nThe most helpful error categories will be ones that you have an idea for improving. For  \\nexample, the Instagram category will be most helpful to add if you have an idea to “undo”  \\nInstagram filters and recover the original image. But you don’t have to restrict yourself only  \\nto error categories you know how to improve; the goal of this process is to build your  \\nintuition about the most promising areas to focus on.   \\nError analysis is an iterative process. Don’t worry if you start off with no categories in mind.  \\nAfter looking at a couple of images, you might come up with a few ideas for error categories.  \\nAfter manually categorizing some images, you might think of  new categories and re-examine  \\nthe images in light of the new categories, and so on.   \\nSuppose you finish carrying out error analysis on 100 misclassified dev set examples and get  \\nthe following:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments\\xa0\\n1 ✔ \\xa0   Usual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken \\xa0\\nat zoo on rainy day \\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n…\\xa0 …\\xa0 …\\xa0 …\\xa0 ...\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0  \\n \\nYou now know that working on a project to address the Dog mistakes can eliminate 8% of  \\nthe errors at most. Working on Great Cat or Blurry image errors could help eliminate more  \\nerrors. Therefore, you might pick one of the two latter categories to focus on. If your team  \\nhas enough people to pursue multiple directions in parallel, you can also ask some engineers  \\nto work on Great Cats and others to work on Blurry images.   \\nError analysis does not produce a rigid mathematical formula that tells you what the highest  \\npriority task should be. You also have to take into account how much progress you expect to  \\nmake on different categories and the amount of work needed to tackle each one.   \\n\\xa0\\n\\xa0\\nPage 33 Machine Learning Yearning-Draft Andrew Ng  \\n16 Cleaning up mislabeled dev and test set  \\nexamples  \\n \\nDuring error analysis, you might notice that some examples in your dev set are mislabeled.  \\nWhen I say “mislabeled” here, I mean that the pictures were already mislabeled by a human  \\nlabeler even before the algorithm encountered it. I.e., the class label in an example \\u200b(x,y)\\u200b has \\nan incorrect value for \\u200by\\u200b. For example, perhaps some pictures that are not cats are mislabeled  \\nas containing a cat, and vice versa. If you suspect the fraction of mislabeled images is  \\nsignificant, add a category to keep track of the fraction of examples mislabeled:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Mislabeled \\xa0 Comments \\xa0\\n…\\xa0     \\n98    ✔ \\xa0 Labeler missed cat \\xa0\\nin background \\xa0\\n99  ✔ \\xa0    \\n100    ✔ \\xa0 Drawing of a cat;\\xa0\\nnot a real cat. \\xa0\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0 6%\\xa0  \\n \\nShould you correct the labels in your dev set? Remember that the goal of the dev set is to  \\nhelp you quickly evaluate algorithms so that you can tell if Algorithm A or B is better. If the  \\nfraction of the dev set that is mislabeled impedes your ability to make these judgments, then  \\nit is worth spending time to fix the mislabeled dev set labels.   \\nFor example, suppose your classifier’s performance is:  \\n•Overall accuracy on dev set.………………. 90% (10% overall error.)  \\n•Errors due to mislabeled examples……. 0.6% (6% of dev set errors.)   \\n•Errors due to other causes………………… 9.4% (94% of dev set errors)  \\nHere, the 0.6% inaccuracy due to mislabeling might not be significant enough relative to the  \\n9.4% of errors you could be improving. There is no harm in manually fixing the mislabeled  \\nimages in the dev set, but it is not crucial to do so: It might be fine not knowing whether your  \\nsystem has 10% or 9.4% overall error. \\nSuppose you keep improving the cat classifier and reach the following performance:   \\nPage 34 Machine Learning Yearning-Draft Andrew Ng  \\n•Overall accuracy on dev set.………………. 98.0% (2.0% overall error.) \\n•Errors due to mislabeled examples……. 0.6%. (30% of dev set errors.)   \\n•Errors due to other causes………………… 1.4% (70% of dev set errors)  \\n30% of your errors are due to the mislabeled dev set images, adding significant error to your  \\nestimates of accuracy. It is now worthwhile to improve the quality of the labels in the dev set.  \\nTackling the mislabeled examples will help you figure out if a classifier’s error is closer to  \\n1.4% or 2%—a significant relative difference.   \\nIt is not uncommon to start off tolerating some mislabeled dev/test set examples, only later  \\nto change your mind as your system improves so that the fraction of mislabeled examples  \\ngrows relative to the total set of errors.   \\nThe last chapter explained how you can improve error categories such as Dog, Great Cat and  \\nBlurry through algorithmic improvements. You have learned in this chapter that you can  \\nwork on the Mislabeled category as well—through improving the data’s labels.   \\nWhatever process you apply to fixing dev set labels, remember to apply it to the test set  \\nlabels too so that your dev and test sets continue to be drawn from the same distribution.  \\nFixing your dev and test sets together would prevent the problem we discussed in Chapter 6,  \\nwhere your team optimizes for dev set performance only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\nto use the result in an academic research paper or need a completely unbiased measure of  \\ntest set accuracy.  \\xa0\\n\\xa0\\n  \\nPage 35 Machine Learning Yearning-Draft Andrew Ng  \\n17 If you have a large dev set, split it into two  \\nsubsets, only one of which you look at   \\n \\nSuppose you have a large dev set of 5,000 examples in which you have a 20% error rate.  \\nThus, your algorithm is misclassifying ~1,000 dev images. It takes a long time to manually  \\nexamine 1,000 images, so we might decide not to use all of them in the error analysis.   \\nIn this case, I would explicitly split the dev set into two subsets, one of which you look at, and  \\none of which you don’t. You will more rapidly overfit the portion that you are manually  \\nlooking at. You can use the portion you are not manually looking at to tune parameters.   \\nLet\\u200b’\\u200bs continue our example above, in which the algorithm is misclassifying 1,000 out of  \\n5,000 dev set examples. Suppose we want to manually examine about 100 errors for error  \\nanalysis (10% of the errors). You should randomly select 10% of the dev set and place that  \\ninto what we’ll call an \\u200bEyeball dev set \\u200b to remind ourselves that we are looking at it with our  \\neyes. (For a project on speech recognition, in which you would be listening to audio clips,  \\nperhaps you would call this set an Ear dev set instead). The Eyeball dev set therefore has 500  \\nexamples, of which we would expect our algorithm to misclassify about 100.   \\nThe second subset of the dev set, called the \\u200bBlackbox dev set \\u200b, will have the remaining  \\n4500 examples. You can use the Blackbox dev set to evaluate classifiers automatically by  \\nmeasuring their error rates. You can also use it to select among algorithms or tune  \\nhyperparameters. However, you should avoid looking at it with your eyes. We use the term  \\n“Blackbox” because we will only use this subset of the data to obtain “Blackbox” evaluations  \\nof classifiers.   \\nPage 36 Machine Learning Yearning-Draft Andrew Ng \\n \\nWhy do we explicitly separate the dev set into Eyeball and Blackbox dev sets? Since you will  \\ngain intuition about the examples in the Eyeball dev set, you will start to overfit the Eyeball  \\ndev set faster. If you see the performance on the Eyeball dev set improving much more  \\nrapidly than the performance on the Blackbox dev set, you have overfit the Eyeball dev set.  \\nIn this case, you might need to discard it and find a new Eyeball dev set by moving more  \\nexamples from the Blackbox dev set into the Eyeball dev set or by acquiring new labeled  \\ndata.   \\nExplicitly splitting your dev set into Eyeball and Blackbox dev sets allows you to tell when  \\nyour manual error analysis process is causing you to overfit the Eyeball portion of your data.   \\n \\n  \\nPage 37 Machine Learning Yearning-Draft Andrew Ng \\n \\n18 How big should the Eyeball and Blackbox  \\ndev sets be?  \\nYour Eyeball dev set should be large enough to give you a sense of your algorithm’s major  \\nerror categories. If you are working on a task that humans do well (such as recognizing cats  \\nin images), here are some rough guidelines:   \\n•An eyeball dev set in which your classifier makes 10 mistakes would be considered very  \\nsmall. With just 10 errors, it’s hard to accurately estimate the impact of different error  \\ncategories. But if you have very little data and cannot afford to put more into the Eyeball  \\ndev set, it \\u200b’\\u200bs better than nothing and will help with project prioritization.   \\n•If your classifier makes ~20 mistakes on eyeball dev examples, you would start to get a  \\nrough sense of the major error sources.   \\n•With ~50 mistakes, you would get a good sense of the major error sources.  \\n•With ~100 mistakes, you would get a very good sense of the major sources of errors. I’ve  \\nseen people manually analyze even more errors—sometimes as many as 500. There is no  \\nharm in this as long as you have enough data.   \\nSay your classifier has a 5% error rate. To make sure you have ~100 misclassified examples  \\nin the Eyeball dev set, the Eyeball dev set would have to have about 2,000 examples (since  \\n0.05*2,000 = 100). The lower your classifier’s error rate, the larger your Eyeball dev set  \\nneeds to be in order to get a large enough set of errors to analyze.   \\nIf you are working on a task that even humans cannot do well, then the exercise of examining  \\nan Eyeball dev set will not be as helpful because it is harder to figure out why the algorithm  \\ndidn’t classify an example correctly. In this case, you might omit having an Eyeball dev set.  \\nWe discuss guidelines for such problems in a later chapter.   \\nPage 38 Machine Learning Yearning-Draft Andrew Ng \\n \\nHow about the Blackbox dev set? We previously said that dev sets of around 1,000-10,000  \\nexamples are common. To refine that statement, a Blackbox dev set of 1,000-10,000  \\nexamples will often give you enough data to tune hyperparameters and select among models,  \\nthough there is little harm in having even more data. A Blackbox dev set of 100 would be  \\nsmall but still useful.   \\nIf you have a small dev set, then you might not have enough data to split into Eyeball and  \\nBlackbox dev sets that are both large enough to serve their purposes. Instead, your entire dev  \\nset might have to be used as the Eyeball dev set—i.e., you would manually examine all the  \\ndev set data.   \\nBetween the Eyeball and Blackbox dev sets, I consider the Eyeball dev set more important  \\n(assuming that you are working on a problem that humans can solve well and that examining  \\nthe examples helps you gain insight). If you only have an Eyeball dev set, you can perform  \\nerror analyses, model selection and hyperparameter tuning all on that set. The downside of  \\nhaving only an Eyeball dev set is that the risk of overfitting the dev set is greater.   \\nIf you have plentiful access to data, then the size of the Eyeball dev set would be determined  \\nmainly by how many examples you have time to manually analyze. For example, I’ve rarely  \\nseen anyone manually analyze more than 1,000 errors.  \\n \\n \\n \\xa0\\nPage 39 Machine Learning Yearning-Draft Andrew Ng \\n \\n19 Takeaways: Basic error analysis  \\n \\n•When you start a new project, especially if it is in an area in which you are not an expert,  \\nit is hard to correctly guess the most promising directions.   \\n•So don’t start off trying to design and build the perfect system. Instead build and train a  \\nbasic system as quickly as possible—perhaps in a few days. Then use error analysis to  \\nhelp you identify the most promising directions and iteratively improve your algorithm  \\nfrom there.  \\n•Carry out error analysis by manually examining ~100 dev set examples the algorithm  \\nmisclassifies and counting the major categories of errors. Use this information to  \\nprioritize what types of errors to work on fixing.   \\n•Consider splitting the dev set into an Eyeball dev set, which you will manually examine,  \\nand a Blackbox dev set, which you will not manually examine. If performance on the  \\nEyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev  \\nset and should consider acquiring more data for it.   \\n•The Eyeball dev set should be big enough so that your algorithm misclassifies enough  \\nexamples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is sufficient  \\nfor many applications.  \\n•If your dev set is not big enough to split this way, just use the entire dev set as an Eyeball  \\ndev set for manual error analysis, model selection, and hyperparameter tuning.   \\n \\n \\n \\n  \\nPage 40 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBias and Variance  \\n \\n \\n \\n \\n \\n \\xa0\\nPage 41 Machine Learning Yearning-Draft Andrew Ng  \\n20 Bias and Variance: The two big sources of  \\nerror  \\n \\nSuppose your training, dev and test sets all come from the same distribution. Then you  \\nshould always try to get more training data, since that can only improve performance, right?  \\nEven though having more data can’t hurt, unfortunately it doesn’t always help as much as  \\nyou might hope. It could be a waste of time to work on getting more data. So, how do you  \\ndecide when to add data, and when not to bother?  \\nThere are two major sources of error in machine learning: bias and variance. Understanding  \\nthem will help you decide whether adding data, as well as other tactics to improve  \\nperformance, are a good use of time.   \\nSuppose you hope to build a cat recognizer that has 5% error. Right now, your training set  \\nhas an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding  \\ntraining data probably won’t help much. You should focus on other changes. Indeed, adding  \\nmore examples to your training set only makes it harder for your algorithm to do well on the  \\ntraining set. (We explain why in a later chapter.)   \\nIf your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error  \\n(95% accuracy), then the first problem to solve is to improve your algorithm \\u200b’\\u200bs performance  \\non your training set. Your dev/test set performance is usually worse than your training set  \\nperformance. So if you are getting 85% accuracy on the examples your algorithm has seen,  \\nthere’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen.   \\nSuppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break  \\nthe 16% error into two components:   \\n•First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of  \\nthis informally as the algorithm’s \\u200bbias\\u200b.  \\n•Second, how much worse the algorithm does on the dev (or test) set than the training set.  \\nIn this example, it does 1% worse on the dev set than the training set. We think of this  \\ninformally as the algorithm’s \\u200bvariance \\u200b. 6\\n6 The field of statistics has mor e for mal definitions of bias and v ariance that we won’ t worr y about.  \\nRoughly,  the bias is the err or rate of your algor ithm on your  training set when you hav e a v ery large  \\ntraining set.  The variance is how much worse you do on the test set compar ed to the tr aining set in  \\nPage 42 Machine Learning Yearning-Draft Andrew Ng  \\nSome changes to a learning algorithm can address the first component of error— \\u200bbias\\u200b—and \\nimprove its performance on the training set. Some changes address the second  \\ncomponent— \\u200bvariance \\u200b—and help it generalize better from the training set to the dev/test  \\nsets.  To select the most promising changes, it is incredibly useful to understand which of  7\\nthese two components of error is more pressing to address.  \\nDeveloping good intuition about Bias and Variance will help you choose effective changes for  \\nyour algorithm.  \\n\\xa0\\n\\xa0  \\nthis setting.  When your err or metr ic is mean sq uared er ror, you can write down formulas specifying \\nthese two q uantities,  and pr ove that T otal Err or = Bias + V ariance.  But for  our purposes of deciding  \\nhow to mak e progress on an ML  problem,  the more informal definition of bias and v ariance giv en \\nhere will suffice.   \\n7 There are also some methods that can simultaneously red uce bias and v ariance,  by mak ing major  \\nchanges to the system architecture.  But these tend to be harder to identify and implement.   \\nPage 43 Machine Learning Yearning-Draft Andrew Ng  \\n21 Examples of Bias and Variance \\n \\nConsider our cat classification task. An “ideal” classifier (such as a human) might achieve  \\nnearly perfect performance in this task.   \\nSuppose your algorithm performs as follows:  \\n•Training error = 1%  \\n•Dev error = 11%  \\nWhat problem does it have? Applying the definitions from the previous chapter, we estimate  \\nthe bias as 1%, and the variance as 10% (=11%-1%). Thus, it has \\u200bhigh variance \\u200b. The \\nclassifier has very low training error, but it is failing to generalize to the dev set. This is also  \\ncalled \\u200boverfitting \\u200b. \\nNow consider this:  \\n•Training error = 15%  \\n•Dev error = 16% \\nWe estimate the bias as 15%, and variance as 1%. This classifier is fitting the training set  \\npoorly with 15% error, but its error on the dev set is barely higher than the training error.  \\nThis classifier therefore has \\u200bhigh bias \\u200b, but low variance. We say that this algorithm is  \\nunderfitting \\u200b. \\nNow, consider this:  \\n•Training error = 15%  \\n•Dev error = 30% \\nWe estimate the bias as 15%, and variance as 15%. This classifier has \\u200bhigh bias and high  \\nvariance \\u200b: It is doing poorly on the training set, and therefore has high bias, and its  \\nperformance on the dev set is even worse, so it also has high variance. The  \\noverfitting/underfitting terminology is hard to apply here since the classifier is  \\nsimultaneously overfitting and underfitting.   \\n \\n \\nPage 44 Machine Learning Yearning-Draft Andrew Ng  \\nFinally, consider this:  \\n•Training error = 0.5%  \\n•Dev error = 1% \\nThis classifier is doing well, as it has low bias and low variance. Congratulations on achieving  \\nthis great performance!  \\xa0\\n  \\nPage 45 Machine Learning Yearning-Draft Andrew Ng  \\n22 Comparing to the optimal error rate  \\n \\nIn our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal”  \\nclassifier—is nearly 0%. A human looking at a picture would be able to recognize if it  \\ncontains a cat almost all the time; thus, we can hope for a machine that would do just as well.   \\nOther problems are harder. For example, suppose that you are building a speech recognition  \\nsystem, and find that 14% of the audio clips have so much background noise or are so  \\nunintelligible that even a human cannot recognize what was said. In this case, even the most  \\n“optimal” speech recognition system might have error around 14%.   \\nSuppose that on this speech recognition problem, your algorithm achieves:   \\n•Training error = 15%  \\n•Dev error = 30% \\nThe training set performance is already close to the optimal error rate of 14%. Thus, there is  \\nnot much room for improvement in terms of bias or in terms of training set performance.  \\nHowever, this algorithm is not generalizing well to the dev set; thus there is ample room for  \\nimprovement in the errors due to variance.   \\nThis example is similar to the third example from the previous chapter, which also had a  \\ntraining error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training  \\nerror of 15% leaves much room for improvement. This suggests bias-reducing changes might  \\nbe fruitful. But if the optimal error rate is 14%, then the same training set performance tells  \\nus that there’s little room for improvement in the classifier’s bias.   \\nFor problems where the optimal error rate is far from zero, here \\u200b’\\u200bs a more detailed  \\nbreakdown of an algorithm \\u200b’\\u200bs error. Continuing with our speech recognition example above,  \\nthe total dev set error of 30% can be broken down as follows (a similar analysis can be  \\napplied to the test set error):   \\n•Optimal error rate (“unavoidable bias”) \\u200b: 14%. Suppose we decide that, even with the  \\nbest possible speech system in the world, we would still suffer 14% error. We can think of  \\nthis as the “unavoidable” part of a learning algorithm \\u200b’\\u200bs bias.   \\nPage 46 Machine Learning Yearning-Draft Andrew Ng  \\n•Avoidable bias \\u200b: 1%. This is calculated as the difference between the training error and  \\nthe optimal error rate.  8\\n•Variance \\u200b: 15%. The difference between the dev error and the training error.   \\nTo relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:   9\\nBias = Optimal error rate (“unavoidable bias”) + Avoidable bias  \\nThe “avoidable bias” reflects how much worse your algorithm performs on the training set  \\nthan the “optimal classifier.”   \\nThe concept of variance remains the same as before. In theory, we can always reduce  \\nvariance to nearly zero by training on a massive training set. Thus, all variance is “avoidable”  \\nwith a sufficiently large dataset, so there is no such thing as “unavoidable variance.”   \\nConsider one more example, where the optimal error rate is 14%, and we have:   \\n•Training error = 15%  \\n•Dev error = 16% \\nWhereas in the previous chapter we called this a high bias classifier, now we would say that  \\nerror from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm  \\nis already doing well, with little room for improvement. It is only 2% worse than the optimal  \\nerror rate.   \\nWe see from these examples that knowing the optimal error rate is helpful for guiding our  \\nnext steps. In statistics, the optimal error rate is also called \\u200bBayes error rate \\u200b, or Bayes \\nrate.  \\nHow do we know what the optimal error rate is? For tasks that humans are reasonably good  \\nat, such as recognizing pictures or transcribing audio clips, you can ask a human to provide  \\nlabels then measure the accuracy of the human labels relative to your training set. This  \\nwould give an estimate of the optimal error rate. If you are working on a problem that even  \\n8 If this number  is negativ e, you are doing better  on the tr aining set than the optimal er ror rate.  This  \\nmeans you are ov erfitting on the training set,  and the algor ithm has ov er-memorized the tr aining set.  \\nYou should focus on v ariance r eduction methods rather than on fur ther  bias reduction methods.  \\n9 These definitions ar e chosen  to conv ey insight on how to impr ove your  lear ning algorithm.  These \\ndefinitions ar e different than how statisticians define Bias and V ariance.  Technically,  what I define \\nhere as “Bias” should be called “Error  we attribute to bias”;  and “Av oidable bias” should be “err or we \\nattribute to the lear ning algorithm’ s bias that is ov er the optimal er ror rate. ”  \\nPage 47 Machine Learning Yearning-Draft Andrew Ng  \\nhumans have a hard time solving (e.g., predicting what movie to recommend, or what ad to  \\nshow to a user) it can be hard to estimate the optimal error rate.   \\nIn the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss  \\nin more detail the process of comparing a learning algorithm’s performance to human-level  \\nperformance.  \\nIn the last few chapters, you learned how to estimate avoidable/unavoidable bias and  \\nvariance by looking at training and dev set error rates. The next chapter will discuss how you  \\ncan use insights from such an analysis to prioritize techniques that reduce bias vs.  \\ntechniques that reduce variance. There are very different techniques that you should apply  \\ndepending on whether your project’s current problem is high (avoidable) bias or high  \\nvariance. Read on!   \\n  \\nPage 48 Machine Learning Yearning-Draft Andrew Ng  \\n23 Addressing Bias and Variance \\n \\nHere is the simplest formula for addressing bias and variance issues:  \\n•If you have high avoidable bias, increase the size of your model (for example, increase the  \\nsize of your neural network by adding layers/neurons).  \\n•If you have high variance, add data to your training set.  \\nIf you are able to increase the neural network size and increase training data without limit, it  \\nis possible to do very well on many learning problems.   \\nIn practice, increasing the size of your model will eventually cause you to run into  \\ncomputational problems because training very large models is slow. You might also exhaust  \\nyour ability to acquire more training data. (Even on the internet, there is only a finite  \\nnumber of cat pictures!)   \\nDifferent model architectures—for example, different neural network architectures—will  \\nhave different amounts of bias/variance for your problem. A lot of recent deep learning  \\nresearch has developed many innovative model architectures. So if you are using neural  \\nnetworks, the academic literature can be a great source of inspiration. There are also many  \\ngreat open-source implementations on github. But the results of trying new architectures are  \\nless predictable than the simple formula of increasing the model size and adding data.   \\nIncreasing the model size generally reduces bias, but it might also increase variance and the  \\nrisk of overfitting. However, this overfitting problem usually arises only when you are not  \\nusing regularization. If you include a well-designed regularization method, then you can  \\nusually safely increase the size of the model without increasing overfitting.   \\nSuppose you are applying deep learning, with L2 regularization or dropout, with the  \\nregularization parameter that performs best on the dev set. If you increase the model size,  \\nusually your performance will stay the same or improve; it is unlikely to worsen significantly.  \\nThe only reason to avoid using a bigger model is the increased computational cost.   \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 49 Machine Learning Yearning-Draft Andrew Ng  \\n24 Bias vs. Variance tradeoff \\n \\nYou might have heard of the “Bias vs. Variance tradeoff.” Of the changes you could make to  \\nmost learning algorithms, there are some that reduce bias errors but at the cost of increasing  \\nvariance, and vice versa. This creates a “trade off” between bias and variance.   \\nFor example, increasing the size of your model—adding neurons/layers in a neural network,  \\nor adding input features—generally reduces bias but could increase variance. Alternatively,  \\nadding regularization generally increases bias but reduces variance.   \\nIn the modern era, we often have access to plentiful data and can use very large neural  \\nnetworks (deep learning). Therefore, there is less of a tradeoff, and there are now more  \\noptions for reducing bias without hurting variance, and vice versa.   \\nFor example, you can usually increase a neural network size and tune the regularization  \\nmethod to reduce bias without noticeably increasing variance. By adding training data, you  \\ncan also usually reduce variance without affecting bias.   \\nIf you select a model architecture that is well suited for your task, you might also reduce bias  \\nand variance simultaneously. Selecting such an architecture can be difficult.   \\nIn the next few chapters, we discuss additional specific techniques for addressing bias and  \\nvariance.   \\n \\n  \\nPage 50 Machine Learning Yearning-Draft Andrew Ng  \\n25 Techniques for reducing avoidable bias  \\n \\nIf your learning algorithm suffers from high avoidable bias, you might try the following  \\ntechniques:  \\n•Increase the model size \\u200b(such as number of neurons/layers): This technique reduces  \\nbias, since it should allow you to fit the training set better. If you find that this increases  \\nvariance, then use regularization, which will usually eliminate the increase in variance.  \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm eliminate a  \\nparticular category of errors. (We discuss this further in the next chapter.) These new  \\nfeatures could help with both bias and variance. In theory, adding more features could  \\nincrease the variance; but if you find this to be the case, then use regularization, which will  \\nusually eliminate the increase in variance.   \\n•Reduce or eliminate regularization \\u200b (L2 regularization, L1 regularization, dropout):  \\nThis will reduce avoidable bias, but increase variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\nOne method that is not helpful:   \\n•Add more training data \\u200b: This technique helps with variance problems, but it usually  \\nhas no significant effect on bias.   \\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 51 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n \\nYour algorithm must perform well on the training set before you can expect it to perform  \\nwell on the dev/test sets.   \\nIn addition to the techniques described earlier to address high bias, I sometimes also carry  \\nout an error analysis on the \\u200btraining data \\u200b, following a protocol similar to error analysis on  \\nthe Eyeball dev set. This can be useful if your algorithm has high bias—i.e., if it is not fitting  \\nthe training set well.   \\nFor example, suppose you are building a speech recognition system for an app and have  \\ncollected a training set of audio clips from volunteers. If your system is not doing well on the  \\ntraining set, you might consider listening to a set of ~100 examples that the algorithm is  \\ndoing poorly on to understand the major categories of training set errors. Similar to the dev  \\nset error analysis, you can count the errors in different categories:   \\nAudio clip \\xa0 Loud background \\xa0\\nnoise\\xa0User spoke\\xa0\\nquickly\\xa0Far from \\xa0\\nmicrophone \\xa0Comments\\xa0\\n1 ✔ \\xa0   Car noise \\xa0\\n2 ✔ \\xa0  ✔ \\xa0 Restaurant noise \\xa0\\n3  ✔ \\xa0 ✔ \\xa0 User shouting\\xa0\\nacross living room?\\xa0\\n4 ✔ \\xa0 \\xa0\\xa0  Coffeeshop \\xa0\\n% of total \\xa0 75%\\xa0 25%\\xa0 50%\\xa0  \\n \\nIn this example, you might realize that your algorithm is having a particularly hard time with  \\ntraining examples that have a lot of background noise. Thus, you might focus on techniques  \\nthat allow it to better fit training examples with background noise.   \\nYou might also double-check whether it is possible for a person to transcribe these audio  \\nclips, given the same input audio as your learning algorithm. If there is so much background  \\nnoise that it is simply impossible for anyone to make out what was said, then it might be  \\nunreasonable to expect any algorithm to correctly recognize such utterances. We will discuss  \\nthe benefits of comparing your algorithm to human-level performance in a later section.  \\n\\xa0\\n\\xa0\\nPage 52 Machine Learning Yearning-Draft Andrew Ng  \\n27 Techniques for reducing variance  \\n \\nIf your learning algorithm suffers from high variance, you might try the following  \\ntechniques:  \\n•Add more training data \\u200b: This is the simplest and most reliable way to address variance,  \\nso long as you have access to significantly more data and enough computational power to  \\nprocess the data.   \\n•Add regularization \\u200b (L2 regularization, L1 regularization, dropout): This technique \\nreduces variance but increases bias.   \\n•Add early stopping \\u200b (i.e., stop gradient descent early, based on dev set error): This  \\ntechnique reduces variance but increases bias. Early stopping behaves a lot like  \\nregularization methods, and some authors call it a regularization technique.   \\n•Feature selection to decrease number/type of input features: \\u200b This technique  \\nmight help with variance problems, but it might also increase bias. Reducing the number  \\nof features slightly (say going from 1,000 features to 900) is unlikely to have a huge effect  \\non bias. Reducing it significantly (say going from 1,000 features to 100—a 10x reduction)  \\nis more likely to have a significant effect, so long as you are not excluding too many useful  \\nfeatures. In modern deep learning, when data is plentiful, there has been a shift away from  \\nfeature selection, and we are now more likely to give all the features we have to the  \\nalgorithm and let the algorithm sort out which ones to use based on the data. But when  \\nyour training set is small, feature selection can be very useful.   \\n•Decrease the model size \\u200b(such as number of neurons/layers): \\u200bUse with caution. \\u200b This \\ntechnique could decrease variance, while possibly increasing bias. However, I don’t  \\nrecommend this technique for addressing variance. Adding regularization usually gives  \\nbetter classification performance. The advantage of reducing the model size is reducing  \\nyour computational cost and thus speeding up how quickly you can train models. If  \\nspeeding up model training is useful, then by all means consider decreasing the model size.  \\nBut if your goal is to reduce variance, and you are not concerned about the computational  \\ncost, consider adding regularization instead.   \\nHere are two additional tactics, repeated from the previous chapter on addressing bias:   \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm to eliminate a  \\nparticular category of errors. These new features could help with both bias and variance. In  \\nPage 53 Machine Learning Yearning-Draft Andrew Ng  \\ntheory, adding more features could increase the variance; but if you find this to be the case,  \\nthen use regularization, which will usually eliminate the increase in variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\n \\n \\n  \\nPage 54 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLearning curves  \\n\\xa0\\n\\xa0 \\xa0\\nPage 55 Machine Learning Yearning-Draft Andrew Ng  \\n28 Diagnosing bias and variance: Learning  \\ncurves  \\n \\nWe’ve seen some ways to estimate how much error can be attributed to avoidable bias vs.  \\nvariance. We did so by estimating the optimal error rate and computing the algorithm’s  \\ntraining set and dev set errors. Let’s discuss a technique that is even more informative:  \\nplotting a learning curve.  \\nA learning curve plots your dev set error against the number of training examples. To plot it,  \\nyou would run your algorithm using different training set sizes. For example, if you have  \\n1,000 examples, you might train separate copies of the algorithm on 100, 200, 300, …, 1000 \\nexamples. Then you could plot how dev set error varies with the training set size. Here is an  \\nexample: \\nAs the training set size increases, the dev set error should decrease.   \\nWe will often have some “desired error rate” that we hope our learning algorithm will  \\neventually achieve. For example:   \\n•If we hope for human-level performance, then the human error rate could be the “desired  \\nerror rate.”  \\n•If our learning algorithm serves some product (such as delivering cat pictures), we might  \\nhave an intuition about what level of performance is needed to give users a great  \\nexperience.  \\nPage 56 Machine Learning Yearning-Draft Andrew Ng \\n \\n•If you have worked on a important application for a long time, then you might have  \\nintuition about how much more progress you can reasonably make in the next  \\nquarter/year.   \\nAdd the desired level of performance to your learning curve:   \\n \\n \\n \\n \\n \\n \\n \\nYou can visually extrapolate the red “dev error” curve  to guess how much closer you could  \\nget to the desired level of performance by adding more data. In the example above, it looks  \\nplausible that doubling the training set size might allow you to reach the desired  \\nperformance.  \\nBut if the dev error curve has “plateaued” (i.e. flattened out), then you can immediately tell  \\nthat adding more data won’t get you to your goal:   \\nLooking at the learning curve might therefore help you avoid spending months collecting  \\ntwice as much training data, only to realize it does not help.  \\nPage 57 Machine Learning Yearning-Draft Andrew Ng \\n \\nOne downside of this process is that if you only look at the dev error curve, it can be hard to  \\nextrapolate and predict exactly where the red curve will go if you had more data. There is one  \\nadditional plot that can help you estimate the impact of adding more data: the training error.  \\nPage 58 Machine Learning Yearning-Draft Andrew Ng  \\n29 Plotting training error \\n \\nYour dev set (and test set) error should decrease as the training set size grows. But your  \\ntraining set error usually \\u200bincreases \\u200b as the training set size grows.   \\nLet’s illustrate this effect with an example. Suppose your training set has only 2 examples:  \\nOne cat image and one non-cat image. Then it is easy for the learning algorithms to  \\n“memorize” both examples in the training set, and get 0% training set error. Even if either or  \\nboth of the training examples were mislabeled, it is still easy for the algorithm to memorize  \\nboth labels.   \\nNow suppose your training set has 100 examples. Perhaps even a few examples are  \\nmislabeled, or ambiguous—some images are very blurry, so even humans cannot tell if there  \\nis a cat. Perhaps the learning algorithm can still “memorize” most or all of the training set,  \\nbut it is now harder to obtain 100% accuracy. By increasing the training set from 2 to 100  \\nexamples, you will find that the training set accuracy will drop slightly.   \\nFinally, suppose your training set has 10,000 examples. In this case, it becomes even harder  \\nfor the algorithm to perfectly fit all 10,000 examples, especially if some are ambiguous or  \\nmislabeled. Thus, your learning algorithm will do even worse on this training set.   \\nLet’s add a plot of training error to our earlier figures:   \\n \\n \\n \\n \\n \\n \\n \\nYou can see that the blue “training error” curve increases with the size of the training set.  \\nFurthermore, your algorithm usually does better on the training set than on the dev set; thus  \\nthe red dev error curve usually lies strictly above the blue training error curve.   \\nLet’s discuss next how to interpret these plots. \\xa0\\nPage 59 Machine Learning Yearning-Draft Andrew Ng \\n \\n30 Interpreting learning curves: High bias  \\n \\nSuppose your dev error curve looks like this:   \\n \\n \\n \\n \\n \\n \\nWe previously said that, if your dev error curve plateaus, you are unlikely to achieve the  \\ndesired performance just by adding data.   \\nBut it is hard to know exactly what an extrapolation of the red dev error curve will look like.  \\nIf the dev set was small, you would be even less certain because the curves could be noisy.  \\nSuppose we add the training error curve to this plot and get the following:   \\n \\n \\n \\n \\n \\n \\n \\nNow, you can be absolutely sure that adding more data will not, by itself, be sufficient. Why  \\nis that? Remember our two observations:  \\nPage 60 Machine Learning Yearning-Draft Andrew Ng \\n \\n•As we add more training data, training error can only get worse. Thus, the blue training  \\nerror curve can only stay the same or go higher, and thus it can only get further away from  \\nthe (green line) level of desired performance.   \\n•The red dev error curve is usually higher than the blue training error. Thus, there’s almost  \\nno way that adding more data would allow the red dev error curve to drop down to the  \\ndesired level of performance when even the training error is higher than the desired level  \\nof performance.  \\nExamining both the dev error curve and the training error curve on the same plot allows us  \\nto more confidently extrapolate the dev error curve.   \\nSuppose, for the sake of discussion, that the desired performance is our estimate of the  \\noptimal error rate. The figure above is then the standard “textbook” example of what a  \\nlearning curve with high avoidable bias looks like: At the largest training set  \\nsize—presumably corresponding to all the training data we have—there is a large gap  \\nbetween the training error and the desired performance, indicating large avoidable bias.  \\nFurthermore, the gap between the training and dev curves is small, indicating small  \\nvariance.   \\nPreviously, we were measuring training and dev set error only at the rightmost point of this  \\nplot, which corresponds to using all the available training data. Plotting the full learning  \\ncurve gives us a more comprehensive picture of the algorithms’ performance on different  \\ntraining set sizes.   \\n \\n  \\nPage 61 Machine Learning Yearning-Draft Andrew Ng  \\n31 Interpreting learning curves: Other cases   \\n \\nConsider this learning curve:  \\n \\nDoes this plot indicate high bias, high variance, or both?  \\nThe blue training error curve is relatively low, and the red dev error curve is much higher  \\nthan the blue training error. Thus, the bias is small, but the variance is large. Adding more  \\ntraining data will probably help close the gap between dev error and training error.  \\nNow, consider this: \\n \\nThis time, the training error is large, as it is much higher than the desired level of  \\nperformance. The dev error is also much larger than the training error. Thus, you have  \\nsignificant bias and significant variance. You will have to find a way to reduce both bias and  \\nvariance in your algorithm.   \\nPage 62 Machine Learning Yearning-Draft Andrew Ng  \\n32 Plotting learning curves  \\n \\nSuppose you have a very small training set of 100 examples. You train your algorithm using a  \\nrandomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing  \\nthe number of examples by intervals of ten. You then use these 10 data points to plot your  \\nlearning curve. You might find that the curve looks slightly noisy (meaning that the values  \\nare higher/lower than expected) at the smaller training set sizes.   \\nWhen training on just 10 randomly chosen examples, you might be unlucky and have a  \\nparticularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or,  \\nyou might get lucky and get a particularly “good” training set. Having a small training set  \\nmeans that the dev and training errors may randomly fluctuate.   \\nIf your machine learning application is heavily skewed toward one class (such as a cat  \\nclassification task where the fraction of negative examples is much larger than positive  \\nexamples), or if it has a huge number of classes (such as recognizing 100 different animal  \\nspecies), then the chance of selecting an especially “unrepresentative” or bad training set is  \\nalso larger. For example, if 80% of your examples are negative examples (y=0), and only  \\n20% are positive examples (y=1), then there is a chance that a training set of 10 examples  \\ncontains only negative examples, thus making it very difficult for the algorithm to learn  \\nsomething meaningful.   \\nIf the noise in the training curve makes it hard to see the true trends, here are two solutions:   \\n•Instead of training just one model on 10 examples, instead select several (say 3-10)  \\ndifferent randomly chosen training sets of 10 examples by sampling with replacement  10\\nfrom your original set of 100. Train a different model on each of these, and compute the  \\ntraining and dev set error of each of the resulting models. Compute and plot the average  \\ntraining error and average dev set error.  \\n•If your training set is skewed towards one class, or if it has many classes, choose a  \\n“balanced” subset instead of 10 training examples at random out of the set of 100. For  \\nexample, you can make sure that 2/10 of the examples are positive examples, and 8/10 are  \\n10 Here’s what sampling \\u200bwith replacement \\u200b means: You would randomly pick 10 different examples out of the 100 to form  \\nyour first training set. Then to form the second training set, you would again pick 10 examples, but without taking into \\naccount what had been chosen in the first training set. Thus, it is possible for one specific exa mple to appear in both the  \\nfirst and second training sets. In contrast, if you were sampling \\u200bwithout replacement \\u200b, the second training set would be  \\nchosen from just the 90 examples that had not been chosen the first time around. In practice, sampling with or without \\nreplacement shouldn’t make a huge difference, but the former is common practice.   \\nPage 63 Machine Learning Yearning-Draft Andrew Ng  \\nnegative. More generally, you can make sure the fraction of examples from each class is as  \\nclose as possible to the overall fraction in the original training set.   \\nI would not bother with either of these techniques unless you have already tried plotting  \\nlearning curves and concluded that the curves are too noisy to see the underlying trends. If  \\nyour training set is large—say over 10,000 examples—and your class distribution is not very  \\nskewed, you probably won’t need these techniques.   \\nFinally, plotting a learning curve may be computationally expensive: For example, you might  \\nhave to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training  \\nmodels with small datasets is much faster than training models with large datasets. Thus,  \\ninstead of evenly spacing out the training set sizes on a linear scale as above, you might train  \\nmodels with 1,000, 2,000, 4,000, 6,000, and 10,000 examples. This should still give you a  \\nclear sense of the trends in the learning curves. Of course, this technique is relevant only if  \\nthe computational cost of training all the additional models is significant.   \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 64 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nComparing to \\nhuman-level \\nperformance \\n\\xa0\\n\\xa0\\n  \\nPage 65 Machine Learning Yearning-Draft Andrew Ng  \\n33 Why we compare to human-level  \\nperformance  \\n \\nMany machine learning systems aim to automate things that humans do well. Examples  \\ninclude image recognition, speech recognition, and email spam classification. Learning  \\nalgorithms have also improved so much that we are now surpassing human-level  \\nperformance on more and more of these tasks.   \\nFurther, there are several reasons building an ML system is easier if you are trying to do a  \\ntask that people can do well:   \\n1. Ease of obtaining data from human labelers. \\u200b For example, since people recognize  \\ncat images well, it is straightforward for people to provide high accuracy labels for your  \\nlearning algorithm.  \\n2. Error analysis can draw on human intuition. \\u200b Suppose a speech recognition  \\nalgorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\naudio clip as “This recipe calls for a \\u200bpear\\u200b of apples,” mistaking “pair” for “pear.” You can  \\ndraw on human intuition and try to understand what information a person uses to get the  \\ncorrect transcription, and use this knowledge to modify the learning algorithm.   \\n3. Use human-level performance to estimate the optimal error rate and also set  \\na “desired error rate.” \\u200b Suppose your algorithm achieves 10% error on a task, but a person  \\nachieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\navoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\nEven though item #3 might not sound important, I find that having a reasonable and  \\nachievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\nhigh avoidable bias is incredibly valuable and opens up a menu of options to try.   \\nThere are some tasks that even humans aren’t good at. For example, picking a book to  \\nrecommend to you; or picking an ad to show a user on a website; or predicting the stock  \\nmarket. Computers already surpass the performance of most people on these tasks. With  \\nthese applications, we run into the following problems:   \\n•It is harder to obtain labels. \\u200b For example, it’s hard for human labelers to annotate a  \\ndatabase of users with the “optimal” book recommendation. If you operate a website or  \\napp that sells books, you can obtain data by showing books to users and seeing what they  \\nbuy. If you do not operate such a site, you need to find more creative ways to get data.  \\nPage 66 Machine Learning Yearning-Draft Andrew Ng  \\n•Human intuition is harder to count on. \\u200b For example, pretty much no one can  \\npredict the stock market. So if our stock prediction algorithm does no better than random  \\nguessing, it is hard to figure out how to improve it.   \\n•It is hard to know what the optimal error rate and reasonable desired error  \\nrate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\nprevious chapter for comparing to human-level performance apply:   \\n•Ease of obtaining labeled data from human labelers. \\u200b You can get a team of doctors  \\nto provide labels to you with a 2% error rate.  \\n•Error analysis can draw on human intuition. \\u200bBy discussing images with a team of  \\ndoctors, you can draw on their intuitions.  \\n•Use human-level performance to estimate the optimal error rate and also set  \\nachievable “desired error rate.” \\u200b It is reasonable to use 2% error as our estimate of the  \\noptimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\nhigher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\nreasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\nestimates are necessarily too high.  \\nWhen it comes to obtaining labeled data, you might not want to discuss every image with an  \\nentire team of doctors since their time is expensive. Perhaps you can have a single junior  \\ndoctor label the vast majority of cases and bring only the harder cases to more experienced  \\ndoctors or to the team of doctors.   \\nIf your system is currently at 40% error, then it doesn’t matter much whether you use a  \\njunior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\nintuitions. But if your system is already at 10% error, then defining the human-level  \\nreference as 2% gives you better tools to keep improving your system.   \\n \\xa0\\nPage 68 Machine Learning Yearning-Draft Andrew Ng  \\n35 Surpassing human-level performance   \\n \\nYou are working on speech recognition and have a dataset of audio clips. Suppose your  \\ndataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\nsystem already achieves 8% error. Can you use any of the three techniques described in  \\nChapter 33 to continue making rapid progress?  \\nIf you can identify a subset of data in which humans significantly surpass your system, then  \\nyou can still use those techniques to drive rapid progress. For example, suppose your system  \\nis much better than people at recognizing speech in noisy audio, but humans are still better  \\nat transcribing very rapidly spoken speech.  \\nFor the subset of data with rapidly spoken speech:  \\n1.You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\noutput.  \\n2.You can draw on human intuition to understand why they correctly heard a rapidly  \\nspoken utterance when your system didn’t.  \\n3.You can use human-level performance on rapidly spoken speech as a desired performance  \\ntarget. \\nMore generally, so long as there are dev set examples where humans are right and your  \\nalgorithm is wrong, then many of the techniques described earlier will apply. This is true  \\neven if, averaged over the entire dev/test set, your performance is already surpassing  \\nhuman-level performance.  \\nThere are many important machine learning applications where machines surpass human  \\nlevel performance. For example, machines are better at predicting movie ratings, how long it  \\ntakes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\nsubset of techniques apply once humans have a hard time identifying examples that the  \\nalgorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\nwhere machines already surpass human-level performance, while progress is faster when  \\nmachines are still trying to catch up to humans.   \\n \\n  \\nPage 69 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTraining and  \\ntesting on different  \\ndistributions  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 70 Machine Learning Yearning-Draft Andrew Ng  \\n36 When you should train and test on  \\ndifferent distributions  \\n \\nUsers of your cat pictures app have uploaded 10,000 images, which you have manually  \\nlabeled as containing cats or not. You also have a larger set of 200,000 images that you  \\ndownloaded off the internet. How should you define train/dev/test sets?   \\nSince the 10,000 user images closely reflect the actual probability distribution of data you  \\nwant to do well on, you might use that for your dev and test sets. If you are training a  \\ndata-hungry deep learning algorithm, you might give it the additional 200,000 internet  \\nimages for training. Thus, your training and dev/test sets come from different probability  \\ndistributions. How does this affect your work?   \\nInstead of partitioning our data into train/dev/test sets, we could take all 210,000 images we  \\nhave, and randomly shuffle them into train/dev/test sets. In this case, all the data comes  \\nfrom the same distribution. But I recommend against this method, because about  \\n205,000/210,000 ≈ 97.6% of your dev/test data would come from internet images, which  \\ndoes not reflect the actual distribution you want to do well on. Remember our  \\nrecommendation on choosing dev/test sets:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nMost of the academic literature on machine learning assumes that the training set, dev set  \\nand test set all come from the same distribution.  In the early days of machine learning, data  11\\nwas scarce. We usually only had one dataset drawn from some probability distribution. So  \\nwe would randomly split that data into train/dev/test sets, and the assumption that all the  \\ndata was coming from the same source was usually satisfied.   \\n11 There is some academic research on training and testing on differ ent distributions.  Examples  \\ninclude “domain adaptation, ” “tr ansfer lear ning” and “multitask  lear ning. ” But ther e is still a huge \\ngap between theory and practice.  If you tr ain on dataset A and test on some v ery differ ent type of data \\nB, luck could hav e a huge effect on how well  your  algor ithm performs.  (Her e, “luck ” includes the \\nresearcher’ s hand-designed features for  the particular task , as well as other factors that we just don’ t \\nunder stand yet. ) This mak es the academic study of training and testing on differ ent distributions  \\ndifficult to carr y out in a systematic way.   \\nPage 71 Machine Learning Yearning-Draft Andrew Ng  \\nBut in the era of big data, we now have access to huge training sets, such as cat internet  \\nimages. Even if the training set comes from a different distribution than the dev/test set, we  \\nstill want to use it for learning since it can provide a lot of information.  \\nFor the cat detector example, instead of putting all 10,000 user-uploaded images into the  \\ndev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining  \\n5,000 user-uploaded examples into the training set. This way, your training set of 205,000  \\nexamples contains some data that comes from your dev/test distribution along with the  \\n200,000 internet images. We will discuss in a later chapter why this method is helpful.   \\nLet’s consider a second example. Suppose you are building a speech recognition system to  \\ntranscribe street addresses for a voice-controlled mobile map/navigation app. You have  \\n20,000 examples of users speaking street addresses. But you also have 500,000 examples of  \\nother audio clips with users speaking about other topics. You might take 10,000 examples of  \\nstreet addresses for the dev/test sets, and use the remaining 10,000, plus the additional  \\n500,000 examples, for training.   \\nWe will continue to assume that your dev data and your test data come from the same  \\ndistribution. But it is important to understand that different training and dev/test  \\ndistributions offer some special challenges.   \\n  \\nPage 72 Machine Learning Yearning-Draft Andrew Ng  \\n37 How to decide whether to use all your data  \\n \\nSuppose your cat detector’s training set includes 10,000 user-uploaded images. This data  \\ncomes from the same distribution as a separate dev/test set, and represents the distribution  \\nyou care about doing well on. You also have an additional 20,000 images downloaded from  \\nthe internet. Should you provide all 20,000+10,000=30,000 images to your learning \\nalgorithm as its training set, or discard the 20,000 internet images for fear of it biasing your  \\nlearning algorithm?   \\nWhen using earlier generations of learning algorithms (such as hand-designed computer  \\nvision features, followed by a simple linear classifier) there was a real risk that merging both  \\ntypes of data would cause you to perform worse. Thus, some engineers will warn you against  \\nincluding the 20,000 internet images. \\nBut in the modern era of powerful, flexible learning algorithms—such as large neural  \\nnetworks—this risk has greatly diminished. If you can afford to build a neural network with a  \\nlarge enough number of hidden units/layers, you can safely add the 20,000 images to your  \\ntraining set. Adding the images is more likely to increase your performance.   \\nThis observation relies on the fact that there is some x —> y mapping that works well for  \\nboth types of data. In other words, there exists some system that inputs either an internet  \\nimage or a mobile app image and reliably predicts the label, even without knowing the  \\nsource of the image.   \\nAdding the additional 20,000 images has the following effects:  \\n1.It gives your neural network more examples of what cats do/do not look like. This is  \\nhelpful, since internet images and user-uploaded mobile app images do share some  \\nsimilarities. Your neural network can apply some of the knowledge acquired from internet  \\nimages to mobile app images.   \\n2.It forces the neural network to expend some of its capacity to learn about properties that  \\nare specific to internet images (such as higher resolution, different distributions of how  \\nthe images are framed, etc.) If these properties differ greatly from mobile app images, it  \\nwill “use up” some of the representational capacity of the neural network. Thus there is  \\nless capacity for recognizing data drawn from the distribution of mobile app images,  \\nwhich is what you really care about. Theoretically, this could hurt your algorithms’  \\nperformance.  \\nPage 73 Machine Learning Yearning-Draft Andrew Ng  \\nTo describe the second effect in different terms, we can turn to the fictional character  \\nSherlock Holmes, who says that your brain is like an attic; it only has a finite amount of  \\nspace. He says that “for every addition of knowledge, you forget something that you knew  \\nbefore. It is of the highest importance, therefore, not to have useless facts elbowing out the  \\nuseful ones.”   12\\nFortunately, if you have the computational capacity needed to build a big enough neural  \\nnetwork—i.e., a big enough attic—then this is not a serious concern. You have enough  \\ncapacity to learn from both internet and from mobile app images, without the two types of  \\ndata competing for capacity. Your algorithm’s “brain” is big enough that you don’t have to  \\nworry about running out of attic space.   \\nBut if you do not have a big enough neural network (or another highly flexible learning  \\nalgorithm), then you should pay more attention to your training data matching your dev/test  \\nset distribution.  \\nIf you think you have data that has no benefit,you should just leave out that data for  \\ncomputational reasons. For example, suppose your dev/test sets contain mainly casual  \\npictures of people, places, landmarks, animals. Suppose you also have a large collection of  \\nscanned historical documents:   \\n \\n \\n \\n \\n \\n \\n \\nThese documents don’t contain anything resembling a cat. They also look completely unlike  \\nyour dev/test distribution. There is no point including this data as negative examples,  \\nbecause the benefit from the first effect above is negligible—there is almost nothing your  \\nneural network can learn from this data that it can apply to your dev/test set distribution.  \\nIncluding them would waste computation resources and representation capacity of the  \\nneural network. \\xa0\\n12 \\u200bA Study  in Sc arlet\\u200b \\u200bby Arthur Conan Doyle \\nPage 74 Machine Learning Yearning-Draft Andrew Ng \\n \\n38 How to decide whether to include  \\ninconsistent data  \\n \\nSuppose you want to learn to predict housing prices in New York City. Given the size of a  \\nhouse (input feature x), you want to predict the price (target label y).   \\nHousing prices in New York City are very high. Suppose you have a second dataset of  \\nhousing prices in Detroit, Michigan, where housing prices are much lower. Should you  \\ninclude this data in your training set?   \\nGiven the same size x, the price of a house y is very different depending on whether it is in  \\nNew York City or in Detroit. If you only care about predicting New York City housing prices,  \\nputting the two datasets together will hurt your performance.  In this case, it would be better  \\nto leave out the inconsistent Detroit data.  13\\nHow is this New York City vs. Detroit example different from the  mobile app vs. internet cat  \\nimages example?  \\nThe cat image example is different because, given an input picture x, one can reliably predict  \\nthe label y indicating whether there is a cat, even without knowing if the image is an internet  \\nimage or a mobile app image. I.e., there is a function f(x) that reliably maps from the input x  \\nto the target output y, even without knowing the origin of x. Thus, the task of recognition  \\nfrom internet images is “consistent” with the task of recognition from mobile app images.  \\nThis means there was little downside (other than computational cost) to including all the  \\ndata, and some possible significant upside. In contrast, New York City and Detroit, Michigan  \\ndata are not consistent. Given the same x (size of house), the price is very different  \\ndepending on where the house is.   \\n \\n \\n  \\n13 There is one way to address the problem of Detr oit data being inconsistent with New Yor k City  \\ndata,  which is to add an extra featur e to each training example indicating the city.  Given an input \\nx—which now specifies the city—the target v alue of y is now unambiguous.  Howev er, in practice I  do \\nnot see this done fr equently.   \\nPage 75 Machine Learning Yearning-Draft Andrew Ng  \\n39 Weighting data   \\n \\nSuppose you have 200,000 images from the internet and 5,000 images from your mobile  \\napp users. There is a 40:1 ratio between the size of these datasets. In theory, so long as you  \\nbuild a huge neural network and train it long enough on all 205,000 images, there is no  \\nharm in trying to make the algorithm do well on both internet images and mobile images.   \\nBut in practice, having 40x as many internet images as mobile app images might mean you  \\nneed to spend 40x (or more) as much computational resources to model both, compared to if  \\nyou trained on only the 5,000 images.   \\nIf you don’t have huge computational resources, you could  give the internet images a much  \\nlower weight as a compromise.   \\nFor example, suppose your optimization objective is squared error (This is not a good choice  \\nfor a classification task, but it will simplify our explanation.) Thus, our learning algorithm  \\ntries to optimize:  \\n  \\nThe first sum above is over the 5,000 mobile images, and the second sum is over the  \\n200,000 internet images. You can instead optimize with an additional parameter \\u200b𝛽\\u200b:  \\n \\n If you set \\u200b𝛽\\u200b=1/40, the algorithm would give equal weight to th e 5,000 mobile images and the  \\n200,000 internet images. You can also set the parameter \\u200b𝛽\\u200b to other values, perhaps by  \\ntuning to the dev set.   \\nBy weighting the additional Internet images less, you don’t have to build as massive a neural  \\nnetwork to make sure the algorithm does well on both types of tasks. This type of  \\nre-weighting is needed only when you suspect the additional data (Internet Images) has a  \\nvery different distribution than the dev/test set, or if the additional data is much larger than  \\nthe data that came from the same distribution as the dev/test set (mobile images).   \\nPage 76 Machine Learning Yearning-Draft Andrew Ng  \\n40 Generalizing from the training set to the  \\ndev set  \\n \\nSuppose you are applying ML in a setting where the training and the dev/test distributions  \\nare different. Say, the training set contains Internet images + Mobile images, and the  \\ndev/test sets contain only Mobile images. However, the algorithm is not working well: It has  \\na much higher dev/test set error than you would like. Here are some possibilities of what  \\nmight be wrong:  \\n1.It does not do well on the training set. This is the problem of high (avoidable) bias on the  \\ntraining set distribution.   \\n2.It does well on the training set, but does not generalize well to previously unseen data  \\ndrawn from the same distribution as the training set \\u200b. This is high variance.   \\n3.It generalizes well to new data drawn from the same distribution as the training set, but  \\nnot to data drawn from the dev/test set distribution. We call this problem \\u200bdata  \\nmismatch \\u200b, since it is because the training set data is a poor match for the dev/test set  \\ndata.   \\nFor example, suppose that humans achieve near perfect performance on the cat recognition  \\ntask. Your algorithm achieves this:  \\n•1% error on the training set  \\n•1.5% error on data drawn from the same distribution as the training set that the algorithm  \\nhas not seen  \\n•10% error on the dev set   \\nIn this case, you clearly have a data mismatch problem. To address this, you might try to  \\nmake the training data more similar to the dev/test data. We discuss some techniques for  \\nthis later.  \\nIn order to diagnose to what extent an algorithm suffers from each of the problems 1-3  \\nabove, it will be useful to have another dataset. Specifically, rather than giving the algorithm  \\nall the available training data, you can split it into two subsets: The actual training set which  \\nthe algorithm will train on, and a separate set, which we will call the “Training dev” set, that  \\nwe will not train on.   \\nYou now have four subsets of data:  \\nPage 77 Machine Learning Yearning-Draft Andrew Ng  \\n•Training set. This is the data that the algorithm will learn from (e.g., Internet images +  \\nMobile images). This does not have to be drawn from the same distribution as what we  \\nreally care about (the dev/test set distribution).  \\n•Training dev set: This data is drawn from the same distribution as the training set (e.g.,  \\nInternet images + Mobile images). This is usually smaller than the training set; it only  \\nneeds to be large enough to evaluate and track the progress of our learning algorithm.   \\n•Dev set: This is drawn from the same distribution as the test set, and it reflects the  \\ndistribution of data that we ultimately care about doing well on. (E.g., mobile images.)   \\n•Test set: This is drawn from the same distribution as the dev set. (E.g., mobile images.)  \\nArmed with these four separate datasets, you can now evaluate:  \\n•Training error, by evaluating on the training set.   \\n•The algorithm’s ability to generalize to new data drawn from the training set distribution,  \\nby evaluating on the training dev set.  \\n•The algorithm’s performance on the task you care about, by evaluating on the dev and/or  \\ntest sets.   \\nMost of the guidelines in Chapters 5-7 for picking the size of the dev set also apply to the  \\ntraining dev set.  \\n \\n \\n \\n \\n \\n \\n  \\nPage 78 Machine Learning Yearning-Draft Andrew Ng  \\n41 Identifying Bias, Variance, and Data  \\nMismatch Errors  \\n \\nSuppose humans achieve almost perfect performance (≈0% error) on the cat detection task,  \\nand thus the optimal error rate is about 0%. Suppose you have:  \\n•1% error on the training set. \\n•5% error on training dev set.   \\n•5% error on the dev set.   \\nWhat does this tell you? Here, you know that you have high variance. The variance reduction  \\ntechniques described earlier should allow you to make progress.   \\nNow, suppose your algorithm achieves:   \\n•10% error on the training set.  \\n•11% error on training dev set.  \\n•12% error on the dev set.   \\nThis tells you that you have high avoidable bias on the training set. I.e., the algorithm is  \\ndoing poorly on the training set. Bias reduction techniques should help.  \\nIn the two examples above, the algorithm suffered from only high avoidable bias or high  \\nvariance. It is possible for an algorithm to suffer from any subset of high avoidable bias, high  \\nvariance, and data mismatch. For example:   \\n•10% error on the training set.   \\n•11% error on training dev set.  \\n•20% error on the dev set.   \\nThis algorithm suffers from high avoidable bias and from data mismatch. It does not,  \\nhowever, suffer from high variance on the training set distribution.   \\nIt might be easier to understand how the different types of errors relate to each other by  \\ndrawing them as entries in a table:   \\nPage 79 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\nContinuing with the example of th \\u200be cat image detector, you can see that there are two  \\ndifferent distributions of data on the x-axis. On the y-axis, we ha \\u200bve three types of error:  \\nhuman level error, error on examples the algorithm has trained on, and error on examples  \\nthe algorithm has not trained on. We can fill in the boxes with the different types of errors we  \\nidentified in the previous chapter.   \\nIf you wish, you can also fill in the remaining two boxes in this table: You can fill in the  \\nupper-right box (Human level performance on Mobile Images) by asking some humans to  \\nlabel your mobile cat images data and measure their error. You can fill in the next box by  \\ntaking the mobile cat images (Distribution B) and putting a small fraction of into the training  \\nset so that the neural network learns on it too. Then you measure the learned model’s error  \\non that subset of data. Filling in these two additional entries may sometimes give additional  \\ninsight about what the algorithm is doing on the two different distributions (Distribution A  \\nand B) of data.  \\nBy understanding which types of error the algorithm suffers from the most, you will be better  \\npositioned to decide whether to focus on reducing bias, reducing variance, or reducing data  \\nmismatch.  \\n  \\nPage 80 Machine Learning Yearning-Draft Andrew Ng  \\n42 Addressing data mismatch  \\n \\nSuppose you have developed a speech recognition system that does very well on the training  \\nset and on the training dev set. However, it does poorly on your dev set: You have a data  \\nmismatch problem. What can you do?  \\nI recommend that you: (i) Try to understand what properties of the data differ between the  \\ntraining and the dev set distributions. (ii) Try to find more training data that better matches  \\nthe dev set examples that your algorithm has trouble with.  14\\nFor example, suppose you carry out an error analysis on the speech recognition dev set: You  \\nmanually go through 100 examples, and try to understand where the algorithm is making  \\nmistakes. You find that your system does poorly because most of the audio clips in the dev  \\nset are taken within a car, whereas most of the training examples were recorded against a  \\nquiet background. The engine and road noise dramatically worsen the performance of your  \\nspeech system. In this case, you might try to acquire more training data comprising audio  \\nclips that were taken in a car. The purpose of the error analysis is to understand the  \\nsignificant differences between the training and the dev set, which is what leads to the data  \\nmismatch.  \\nIf your training and training dev sets include audio recorded within a car, you should also  \\ndouble-check your system’s performance on this subset of data. If it is doing well on the car  \\ndata in the training set but not on car data in the training dev set, then this further validates  \\nthe hypothesis that getting more car data would help. This is why we discussed the  \\npossibility of including in your training set some data drawn from the same distribution as  \\nyour dev/test set in the previous chapter. Doing so allows you to compare your performance  \\non the car data in the training set vs. the dev/test set.   \\nUnfortunately, there are no guarantees in this process. For example, if you don't have any  \\nway to get more training data that better match the dev set data, you might not have a clear  \\npath towards improving performance.  \\n \\n \\xa0\\n14There is also some research on “domain adaptation”—how to tr ain an algorithm on one distribution  \\nand hav e it gener alize to a differ ent distribution.  These methods ar e typically applicable only in  \\nspecial types of problems and are much less widely used than the ideas descr ibed in this chapter .  \\nPage 81 Machine Learning Yearning-Draft Andrew Ng  \\n43 Artificial data synthesis  \\n \\nYour speech system needs more data that sounds as if it were taken from within a car. Rather  \\nthan collecting a lot of data while driving around, there might be an easier way to get this  \\ndata: By artificially synthesizing it.  \\nSuppose you obtain a large quantity of car/road noise audio clips. You can download this  \\ndata from several websites. Suppose you also have a large training set of people speaking in a  \\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip  \\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in  \\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it  \\nwere collected inside a car.   \\nMore generally, there are several circumstances where artificial data synthesis allows you to  \\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as  \\na second example. You notice that dev set images have much more motion blur because they  \\ntend to come from cellphone users who are moving their phone slightly while taking the  \\npicture. You can take non-blurry images from the training set of internet images, and add  \\nsimulated motion blur to them, thus making them more similar to the dev set.   \\nKeep in mind that artificial data synthesis has its challenges: it is sometimes easier to create  \\nsynthetic data that appears realistic to a person than it is to create data that appears realistic  \\nto a computer. For example, suppose you have 1,000 hours of speech training data, but only  \\n1 hour of car noise. If you repeatedly use the same 1 hour of car noise with different portions  \\nfrom the original 1,000 hours of training data, you will end up with a synthetic dataset where  \\nthe same car noise is repeated over and over. While a person listening to this audio probably  \\nwould not be able to tell—all car noise sounds the same to most of us—it is possible that a  \\nlearning algorithm would “overfit” to the 1 hour of car noise. Thus, it could generalize poorly  \\nto a new audio clip where the car noise happens to sound different.   \\nAlternatively, suppose you have 1,000 unique hours of car noise, but all of it was taken from  \\njust 10 different cars. In this case, it is possible for an algorithm to “overfit” to these 10 cars  \\nand perform poorly if tested on audio from a different car. Unfortunately, these problems  \\ncan be hard to spot.   \\n \\n \\n \\nPage 82 Machine Learning Yearning-Draft Andrew Ng  \\n \\nTo take one more example, suppose you are building a computer vision system to recognize  \\ncars. Suppose you partner with a video gaming company, which has computer graphics  \\nmodels of several cars. To train your algorithm, you use the models to generate synthetic  \\nimages of cars. Even if the synthesized images look very realistic, this approach (which has  \\nbeen independently proposed by many people) will probably not work well. The video game  \\nmight have ~20 car designs in the entire video game. It is very expensive to build a 3D car  \\nmodel of a car; if you were playing the game, you probably wouldn’t notice that you’re seeing  \\nthe same cars over and over, perhaps only painted differently. I.e., this data looks very  \\nrealistic to you. But compared to the set of all cars out on roads—and therefore what you’re  \\nlikely to see in the dev/test sets—this set of 20 synthesized cars captures only a minuscule  \\nfraction of the world’s distribution of cars. Thus if your 100,000 training examples all come  \\nfrom these 20 cars, your system will “overfit” to these 20 specific car designs, and it will fail  \\nto generalize well to dev/test sets that include other car designs.   \\nWhen synthesizing data, put some thought into whether you’re really synthesizing a  \\nrepresentative set of examples. Try to avoid giving the synthesized data properties that  \\nmakes it possible for a learning algorithm to distinguish synthesized from non-synthesized  \\nexamples—such as if all the synthesized data comes from one of 20 car designs, or all the  \\nsynthesized audio comes from only 1 hour of car noise. This advice can be hard to follow.   \\nWhen working on data synthesis, my teams have sometimes taken weeks before we produced  \\ndata with details that are close enough to the actual distribution for the synthesized data to  \\nhave a significant effect. But if you are able to get the details right, you can suddenly access a  \\nfar larger training set than before.   \\n \\n \\nPage 83 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\nDebugging  \\ninference  \\nalgorithms \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 84 Machine Learning Yearning-Draft Andrew Ng  \\n44 The Optimization Verification test  \\n \\nSuppose you are building a speech recognition system. Your system works by inputting an  \\naudio clip \\u200bA\\u200b, and computing some Score \\u200bA\\u200b(\\u200bS\\u200b) for each possible  output sentence \\u200bS\\u200b. For  \\nexample, you might try to estimate Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b), the pro bability that the correct  \\noutput transcription is the sentence \\u200bS\\u200b,  given that the input audio was \\u200bA.   \\nGiven a way to compute Score \\u200bA\\u200b(\\u200bS\\u200b), you still have to find the En glish sentence \\u200bS\\u200b that \\nmaximizes it:  \\n \\nHow do you compute the “arg max” above? If the English language has 50,000 words, then  \\nthere are (50,000)\\u200bN \\u200bpossi ble sentences of length \\u200bN\\u200b—far too many to exhaustively enumerate.  \\nSo, you need to apply an approximate search algorithm, to try to find the value of \\u200bS\\u200b that  \\noptimizes (maximizes) Score \\u200bA\\u200b(\\u200bS\\u200b). One example search algorith m is “beam search,” which  \\nkeeps only \\u200bK\\u200b top candidates during the search process. (For the purposes of this chapter, you  \\ndon’t need to understand the details of beam search.) Algorithms like this are not guaranteed  \\nto find the value of \\u200bS \\u200bthat maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nSuppose that an audio clip \\u200bA\\u200b records someone saying “I love machine learning.” But instead  \\nof outputting the correct transcription, your system outputs the incorrect “I love robots.”  \\nThere are now two possibilities for what went wrong:   \\n1.Search algorithm problem \\u200b. The approximate search algorithm (beam search) failed  \\nto find the value of \\u200bS\\u200b that maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\n2.Objective (scoring function) problem. \\u200b Our estimates for Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b) were \\ninaccurate. In particular, our choice of Score \\u200bA\\u200b(\\u200bS\\u200b) failed to r ecognize that “I love machine  \\nlearning” is the correct transcription.   \\nDepending on which of these was the cause of the failure, you should prioritize your efforts  \\nvery differently. If #1 was the problem, you should work on improving the search algorithm.  \\nIf #2 was the problem, you should work on the learning algorithm that estimates Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nFacing this situation, some researchers will randomly decide to work on the search  \\nalgorithm; others will randomly work on a better way to learn values for Score \\u200bA\\u200b(S). But \\nunless you know which of these is the underlying cause of the error, your efforts could be  \\nwasted. How can you decide more systematically what to work on?   \\nPage 85 Machine Learning Yearning-Draft Andrew Ng  \\nLet S\\u200bout\\u200b be the output tran scription (“I love robots”). Let S* be the correct transcription (“I  \\nlove machine learning”). In order to understand whether #1 or #2 above is the problem, you  \\ncan perform the \\u200bOptimization Verification test \\u200b: First, compute Score \\u200bA\\u200b(\\u200bS\\u200b*) and \\nScore\\u200bA\\u200b(\\u200bS\\u200bout\\u200b). Then check w hether Score \\u200bA\\u200b(\\u200bS\\u200b*) > Score \\u200bA\\u200b(\\u200bS\\u200bout\\u200b). There are two possibilities:   \\nCase 1: Score\\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, your learning algorithm has correctly given S* a higher score than S \\u200bout\\u200b. \\nNevertheless, our approximate search algorithm chose S \\u200bout \\u200brather than S*. This tells you that  \\nyour approximate search algorithm is failing to choose the value of S that maximizes  \\nScore\\u200bA\\u200b(\\u200bS\\u200b). In this case, the Optimization Verification test tells  you that you have a search  \\nalgorithm problem and should focus on that. For example, you could try increasing the beam  \\nwidth of beam search.   \\nCase 2: Score\\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, you know that the way you’re computing Score \\u200bA\\u200b(.) is at fault: It is failing to give a  \\nstrictly higher score to the correct output \\u200bS\\u200b* than the incorrect \\u200bS\\u200bout\\u200b. The Optimization \\nVerification test tells you that you have an objective (scoring) function problem. Thus, you  \\nshould focus on improving how you learn or approximate Score \\u200bA\\u200b(\\u200bS\\u200b) for different sentences \\u200bS\\u200b.  \\nOur discussion has focused on a single example. To apply the Optimization Verification test  \\nin practice, you should examine the errors in your dev set. For each error, you would test  \\nwhether Score \\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b). Each dev example for which this inequality holds will get  \\nmarked as an error caused by the optimization algorithm. Each example for which this does  \\nnot hold (Score \\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)) gets counted as a mistake due to the way you’re  \\ncomputing Score \\u200bA\\u200b(.).  \\nFor example, suppose you find that 95% of the errors were due to the scoring function  \\nScore\\u200bA\\u200b(.), and only 5% due to the optimization algorithm. Now  you know that no matter how  \\nmuch you improve your optimization procedure, you would realistically eliminate only ~5%  \\nof our errors. Thus, you should instead focus on improving how you estimate Score \\u200bA\\u200b(.).  \\n \\n \\n \\n \\xa0\\nPage 86 Machine Learning Yearning-Draft Andrew Ng  \\n45 General form of Optimization Verification  \\ntest \\n \\nYou can apply the Optimization Verification test when, given some input \\u200bx\\u200b, you know how  to  \\ncompute Score \\u200bx\\u200b(\\u200by\\u200b) that indicates how good a response \\u200by\\u200b is to a n input \\u200bx\\u200b. Furthermore, you \\nare using an approximate algorithm to try to find arg max \\u200by\\u200b Score\\u200bx\\u200b(\\u200by\\u200b), but suspect that the  \\nsearch algorithm is sometimes failing to find the maximum. In our previous speech  \\nrecognition example, \\u200bx=A\\u200b was an audio clip, and \\u200by=S\\u200b was the output transcript.   \\nSuppose y* is the “correct” output but the algorithm instead outputs y \\u200bout\\u200b. Then the key test is  \\nto measure whether Score \\u200bx\\u200b(y*) > Score \\u200bx\\u200b(y\\u200bout\\u200b). If this inequality holds, then we blame the  \\noptimization algorithm for the mistake. Refer to the previous chapter to make sure you  \\nunderstand the logic behind this. Otherwise, we blame the computation of Score \\u200bx\\u200b(y).  \\nLet’s look at one more example. Suppose you are building a Chinese-to-English machine  \\ntranslation system. Your system works by inputting a Chinese sentence \\u200bC\\u200b, and computing \\nsome Score \\u200bC\\u200b(\\u200bE\\u200b) for each p ossible translation \\u200bE\\u200b. For example, you might use Score \\u200bC\\u200b(\\u200bE\\u200b) = \\nP(\\u200bE\\u200b|\\u200bC\\u200b), the probability of the translation being E given that the input sentence was \\u200bC\\u200b.  \\nYour algorithm translates sentences by trying to compute:   \\n \\nHowever, the set of all possible English sentences \\u200bE \\u200bis too large, so you rely on a heuristic  \\nsearch algorithm.  \\nSuppose your algorithm outputs an incorrect translation \\u200bE\\u200bout\\u200b rather than some correct  \\ntranslation \\u200bE \\u200b*. Then the Optimization Verification test would ask you to compute whether  \\nScore\\u200bC\\u200b(\\u200bE*\\u200b) > Score \\u200bC\\u200b(\\u200bE\\u200bout\\u200b). If this inequality holds, then the Score \\u200bC\\u200b(.) correctly recognized E*  \\nas a superior output to \\u200bE\\u200bout\\u200b; thus, you would attribute this error to the approximate search  \\nalgorithm. Otherwise, you attribute this error to the computation of Score \\u200bC\\u200b(.).  \\nIt is a very common “design pattern” in AI to first learn an approximate scoring function  \\nScore\\u200bx\\u200b(.), then use an approximate maximization algorithm. If you are able to spot this  \\npattern, you will be able to use the Optimization Verification test to understand your source  \\nof errors.  \\n  \\nPage 87 Machine Learning Yearning-Draft Andrew Ng  \\n46 Reinforcement learning example  \\n \\nSuppose you are using machine learning to teach a helicopter to fly complex maneuvers.  \\nHere is a time-lapse photo of a computer-controller helicopter executing a landing with the  \\nengine turned off.  \\nThis is called an “autorotation” maneuver. It allows helicopters to land even if their engine  \\nunexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal  \\nis to use a learning algorithm to fly the helicopter through a trajectory \\u200bT \\u200bthat ends in a safe  \\nlanding.   \\nTo apply reinforcement learning, you have to develop a “Reward function” \\u200bR\\u200b(.) that gives a  \\nscore measuring how good each possible trajectory \\u200bT\\u200b is. For example, if \\u200bT \\u200bresults in the  \\nhelicopter crashing, then perhaps the reward is \\u200bR(T)\\u200b = -1,000—a huge negative reward. A \\ntrajectory \\u200bT\\u200b resulting in a safe landing might result in a positive \\u200bR(T) \\u200bwith the exact value  \\ndepending on how smooth the landing was. The reward function \\u200bR\\u200b(.) is typically chosen by  \\nhand to quantify how desirable different trajectories \\u200bT\\u200b are. It has to trade off how bumpy the  \\nlanding was, whether the helicopter landed in exactly the desired spot, how rough the ride  \\ndown was for passengers, and so on. It is not easy to design good reward functions.   \\nPage 88 Machine Learning Yearning-Draft Andrew Ng \\n \\nGiven a reward function \\u200bR(T), \\u200bthe job of the reinforcement learning algorithm is to control  \\nthe helicopter so that it  achieves max \\u200bT\\u200b R(T). \\u200bHowever, reinforcement learning algorithms  \\nmake many approximations and may not succeed in achieving this maximization.   \\nSuppose you have picked some reward \\u200bR(.)\\u200b and have run your learning algorithm. However, \\nits performance appears far worse than your human pilot—the landings are bumpier and  \\nseem less safe than what a human pilot achieves. How can you tell if the fault is with the  \\nreinforcement learning algorithm—which is trying to carry out a trajectory that achieves  \\nmax\\u200bT\\u200b \\u200bR(T)\\u200b—or if the fault  is with the reward function—which is trying to measure as well as  \\nspecify the ideal tradeoff between ride bumpiness and accuracy of landing spot?   \\nTo apply the Optimization Verification test, let \\u200bT\\u200bhum an\\u200b be the trajectory achieved by the  \\nhuman pilot, and let \\u200bT\\u200bout \\u200bbe the trajectory achieved by the algorithm. According to our  \\ndescription above, \\u200bT\\u200bhum an \\u200bis a superior trajectory to \\u200bT\\u200bout\\u200b. Thus, the key test is the following:  \\nDoes it hold true that \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) > \\u200bR\\u200b(\\u200bT\\u200bout\\u200b)?  \\nCase 1: If this inequality holds, then the reward function \\u200bR\\u200b(.) is correctly rating \\u200bT\\u200bhum an \\u200bas \\nsuperior to \\u200bT\\u200bout\\u200b. But our reinforcement learning algorithm is finding the inferior \\u200bT\\u200bout. \\u200bThis  \\nsuggests that working on improving our reinforcement learning algorithm is worthwhile.   \\nCase 2: The inequality does not hold: \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) ≤ \\u200bR\\u200b(\\u200bT\\u200bout\\u200b). This means \\u200bR \\u200b(.) assigns a worse  \\nscore to \\u200bT\\u200bhum an \\u200beven though it is the superior trajectory. You sh ould work on improving \\u200bR \\u200b(.) to \\nbetter capture the tradeoffs that correspond to a good landing.   \\nMany machine learning applications have this “pattern” of optimizing an approximate  \\nscoring function Score \\u200bx\\u200b(.) using an approximate search algorit hm. Sometimes, there is no  \\nspecified input \\u200bx\\u200b, so this reduces to just Score(.). In our example above, the scoring function  \\nwas the reward function Score( \\u200bT\\u200b)=R(\\u200bT\\u200b), and the optimization algorithm was the  \\nreinforcement learning algorithm trying to execute a good trajectory \\u200bT\\u200b.  \\nOne difference between this and earlier examples is that, rather than comparing to an  \\n“optimal” output, you were instead comparing to human-level performance \\u200bT\\u200bhum an\\u200b.We \\nassumed \\u200bT\\u200bhum an\\u200b is pretty good, even if not optimal. In general, so long as you have some y* (in  \\nthis example, \\u200bT\\u200bhum an\\u200b) that is a superior output to the performance of your current learning  \\nalgorithm—even if it is not the “optimal” output—then the Optimization Verification test can  \\nindicate whether it is more promising to improve the optimization algorithm or  the scoring  \\nfunction.  \\n \\n\\xa0\\nPage 89 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nEnd-to-end  \\ndeep learning  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 90 Machine Learning Yearning-Draft Andrew Ng  \\n47 The rise of end-to-end learning  \\n \\nSuppose you want to build a system to examine online product reviews and automatically tell  \\nyou if the writer liked or disliked that product. For example, you hope to recognize the  \\nfollowing review as highly positive:   \\nThis is a great mop!   \\nand the following as highly negative:  \\nThis mop is low quality--I regret buying it.   \\nThe problem of recognizing positive vs. negative opinions is called “sentiment classification.”  \\nTo build this system, you might build a “pipeline” of two components:  \\n1.Parser: A system that annotates the text with information identifying the most  \\nimportant words.  For example, you might use the parser to label all the adjectives  15\\nand nouns. You would therefore get the following annotated text:   \\nThis is a great \\u200bAdjectiv e\\u200b mop\\u200bNoun\\u200b! \\n2.Sentiment classifier: A learning algorithm that takes as input the annotated text and  \\npredicts the overall sentiment. The parser’s annotation could help this learning  \\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to  \\nquickly hone in on the important words such as “great,” and ignore less important  \\nwords such as “this.”   \\nWe can visualize your “pipeline” of two components as follows:   \\n \\n \\n  \\nThere has been a recent trend toward replacing pipeline systems with a single learning  \\nalgorithm. An \\u200bend-to-end learning algorithm \\u200b for this task would simply take as input  \\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:   \\n \\n15 A parser giv es a much r icher annotation of the text than this,  but this simplified descr iption will  \\nsuffice for explaining end-to-end deep lear ning.   \\nPage 91 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\n \\n \\n \\nNeural networks are commonly used in end-to-end learning systems. The term “end-to-end”  \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to  \\nthe desired output. I.e., the learning algorithm directly connects the “input end” of the  \\nsystem to the “output end.”  \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c”  \\nsound in “cake.” This system tries to recognize the phonemes in the audio clip.  \\n3.Final recognizer: Take the sequence of recognized phonemes, and try to string them  \\ntogether into an output transcript.   \\nIn contrast, an end-to-end system might input an audio clip, and try to directly output the  \\ntranscript:   \\n \\n \\n \\nSo far, we have only described machine learning “pipelines” that are completely linear: the  \\noutput is sequentially passed from one staged to the next. Pipelines can be more complex.  \\nFor example, here is a simple architecture for an autonomous car:   \\n \\n \\n \\n \\nPage 93 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nIt has three components: One detects other cars using the camera images; one detects  \\npedestrians; then a final component plans a path for our own car that avoids the cars and  \\npedestrians.   \\nNot every component in a pipeline has to be learned. For example, the literature on “robot  \\nmotion planning” has numerous algorithms for the final path planning step for the car. Many  \\nof these algorithms do not involve learning.   \\nIn contrast, and end-to-end approach might try to take in the sensor inputs and directly  \\noutput the steering direction:   \\n \\n \\n \\nEven though end-to-end learning has seen many successes, it is not always the best  \\napproach. For example, end-to-end speech recognition works well. But I’m skeptical about  \\nend-to-end learning for autonomous driving. The next few chapters explain why.   \\n  \\nPage 94 Machine Learning Yearning-Draft Andrew Ng \\n \\n49 Pros and cons of end-to-end learning   \\n \\nConsider the same speech pipeline from our earlier example:  \\nMany parts of this pipeline were “hand-engineered”:  \\n•MFCCs are a set of hand-designed audio features. Although they provide a reasonable  \\nsummary of the audio input, they also simplify the input signal by throwing some  \\ninformation away.  \\n•Phonemes are an invention of linguists. They are an imperfect representation of speech  \\nsounds. To the extent that phonemes are a poor approximation of reality, forcing an  \\nalgorithm to use a phoneme representation will limit the speech system’s performance.  \\nThese hand-engineered components limit the potential performance of the speech system.  \\nHowever, allowing hand-engineered components also has some advantages:  \\n•The MFCC features are robust to some properties of speech that do not affect the content,  \\nsuch as speaker pitch. Thus, they help simplify the problem for the learning algorithm.  \\n•To the extent that phonemes are a reasonable representation of speech, they can also help  \\nthe learning algorithm understand basic sound components and therefore improve its  \\nperformance. \\nHaving more hand-engineered components generally allows a speech system to learn with  \\nless data. The hand-engineered knowledge captured by MFCCs and phonemes  \\n“supplements” the knowledge our algorithm acquires from data. When we don’t have much  \\ndata, this knowledge is useful.   \\nNow, consider the end-to-end system:   \\n \\n \\n \\nPage 95 Machine Learning Yearning-Draft Andrew Ng \\n \\nThis system lacks the hand-engineered knowledge. Thus, when the training set is small, it  \\nmight do worse than the hand-engineered pipeline.   \\nHowever, when the training set is large, then it is not hampered by the limitations of an  \\nMFCC or phoneme-based representation. If the learning algorithm is a large-enough neural  \\nnetwork and if it is trained with enough training data, it has the potential to do very well, and  \\nperhaps even approach the optimal error rate.   \\nEnd-to-end learning systems tend to do well when there is a lot of labeled data for “both  \\nends”—the input end and the output end. In this example, we require a large dataset of  \\n(audio, transcript) pairs. When this type of data is not available, approach end-to-end  \\nlearning with great caution.   \\nIf you are working on a machine learning problem where the training set is very small, most  \\nof your algorithm’s knowledge will have to come from your human insight. I.e., from your  \\n“hand engineering” components.   \\nIf you choose not to use an end-to-end system, you will have to decide what are the steps in  \\nyour pipeline, and how they should plug together. In the next few chapters, we’ll give some  \\nsuggestions for designing such pipelines.   \\n \\n \\n \\n  \\nPage 96 Machine Learning Yearning-Draft Andrew Ng  \\n50 Choosing pipeline components: Data  \\navailability \\n \\nWhen building a non-end-to-end pipeline system, what are good candidates for the  \\ncomponents of the pipeline? How you design the pipeline will greatly impact the overall  \\nsystem’s performance. One important factor is whether you can easily collect data to train  \\neach of the components.   \\n \\nFor example, consider this autonomous driving architecture:  \\n \\n \\n \\n \\n \\nYou can use machine learning to detect cars and pedestrians. Further, it is not hard to obtain  \\ndata for these: There are numerous computer vision datasets with large numbers of labeled  \\ncars and pedestrians. You can also use crowdsourcing (such as Amazon Mechanical Turk) to  \\nobtain even larger datasets. It is thus relatively easy to obtain training data to build a car  \\ndetector and a pedestrian detector.   \\nIn contrast, consider a pure end-to-end approach:   \\n \\n \\n \\nTo train this system, we would need a large dataset of (Image, Steering Direction) pairs. It is  \\nvery time-consuming and expensive to have people drive cars around and record their  \\nsteering direction to collect such data. You need a fleet of specially-instrumented cars, and a  \\nhuge amount of driving to cover a wide range of possible scenarios. This makes an  \\nend-to-end system difficult to train. It is much easier to obtain a large dataset of labeled car  \\nor pedestrian images.   \\nMore generally, if there is a lot of data available for training “intermediate modules” of a  \\npipeline (such as a car detector or a pedestrian detector), then you might consider using a  \\nPage 97 Machine Learning Yearning-Draft Andrew Ng \\n \\npipeline with multiple stages. This structure could be superior because you could use all that  \\navailable data to train the intermediate modules.   \\nUntil more end-to-end data becomes available, I believe the non-end-to-end approach is  \\nsignificantly more promising for autonomous driving: Its architecture better matches the  \\navailability of data.  \\n \\n \\n \\n  \\nPage 98 Machine Learning Yearning-Draft Andrew Ng  \\n51 Choosing pipeline components: Task  \\nsimplicity   \\n \\nOther than data availability, you should also consider a second factor when picking  \\ncomponents of a pipeline: How simple are the tasks solved by the individual components?  \\nYou should try to choose pipeline components that are individually easy to build or learn.  \\nBut what does it mean for a component to be “easy” to learn?   \\n \\nConsider these machine learning tasks, listed in order of increasing difficulty:   \\n1.Classifying whether an image is overexposed (like the example above)   \\n2.Classifying whether an image was taken indoor or outdoor  \\n3.Classifying whether an image contains a cat  \\n4.Classifying whether an image contains a cat with both black and white fur  \\n5.Classifying whether an image contains a Siamese cat (a particular breed of cat)  \\n \\nEach of these is a binary image classification task: You have to input an image, and output  \\neither 0 or 1. But the tasks earlier in the list seem much “easier” for a neural network to  \\nlearn. You will be able to learn the easier tasks with fewer training examples.   \\nMachine learning does not yet have a good formal definition of what makes a task easy or  \\nhard.  With the rise of deep learning and multi-layered neural networks, we sometimes say a  16\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a  \\nshallow neural network), and “hard” if it requires more computation steps (requiring a  \\ndeeper neural network). But these are informal definitions.   \\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function  \\nis the length of the shortest computer program that can produce that function. However, this theoretical concept has found  \\nfew practical applications in AI. See also: https://en.wikipedia.org/wiki/Kolmogorov_complexity  \\nPage 99 Machine Learning Yearning-Draft Andrew Ng  \\nIf you are able to take a complex task, and break it down into simpler sub-tasks, then by  \\ncoding in the steps of the sub-tasks explicitly, you are giving the algorithm prior knowledge  \\nthat can help it learn a task more efficiently.   \\n \\nSuppose you are building a Siamese cat detector. This is the pure end-to-end architecture:  \\n \\nIn contrast, you can alternatively use a pipeline with two steps:   \\n \\nThe first step (cat detector) detects all the cats in the image.   \\nPage 100 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThe second step then passes cropped images of each of the detected cats (one at a time) to a  \\ncat species classifier, and finally outputs 1 if any of the cats detected is a Siamese cat.  \\n \\nCompared to training a purely end-to-end classifier using just labels 0/1, each of the two  \\ncomponents in the pipeline--the cat detector and the cat breed classifier--seem much easier  \\nto learn and will require significantly less data.  17\\n \\n17 If you are familiar with practical object detection algorithms, you will recognize that they do not learn just with 0/1 \\nimage labels, but are instead trained with bounding boxes provided as part of the training data. A discussion of them is  \\nbeyond the scope of this chapter. See the Deep Learning specialization on Coursera (\\u200bhttp://deeplearning.ai\\u200b) if you would \\nlike to learn more about such algorithms.   \\nPage 101 Machine Learning Yearning-Draft Andrew Ng  \\nAs one final example, let’s revisit the autonomous driving pipeline.   \\n \\n \\n \\n \\n \\n \\n \\nBy using this pipeline, you are telling the algorithm that there are 3 key steps to driving: (1)  \\nDetect other cars, (2) Detect pedestrians, and (3) Plan a path for your car. Further, each of  \\nthese is a relatively simpler function--and can thus be learned with less data--than the  \\npurely end-to-end approach.  \\n \\nIn summary, when deciding what should be the components of a pipeline, try to build a  \\npipeline where each component is a relatively “simple” function that can therefore be learned  \\nfrom only a modest amount of data.   \\n \\n \\n \\n \\n  \\nPage 102 Machine Learning Yearning-Draft Andrew Ng \\n \\n52 Directly learning rich outputs  \\n \\nAn image classification algorithm will input an image \\u200bx\\u200b, and output an integer indicating the  \\nobject category. Can an algorithm instead output an entire sentence describing the image?   \\nFor example:  \\n \\n \\nx\\u200b =   \\n \\ny\\u200b = “A yellow bus driving down a road with  \\ngreen trees and green grass in the  \\nbackground.” \\nTraditional applications of supervised learning learned a function \\u200bh\\u200b:\\u200bX\\u200b→\\u200bY\\u200b, where the output  \\ny\\u200b was usually an integer or a real number. For example:   \\nProblem \\xa0 X\\xa0 Y\\xa0\\nSpam classification \\xa0 Email\\xa0\\xa0 Spam/Not spam (0/1) \\xa0\\nImage recognition \\xa0 Image\\xa0 Integer label \\xa0\\nHousing price prediction \\xa0Features of house\\xa0 Price in dollars \\xa0\\nProduct recommendation \\xa0Product & user features\\xa0 Chance of purchase \\xa0\\n \\nOne of the most exciting developments in end-to-end deep learning is that it is letting us  \\ndirectly learn \\u200by\\u200b that are much more complex than a number. In the image-captioning  \\nexample above, you can have a neural network input an image ( \\u200bx\\u200b) and directly output a  \\ncaption ( \\u200by\\u200b).  \\n \\n \\n \\n \\n \\nPage 103 Machine Learning Yearning-Draft Andrew Ng  \\nHere are more examples:  \\nProblem \\xa0 X\\xa0 Y\\xa0 Example Citation \\xa0\\nImage captioning \\xa0 Image\\xa0 Text\\xa0 Mao et al., 2014\\xa0\\nMachine translation \\xa0 English text \\xa0 French text\\xa0 Suskever et al., 2014\\xa0\\nQuestion answering \\xa0 (Text,Question) pair \\xa0\\xa0 Answer text \\xa0 Bordes et al., 2015 \\xa0\\nSpeech recognition \\xa0 Audio\\xa0 Transcription \\xa0 Hannun et al., 2015 \\xa0\\nTTS\\xa0 Text features \\xa0 Audio\\xa0 van der Oord et al., 2016\\xa0\\n \\nThis is an accelerating trend in deep learning: When you have the right (input,output)  \\nlabeled pairs, you can sometimes learn end-to-end even when the output is a sentence, an  \\nimage, audio, or other outputs that are richer than a single number.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 104 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n\\xa0\\nError analysis \\nby parts \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 105 Machine Learning Yearning-Draft Andrew Ng  \\n53 Error analysis by parts   \\n \\nSuppose your system is built using a complex machine learning pipeline, and you would like  \\nto improve the system’s performance. Which part of the pipeline should you work on  \\nimproving? By attributing errors to specific parts of the pipeline, you can decide how to  \\nprioritize your work.   \\n \\nLet’s use our Siamese cat classifier example:   \\n \\n \\nThe first part, the cat detector, detects cats and crops them out of the image. The second  \\npart, the cat breed classifier, decides if it is a Siamese cat. It is possible to spend years  \\nworking on improving either of these two pipeline components. How do you decide which  \\ncomponent(s) to focus on?   \\nBy carrying out \\u200berror analysis by parts \\u200b, you can try to attribute each mistake the  \\nalgorithm makes to one (or sometimes both) of the two parts of the pipeline. For example,  \\nthe algorithm misclassifies this image as not containing a Siamese cat (y=0) even though the  \\ncorrect label is y=1.   \\n \\nLet’s manually examine what the two steps of the algorithm did. Suppose the Siamese cat  \\ndetector had detected a cat as follows:   \\nPage 106 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThis means that the cat breed classifier is given the following image:   \\n \\n The cat breed classifier then correctly classifies this image as not containing a Siamese cat.  \\nThus, the cat breed classifier is blameless: It was given of a pile of rocks and outputted a very  \\nreasonable label y=0. Indeed, a human classifying the cropped image above would also have  \\npredicted y=0. Thus, you can clearly attribute this error to the cat detector.   \\nIf, on the other hand, the cat detector had outputted the following bounding box:  \\n \\nthen you would conclude that the cat detector had done its job, and that it was the cat breed  \\nclassifier that is at fault.   \\n \\nSay you go through 100 misclassified dev set images and find that 90 of the errors are  \\nattributable to the cat detector, and only 10 errors are attributable to the cat breed classifier.  \\nYou can safely conclude that you should focus more attention on improving the cat detector.   \\nPage 107 Machine Learning Yearning-Draft Andrew Ng  \\n \\nFurther, you have now also conveniently found 90 examples where the cat detector  \\noutputted incorrect bounding boxes. You can use these 90 examples to carry out a deeper  \\nlevel of error analysis on the cat detector to see how to improve that.   \\n \\nOur description of how you attribute error to one part of the pipeline has been informal so  \\nfar: you look at the output of each of the parts and see if you can decide which one made a  \\nmistake. This informal method could be all you need. But in the next chapter, you’ll also see  \\na more formal way of attributing error.   \\n \\n \\n  \\nPage 108 Machine Learning Yearning-Draft Andrew Ng  \\n54 Attributing error to one part   \\n \\nLet’s continue to use this example:  \\n  \\nSuppose the cat detector outputted this bounding box:   \\n \\n \\n \\nThe cat breed classifier is thus given this cropped image, whereupon it incorrectly outputs  \\ny=0, or that there is no cat in the picture.   \\n \\nThe cat detector did its job poorly. However, a highly skilled human could arguably still  \\nrecognize the Siamese cat from the poorly cropped image. So do we attribute this error to the  \\ncat detector, or the cat breed classifier, or both? It is ambiguous.  \\n \\nIf the number of ambiguous cases like these is small, you can make whatever decision you  \\nwant and get a similar result. But here is a more formal test that lets you more definitively  \\nattribute the error to exactly one part:  \\n \\n1.Replace the cat detector output with a hand-labeled bounding box.  \\nPage 109 Machine Learning Yearning-Draft Andrew Ng  \\n \\n2.Run the corresponding cropped image through the cat breed classifier. If the cat breed  \\nclassifier still misclassifies it, attribute the error to the cat breed classifier. Otherwise,  \\nattribute the error to the cat detector.   \\n \\nIn other words, run an experiment in which you give the cat breed classifier a “perfect” input.  \\nThere are two cases:  \\n \\n●Case 1: Even given a “perfect” bounding box, the cat breed classifier still incorrectly  \\noutputs y=0. In this case, clearly the cat breed classifier is at fault.  \\n●Case 2: Given a “perfect” bounding box, the breed classifier now correctly outputs  \\ny=1. This shows that if only the cat detector had given a more perfect bounding box,  \\nthen the overall system’s output would have been correct. Thus, attribute the error to  \\nthe cat detector.   \\n \\nBy carrying out this analysis on the misclassified dev set images, you can now  \\nunambiguously attribute each error to one component. This allows you to estimate the  \\nfraction of errors due to each component of the pipeline, and therefore decide where to focus  \\nyour attention.  \\n \\n \\n \\n \\n  \\nPage 110 Machine Learning Yearning-Draft Andrew Ng  \\n55 General case of error attribution   \\n \\nHere are the general steps for error attribution. Suppose the pipeline has three steps A, B  \\nand C, where A feeds directly into B, and B feeds directly into C.   \\n \\n \\n \\nFor each mistake the system makes on the dev set:  \\n \\n1.Try manually modifying A’s output to be a “perfect” output (e.g.,  the “perfect”  \\nbounding box for the cat), and run the rest of the pipeline B, C on this output. If the  \\nalgorithm now gives a correct output, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been correct; thus, you can  \\nattribute this error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B’s output to be the “perfect” output for B. If the algorithm  \\nnow gives a correct output, then attribute the error to component B. Otherwise, go on  \\nto Step 3. \\n3.Attribute the error to component C.   \\n \\nLet’s look at a more complex example:  \\nYour self-driving car uses this pipeline. How do you use error analysis by parts to decide  \\nwhich component(s) to focus on?   \\n \\nYou can map the three components to A, B, C as follows:  \\nA: Detect cars  \\nB: Detect pedestrians  \\nC: Plan path for car  \\n \\n \\nPage 111 Machine Learning Yearning-Draft Andrew Ng \\n \\nFollowing the procedure described above, suppose you test out your car on a closed track  \\nand find a case where the car chooses a more jarring steering direction than a skilled driver  \\nwould. In the self-driving world, such a case is usually called a \\u200bscenario \\u200b. You would then:  \\n  \\n1.Try manually modifying A (detecting cars)’s output to be a “perfect” output (e.g.,  \\nmanually go in and tell it where the other cars are). Run the rest of the pipeline B, C as  \\nbefore, but allow C (plan path) to use A’s now perfect output. If the algorithm now  \\nplans a much better path for the car, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been better; Thus, you can attribute  \\nthis error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B (detect pedestrian)’s output to be the “perfect” output for  \\nB. If the algorithm now gives a correct output, then attribute the error to component  \\nB. Otherwise, go on to Step 3.   \\n3.Attribute the error to component C.   \\n \\nThe components of an ML pipeline should be ordered according to a Directed Acyclic Graph  \\n(DAG), meaning that you should be able to compute them in some fixed left-to-right order,  \\nand later components should depend only on earlier components’ outputs. So long as the  \\nmapping of the components to the A->B->C order follows the DAG ordering, then the error  \\nanalysis will be fine. You might get slightly different results if you swap A and B:  \\n \\nA: Detect pedestrians (was previously \\u200bDetect cars \\u200b)  \\nB: Detect cars (was previously \\u200bDetect pedestrians \\u200b) \\nC: Plan path for car  \\n \\nBut the results of this analysis would still be valid and give good guidance for where to focus  \\nyour attention.  \\n \\n \\n  \\nPage 112 Machine Learning Yearning-Draft Andrew Ng  \\n56 Error analysis by parts and comparison to  \\nhuman-level performance   \\n \\nCarrying out error analysis on a learning algorithm is like using data science to analyze an  \\nML system’s mistakes in order to derive insights about what to do next. At its most basic,  \\nerror analysis by parts tells us what component(s) performance is (are) worth the greatest  \\neffort to improve.  \\n \\nSay you have a dataset about customers buying things on a website. A data scientist may  \\nhave many different ways of analyzing the data. She may draw many different conclusions  \\nabout whether the website should raise prices, about the lifetime value of customers acquired  \\nthrough different marketing campaigns, and so on. There is no one “right” way to analyze a  \\ndataset, and there are many possible useful insights one could draw. Similarly, there is no  \\none “right” way to carry out error analysis. Through these chapters you have learned many of  \\nthe most common design patterns for drawing useful insights about your ML system, but you  \\nshould feel free to experiment with other ways of analyzing errors as well.  \\n \\nLet’s return to the self-driving application, where a car detection algorithm outputs the  \\nlocation (and perhaps velocity) of the nearby cars, a pedestrian detection algorithm outputs  \\nthe location of the nearby pedestrians, and these two outputs are finally used to plan a path  \\nfor the car.  \\n \\nTo debug this pipeline, rather than rigorously following the procedure you saw in the  \\nprevious chapter, you could more informally ask:  \\n \\n1.How far is the Detect cars component from human-level performance at detecting  \\ncars?  \\n2.How far is the Detect pedestrians component from human-level performance?  \\nPage 113 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.How far is the overall system’s performance from human-level performance? Here,  \\nhuman-level performance assumes the human has to plan a path for the car given  \\nonly the outputs from the previous two pipeline components (rather than access to  \\nthe camera images). In other words, how does the Plan path component’s  \\nperformance compare to that of a human’s, when the human is given only the same  \\ninput?   \\n \\nIf you find that one of the components is far from human-level performance, you now have a  \\ngood case to focus on improving the performance of that component.   \\n \\nMany error analysis processes work best when we are trying to automate something humans  \\ncan do and can thus benchmark against human-level performance. Most of our preceding  \\nexamples had this implicit assumption. If you are building an ML system where the final  \\noutput or some of the intermediate components are doing things that even humans cannot  \\ndo well, then some of these procedures will not apply.  \\n \\nThis is another advantage of working on problems that humans can solve--you have more  \\npowerful error analysis tools, and thus you can prioritize your team’s work more efficiently.   \\n \\n \\n \\n  \\nPage 114 Machine Learning Yearning-Draft Andrew Ng  \\n57 Spotting a flawed ML pipeline   \\n \\nWhat if each individual component of your ML pipeline is performing at human-level  \\nperformance or near-human-level performance, but the overall pipeline falls far short of  \\nhuman-level? This usually means that the pipeline is flawed and needs to be redesigned.  \\nError analysis can also help you understand if you need to redesign your pipeline.   \\n \\n \\nIn the previous chapter, we posed the question of whether each of the three components’  \\nperformance is at human level. Suppose the answer to all three questions is yes. That is:  \\n \\n1.The Detect cars component is at (roughly) human-level performance for detecting  \\ncars from the camera images.  \\n2.The Detect pedestrians component is at (roughly) human-level performance for  \\ndetecting cars from the camera images.   \\n3.Compared to a human that has to plan a path for the car given only the outputs  \\nfrom the previous two pipeline components (rather than access to the camera  \\nimages), \\u200b the Plan path component’s performance is at a similar level.   \\n \\nHowever, your overall self-driving car is performing significantly below human-level  \\nperformance. I.e., humans given access to the camera images can plan significantly better  \\npaths for the car. What conclusion can you draw?   \\n \\nThe only possible conclusion is that the ML pipeline is flawed. In this case, the Plan path  \\ncomponent is doing as well as it can \\u200bgiven its inputs \\u200b, but the inputs do not contain enough  \\ninformation. You should ask yourself what other information, other than the outputs from  \\nthe two earlier pipeline components, is needed to plan paths very well for a car to drive. In  \\nother words, what other information does a skilled human driver need?   \\n \\nPage 115 Machine Learning Yearning-Draft Andrew Ng  \\nFor example, suppose you realize that a human driver also needs to know the location of the  \\nlane markings. This suggests that you should redesign the pipeline as follows : 18\\n \\n \\n \\n \\nUltimately, if you don’t think your pipeline as a whole will achieve human-level performance,  \\neven if every individual component has human-level performance (remember that you are  \\ncomparing to a human who is given the same input as the component), then the pipeline is  \\nflawed and should be redesigned.  \\n  \\n18 In the self-driv ing example abov e, in theory one could solv e this problem by also feeding the raw camera  \\nimage into the planning component.  Howev er, this would v iolate the design principle of “Task  simplicity”  \\ndescribed in Chapter 51,  because the path planning module now needs to input a raw image and has a v ery \\ncomplex task  to solv e. That’ s why adding a Detect lane markings component  is a better choice--it helps get the \\nimportant and prev iously missing information about lane markings to the path planning module,  but you av oid \\nmak ing any particular module ov erly complex to build/train.   \\n \\nPage 116 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xa0\\nConclusion \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 117 Machine Learning Yearning-Draft Andrew Ng  \\n58 Building a superhero team - Get your  \\nteammates to read this  \\n \\nCongratulations on finishing this book!   \\nIn Chapter 2, we talked about how this book can help you become the superhero of your  \\nteam.  \\n \\nThe only thing better than being a superhero is being part of a superhero team. I hope you’ll  \\ngive copies of this book to your friends and teammates and help create other superheroes!  \\n \\n  \\nPage 118 Machine Learning Yearning-Draft Andrew Ng \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    chunk_size = 10000,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n \\n \\n \\n \\n \\n \\nMachine L earning Year ning is a   \\ndeeplear ning. ai pr oject. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n© 2018 Andrew Ng.  All Rights R eserv ed. \\n \\n  \\nPage 2 Machine Learning Yearning-Draft Andrew Ng Deeplearning.AI \\nTable of Contents \\n \\n1 Why Machine Learning Strategy  \\n2 How to use this book to help your team  \\n3 Prerequisites and Notation  \\n4 Scale drives machine learning progress  \\n5 Your development and test sets  \\n6 Your dev and test sets should come from the same distribution  \\n7 How large do the dev/test sets need to be?  \\n8 Establish a single-number evaluation metric for your team to optimize  \\n9 Optimizing and satisficing metrics  \\n10 Having a dev set and metric speeds up iterations  \\n11 When to change dev/test sets and metrics  \\n12 Takeaways: Setting up development and test sets  \\n13 Build your first system quickly, then iterate  \\n14 Error analysis: Look at dev set examples to evaluate ideas  \\n15 Evaluating multiple ideas in parallel during error analysis  \\n16 Cleaning up mislabeled dev and test set examples  \\n17 If you have a large dev set, split it into two subsets, only one of which you look at  \\n18 How big should the Eyeball and Blackbox dev sets be?  \\n19 Takeaways: Basic error analysis  \\n20 Bias and Variance: The two big sources of error  \\n21 Examples of Bias and Variance  \\n22 Comparing to the optimal error rate  \\n23 Addressing Bias and Variance  \\n24 Bias vs. Variance tradeoff  \\n25 Techniques for reducing avoidable bias  \\nPage 3 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n27 Techniques for reducing variance  \\n28 Diagnosing bias and variance: Learning curves  \\n29 Plotting training error  \\n30 Interpreting learning curves: High bias  \\n31 Interpreting learning curves: Other cases  \\n32 Plotting learning curves  \\n33 Why we compare to human-level performance  \\n34 How to define human-level performance  \\n35 Surpassing human-level performance  \\n36 When you should train and test on different distributions  \\n37 How to decide whether to use all your data  \\n38 How to decide whether to include inconsistent data  \\n39 Weighting data  \\n40 Generalizing from the training set to the dev set  \\n41 Identifying Bias, Variance, and Data Mismatch Errors  \\n42 Addressing data mismatch  \\n43 Artificial data synthesis  \\n44 The Optimization Verification test  \\n45 General form of Optimization Verification test  \\n46 Reinforcement learning example  \\n47 The rise of end-to-end learning  \\n48 More end-to-end learning examples  \\n49 Pros and cons of end-to-end learning  \\n50 Choosing pipeline components: Data availability  \\n51 Choosing pipeline components: Task simplicity  \\nPage 4 Machine Learning Yearning-Draft Andrew Ng  \\n52 Directly learning rich outputs  \\n53 Error analysis by parts  \\n54 Attributing error to one part  \\n55 General case of error attribution  \\n56 Error analysis by parts and comparison to human-level performance  \\n57 Spotting a flawed ML pipeline  \\n58 Building a superhero team - Get your teammates to read this  \\n \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 5 Machine Learning Yearning-Draft Andrew Ng  \\n1 Why Machine Learning Strategy  \\n \\nMachine learning is the foundation of countless important applications, including web  \\nsearch, email anti-spam, speech recognition, product recommendations, and more. I assume  \\nthat you or your team is working on a machine learning application, and that you want to  \\nmake rapid progress. This book will help you do so.   \\nExample:  Building a cat picture startup  \\nSay you’re building a startup that will provide an endless stream of cat pictures to cat lovers.  \\nYou use a neural network to build a computer vision system for detecting cats in pictures.  \\nBut tragically, your learning algorithm’s accuracy is not yet good enough. You are under  \\ntremendous pressure to improve your cat detector. What do you do?   \\nYour team has a lot of ideas, such as:  \\n•Get more data: Collect more pictures of cats.   \\n•Collect a more diverse training set. For example, pictures of cats in unusual positions; cats  \\nwith unusual coloration; pictures shot with a variety of camera settings; ….   \\n•Train the algorithm longer, by running more gradient descent iterations.  \\n•Try a bigger neural network, with more layers/hidden units/parameters.   \\nPage 6 Machine Learning Yearning-Draft Andrew Ng \\n \\n•Try a smaller neural network. \\n•Try adding regularization (such as L2 regularization).  \\n•Change the neural network architecture (activation function, number of hidden units, etc.)  \\n•…  \\nIf you choose well among these possible directions, you’ll build the leading cat picture  \\nplatform, and lead your company to success. If you choose poorly, you might waste months.  \\nHow do you proceed?   \\nThis book will tell you how. Most machine learning problems leave clues that tell you what’s  \\nuseful to try, and what’s not useful to try. Learning to read those clues will save you months  \\nor years of development time.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 7 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n2 How to use this book to help your team  \\n \\nAfter finishing this book, you will have a deep understanding of how to set technical  \\ndirection for a machine learning project.   \\nBut your teammates might not understand why you’re recommending a particular direction.  \\nPerhaps you want your team to define a single-number evaluation metric, but they aren’t  \\nconvinced. How do you persuade them?   \\nThat’s why I made the chapters short: So that you can print them out and get your  \\nteammates to read just the 1-2 pages you need them to know.   \\nA few changes in prioritization can have a huge effect on your team’s productivity. By helping  \\nyour team with a few such changes, I hope that you can become the superhero of your team!   \\n \\n \\n \\xa0\\nPage 8 Machine Learning Yearning-Draft Andrew Ng \\n \\n3 Prerequisites and Notation  \\n \\nIf you have taken a Machine Learning course such as my machine learning MOOC on  \\nCoursera, or if you have experience applying supervised learning, you will be able to  \\nunderstand this text.   \\nI assume you are familiar with \\u200bsupervised learning \\u200b: learning a function that maps from x  \\nto y, using labeled training examples (x,y). Supervised learning algorithms include linear  \\nregression, logistic regression, and neural networks. There are many forms of machine  \\nlearning, but the majority of Machine Learning’s practical value today comes from  \\nsupervised learning.   \\nI will frequently refer to neural networks (also known as “deep learning”). You’ll only need a  \\nbasic understanding of what they are to follow this text.   \\nIf you are not familiar with the concepts mentioned here, watch the first three weeks of  \\nvideos in the Machine Learning course on Coursera at \\u200bhttp://ml-class.org  \\n  \\nPage 9 Machine Learning Yearning-Draft Andrew Ng  \\n4 Scale drives machine learning progress  \\n \\nMany of the ideas of deep learning (neural networks) have been around for decades. Why are  \\nthese ideas taking off now?   \\nTwo of the biggest drivers of recent progress have been:  \\n•Data availability. \\u200b People are now spending more time on digital devices (laptops, mobile  \\ndevices). Their digital activities generate huge amounts of data that we can feed to our  \\nlearning algorithms.  \\n•Computational scale. \\u200bWe started just a few years ago to be able to train neural  \\nnetworks that are big enough to take advantage of the huge datasets we now have.   \\nIn detail, even as you accumulate more data, usually the performance of older learning  \\nalgorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens  \\nout,” and the algorithm stops improving even as you give it more data:   \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIt was as if the older algorithms didn’t know what to do with all the data we now have.   \\nIf you train a small neutral network (NN) on the same supervised learning task, you might  \\nget slightly better performance:   \\n \\n \\n \\nPage 10 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nHere, by “Small NN” we mean a neural network with only a small number of hidden  \\nunits/layers/parameters. Finally, if you train larger and larger neural networks, you can  \\nobtain even better performance:  1\\n \\nThus, you obtain the best performance when you (i) Train a very large neural network, so  \\nthat you are on the green curve above; (ii) Have a huge amount of data.   \\nMany other details such as neural network architecture are also important, and there has  \\nbeen much innovation here. But one of the more reliable ways to improve an algorithm’s  \\nperformance today is still to (i) train a bigger network and (ii) get more data.   \\n1 This diagram shows NNs doing better in the r egime of small datasets.  This effect is less consistent \\nthan the effect of NNs doing well in the r egime of huge datasets.  In the small data regime,  depending  \\non how the features ar e hand-engineer ed, traditional algor ithms may or  may not do better.  For \\nexample,  if you hav e 20 training examples,  it might not matter  much whether you use logist ic \\nregr ession or  a neur al network ; the hand-engineering of featur es will hav e a bigger effect than the  \\nchoice of algorithm.  But if you hav e 1 million examples,  I would fav or the neur al network .  \\nPage 11 Machine Learning Yearning-Draft Andrew Ng  \\nThe process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss  \\nthe details at length. We will start with general strategies that are useful for both traditional  \\nlearning algorithms and neural networks, and build up to the most modern strategies for  \\nbuilding deep learning systems.   \\nPage 12 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSetting up \\ndevelopment and \\ntest sets  \\nPage 13 Machine Learning Yearning-Draft Andrew Ng  \\n5 Your development and test sets  \\n \\nLet’s return to our earlier cat pictures example: You run a mobile app, and users are  \\nuploading pictures of many different things to your app. You want to automatically find the  \\ncat pictures.   \\nYour team gets a large training set by downloading pictures of cats (positive examples) and  \\nnon-cats (negative examples) off of different websites. They split the dataset 70%/30% into  \\ntraining and test sets. Using this data, they build a cat detector that works well on the  \\ntraining and test sets.   \\nBut when you deploy this classifier into the mobile app, you find that the performance is  \\nreally poor!   \\n         \\nWhat happened?  \\nYou figure out that the pictures users are uploading have a different look than the website  \\nimages that make up your training set: Users are uploading pictures taken with mobile  \\nphones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test  \\nsets were made of website images, your algorithm did not generalize well to the actual  \\ndistribution you care about: mobile phone pictures.   \\nBefore the modern era of big data, it was a common rule in machine learning to use a  \\nrandom 70%/30% split to form your training and test sets. This practice can work, but it’s a  \\nbad idea in more and more applications where the training distribution (website images in  \\nPage 14 Machine Learning Yearning-Draft Andrew Ng  \\nour example above) is different from the distribution you ultimately care about (mobile  \\nphone images).  \\nWe usually define:  \\n•Training set \\u200b — Which you run your learning algorithm on.  \\n•Dev (development) set \\u200b — Which you use to tune parameters, select features, and  \\nmake other decisions regarding the learning algorithm. Sometimes also called the  \\nhold-out cross validation set \\u200b.  \\n•Test set\\u200b — which you use to evaluate the performance of the algorithm, but not to make  \\nany decisions regarding what learning algorithm or parameters to use.   \\nOnce you define a dev set (development set) and test set, your team will try a lot of ideas,  \\nsuch as different learning algorithm parameters, to see what works best. The dev and test  \\nsets allow your team to quickly see how well your algorithm is doing.   \\nIn other words, \\u200bthe purpose of the dev and test sets are to direct your team toward  \\nthe most important changes to make to the machine learning system \\u200b.  \\nSo, you should do the following:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nIn other words, your test set should not simply be 30% of the available data, especially if you  \\nexpect your future data (mobile phone images) to be different in nature from your training  \\nset (website images).   \\nIf you have not yet launched your mobile app, you might not have any users yet, and thus  \\nmight not be able to get data that accurately reflects what you have to do well on in the  \\nfuture. But you might still try to approximate this. For example, ask your friends to take  \\nmobile phone pictures of cats and send them to you. Once your app is launched, you can  \\nupdate your dev/test sets using actual user data.   \\nIf you really don’t have any way of getting data that approximates what you expect to get in  \\nthe future, perhaps you can start by using website images. But you should be aware of the  \\nrisk of this leading to a system that doesn’t generalize well.   \\nIt requires judgment to decide how much to invest in developing great dev and test sets. But  \\ndon’t assume your training distribution is the same as your test distribution. Try to pick test  \\nPage 15 Machine Learning Yearning-Draft Andrew Ng  \\nexamples that reflect what you ultimately want to perform well on, rather than whatever data  \\nyou happen to have for training.   \\n \\n  \\nPage 16 Machine Learning Yearning-Draft Andrew Ng  \\n6 Your dev and test sets should come from the  \\nsame distribution  \\nYou have your cat app image data segmented into four regions, based on your largest  \\nmarkets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a dev set and a test  \\nset, say we put US and India in the dev set; China and Other in the test set. In other words,  \\nwe can randomly assign two of these segments to the dev set, and the other two to the test  \\nset, right?  \\nOnce you define the dev and test sets, your team will be focused on improving dev set  \\nperformance. Thus, the dev set should reflect the task you want to improve on the most: To  \\ndo well on all four geographies, and not only two.   \\nThere is a second problem with having different dev and test set distributions: There is a  \\nchance that your team will build something that works well on the dev set, only to find that it  \\ndoes poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid  \\nletting this happen to you.   \\nAs an example, suppose your team develops a system that works well on the dev set but not  \\nthe test set. If your dev and test sets had come from the same distribution, then you would  \\nhave a very clear diagnosis of what went wrong: You have overfit the dev set. The obvious  \\ncure is to get more dev set data.   \\nBut if the dev and test sets come from different distributions, then your options are less  \\nclear. Several things could have gone wrong:   \\n1.You had overfit to the dev set.   \\n2.The test set is harder than the dev set. So your algorithm might be doing as well as could  \\nbe expected, and no further significant improvement is possible.   \\nPage 17 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.The test set is not necessarily harder, but just different, from the dev set. So what works  \\nwell on the dev set just does not work well on the test set. In this case, a lot of your work  \\nto improve dev set performance might be wasted effort.   \\nWorking on machine learning applications is hard enough. Having mismatched dev and test  \\nsets introduces additional uncertainty about whether improving on the dev set distribution  \\nalso improves test set performance. Having mismatched dev and test sets makes it harder to  \\nfigure out what is and isn’t working, and thus makes it harder to prioritize what to work on.   \\nIf you are working on a 3rd party benchmark problem, their creator might have specified dev  \\nand test sets that come from different distributions. Luck, rather than skill, will have a  \\ngreater impact on your performance on such benchmarks compared to if the dev and test  \\nsets come from the same distribution. It is an important research problem to develop  \\nlearning algorithms that are trained on one distribution and generalize well to another. But if  \\nyour goal is to make progress on a specific machine learning application rather than make  \\nresearch progress, I  recommend trying to choose dev and test sets that are drawn from the  \\nsame distribution. This will make your team more efficient.   \\n \\n \\n \\n \\xa0\\nPage 18 Machine Learning Yearning-Draft Andrew Ng  \\n7 How large do the dev/test sets need to be?  \\n \\nThe dev set should be large enough to detect differences between algorithms that you are  \\ntrying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an  \\naccuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1%  \\ndifference. Compared to other machine learning problems I’ve seen, a 100 example dev set is  \\nsmall. Dev sets with sizes from 1,000 to 10,000 examples are common. With 10,000  \\nexamples, you will have a good chance of detecting an improvement of 0.1%.  2\\nFor mature and important applications—for example, advertising, web search, and product  \\nrecommendations—I have also seen teams that are highly motivated to eke out even a 0.01%  \\nimprovement, since it has a direct impact on the company’s profits. In this case, the dev set  \\ncould be much larger than 10,000, in order to detect even smaller improvements.   \\nHow about the size of the test set? It should be large enough to give high confidence in the  \\noverall performance of your system. One popular heuristic had been to use 30% of your data  \\nfor your test set. This works well when you have a modest number of examples—say 100 to  \\n10,000 examples. But in the era of big data where we now have machine learning problems  \\nwith sometimes more than a billion examples, the fraction of data allocated to dev/test sets  \\nhas been shrinking, even as the absolute number of examples in the dev/test sets has been  \\ngrowing. There is no need to have excessively large dev/test sets beyond what is needed to  \\nevaluate the performance of your algorithms.   \\n2 In theory,  one could also test if a change to an algorithm mak es a statistically significant difference \\non the dev  set. In pr actice,  most teams don’t bother  with this (unless they are publishing academic  \\nresearch papers),  and I usually do not find statistical significance tests useful for measuring inter im \\nprogr ess.  \\nPage 19 Machine Learning Yearning-Draft Andrew Ng  \\n8 Establish a single-number evaluation metric  \\nfor your team to optimize   \\n \\nClassification accuracy is an example of a \\u200bsingle-number evaluation metric \\u200b: You run \\nyour classifier on the dev set (or test set), and get back a single number about what fraction  \\nof examples it classified correctly. According to this metric, if classifier A obtains 97%  \\naccuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.  \\nIn contrast, Precision and Recall  is not a single-number evaluation metric: It gives two  3\\nnumbers for assessing your classifier. Having multiple-number evaluation metrics makes it  \\nharder to compare algorithms. Suppose your algorithms perform as follows:   \\n \\nClassifier \\xa0 Precision \\xa0 Recall\\xa0\\nA\\xa0 95% 90% \\nB\\xa0 98% 85% \\n \\nHere, neither classifier is obviously superior, so it doesn’t immediately guide you toward  \\npicking one. \\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\n \\nDuring development, your team will try a lot of ideas about algorithm architecture, model  \\nparameters, choice of features, etc. Having a \\u200bsingle-number evaluation metric \\u200b such as  \\naccuracy allows you to sort all your models according to their performance on this metric,  \\nand quickly decide what is working best.   \\nIf you really care about both Precision and Recall, I recommend using one of the standard  \\nways to combine them into a single number. For example, one could take the average of  \\nprecision and recall, to end up with a single number.  Alternatively, you can compute the “F1  \\n3 The Precision of a cat classifier  is the fraction of images in the dev  (or test) set it labeled as cats that \\nreally are cats.  Its Recall is the percentage of all cat images in the dev  (or test) set that it cor rectly  \\nlabeled as a cat.  There is often a tradeoff between hav ing high pr ecision and high recall.   \\nPage 20 Machine Learning Yearning-Draft Andrew Ng  \\nscore,” which is a modified way of computing their average, and works better than simply  \\ntaking the mean. 4\\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\nB\\xa0 98% 85% 91.0%  \\n \\nHaving a single-number evaluation metric speeds up your ability to make a decision when  \\nyou are selecting among a large number of classifiers. It gives a clear preference ranking  \\namong all of them, and therefore a clear direction for progress.   \\nAs a final example, suppose you are separately tracking the accuracy of your cat classifier in  \\nfour key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By  \\ntaking an average or weighted average of these four numbers, you end up with a single  \\nnumber metric. Taking an average or weighted average is one of the most common ways to  \\ncombine multiple metrics into one.   \\n \\n \\n \\xa0\\n4 If you want to learn more about the F1  scor e, see \\u200bhttps: //en. wikipedia. org/wik i/F1_score\\u200b.  It is the  \\n“harmonic mean” between Precision and R ecall,  and is calculated as 2 /((1/Pr ecision)+(1/R ecall)).   \\nPage 21 Machine Learning Yearning-Draft Andrew Ng  \\n9 Optimizing and satisficing metrics   \\n \\nHere’s another way to combine multiple evaluation metrics.   \\nSuppose you care about both the accuracy and the running time of a learning algorithm. You  \\nneed to choose from these three classifiers:   \\nClassifier \\xa0 Accuracy \\xa0 Running time\\xa0\\nA\\xa0 90%\\xa0 80ms\\xa0\\nB\\xa0 92% 95ms\\xa0\\nC\\xa0 95% 1,500ms \\xa0\\n \\nIt seems unnatural to derive a single metric by putting accuracy and running time into a  \\nsingle formula, such as:  \\nAccuracy - 0.5*RunningTime \\nHere’s what you can do instead: First, define what is an “acceptable” running time. Lets say  \\nanything that runs in 100ms is acceptable. Then, maximize accuracy, subject to your  \\nclassifier meeting the running time criteria. Here, running time is a “satisficing  \\nmetric”—your classifier just has to be “good enough” on this metric, in the sense that it  \\nshould take at most 100ms. Accuracy is the “optimizing metric.”  \\nIf you are trading off N different criteria, such as binary file size of the model (which is  \\nimportant for mobile apps, since users don’t want to download large apps), running time,  \\nand accuracy, you might consider setting N-1 of the criteria as “satisficing” metrics. I.e., you  \\nsimply require that they meet a certain value. Then define the final one as the “optimizing”  \\nmetric. For example, set a threshold for what is acceptable for binary file size and running  \\ntime, and try to optimize accuracy given those constraints.  \\nAs a final example, suppose you are building a hardware device that uses a microphone to  \\nlisten for the user saying a particular “wakeword,” that then causes the system to wake up.  \\nExamples include Amazon Echo listening for “Alexa”; Apple Siri listening for “Hey Siri”;  \\nAndroid listening for “Okay Google”; and Baidu apps listening for “Hello Baidu.” You care  \\nabout both the false positive rate—the frequency with which the system wakes up even when  \\nno one said the wakeword—as well as the false negative rate—how often it fails to wake up  \\nwhen someone says the wakeword. One reasonable goal for the performance of this system is  \\nPage 22 Machine Learning Yearning-Draft Andrew Ng  \\nto minimize the false negative rate (optimizing metric), subject to there being no more than  \\none false positive every 24 hours of operation (satisficing metric).   \\nOnce your team is aligned on the evaluation metric to optimize, they will be able to make  \\nfaster progress.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 23 Machine Learning Yearning-Draft Andrew Ng  \\n10 Having a dev set and metric speeds up  \\niterations \\n \\nIt is very difficult to know in advance what approach will work best for a new problem. Even  \\nexperienced machine learning researchers will usually try out many dozens of ideas before  \\nthey discover something satisfactory. When building a machine learning system, I will often:   \\n1.Start off with some \\u200bidea\\u200b on how to build the system.   \\n2.Implement the idea in \\u200bcode\\u200b.  \\n3.Carry out an \\u200bexperiment \\u200b which tells me how well the idea worked. (Usually my first few  \\nideas don’t work!) Based on these learnings, go back to generate more ideas, and keep on  \\niterating.  \\nThis is an iterative process. The faster you can go round this loop, the faster you will make  \\nprogress. This is why having dev/test sets and a metric are important: Each time you try an  \\nidea, measuring your idea’s performance on the dev set lets you quickly decide if you’re  \\nheading in the right direction.   \\nIn contrast, suppose you don’t have a specific dev set and metric. So each time your team  \\ndevelops a new cat classifier, you have to incorporate it into your app, and play with the app  \\nfor a few hours to get a sense of whether the new classifier is an improvement. This would be  \\nincredibly slow! Also, if your team improves the classifier’s accuracy from 95.0% to 95.1%,  \\nyou might not be able to detect that 0.1% improvement from playing with the app. Yet a lot  \\nof progress in your system will be made by gradually accumulating dozens of these 0.1%  \\nimprovements. Having a dev set and metric allows you to very quickly detect which ideas are  \\nsuccessfully giving you small (or large) improvements, and therefore lets you quickly decide  \\nwhat ideas to keep refining, and which ones to discard.   \\n \\xa0\\nPage 24 Machine Learning Yearning-Draft Andrew Ng \\n \\n11 When to change dev/test sets and metrics   \\n \\nWhen starting out on a new project, I try to quickly choose dev/test sets, since this gives the  \\nteam a well-defined target to aim for.   \\nI typically ask my teams to come up with an initial dev/test set and an initial metric in less  \\nthan one week—rarely longer. It is better to come up with something imperfect and get going  \\nquickly, rather than overthink this. But this one week timeline does not apply to mature  \\napplications. For example, anti-spam is a mature deep learning application. I have seen  \\nteams working on already-mature systems spend months to acquire even better dev/test  \\nsets.  \\nIf you later realize that your initial dev/test set or metric missed the mark, by all means  \\nchange them quickly. For example, if your dev set + metric ranks classifier A above classifier  \\nB, but your team thinks that classifier B is actually superior for your product, then this might  \\nbe a sign that you need to change your dev/test sets or your evaluation metric.   \\nThere are three main possible causes of the dev set/metric incorrectly rating classifier A  \\nhigher:  \\n1. The actual distribution you need to do well on is different from the dev/test sets.  \\nSuppose your initial dev/test set had mainly pictures of adult cats. You ship your cat app,  \\nand find that users are uploading a lot more kitten images than expected. So, the dev/test set  \\ndistribution is not representative of the actual distribution you need to do well on. In this  \\ncase, update your dev/test sets to be more representative.   \\n \\nPage 25 Machine Learning Yearning-Draft Andrew Ng \\n \\n2. You have overfit to the dev set.  \\nThe process of repeatedly evaluating ideas on the dev set causes your algorithm to gradually  \\n“overfit” to the dev set. When you are done developing, you will evaluate your system on the  \\ntest set. If you find that your dev set performance is much better than your test set  \\nperformance, it is a sign that you have overfit to the dev set. In this case, get a fresh dev set.   \\nIf you need to track your team’s progress, you can also evaluate your system regularly—say  \\nonce per week or once per month—on the test set. But do not use the test set to make any  \\ndecisions regarding the algorithm, including whether to roll back to the previous week’s  \\nsystem. If you do so, you will start to overfit to the test set, and can no longer count on it to  \\ngive a completely unbiased estimate of your system’s performance (which you would need if  \\nyou’re publishing research papers, or perhaps using this metric to make important business  \\ndecisions).  \\n3. The metric is measuring something other than what the project needs to optimize.  \\nSuppose that for your cat application, your metric is classification accuracy. This metric  \\ncurrently ranks classifier A as superior to classifier B. But suppose you try out both  \\nalgorithms, and find classifier A is allowing occasional pornographic images to slip through.  \\nEven though classifier A is more accurate, the bad impression left by the occasional  \\npornographic image means its performance is unacceptable. What do you do?   \\nHere, the metric is failing to identify the fact that Algorithm B is in fact better than  \\nAlgorithm A for your product. So, you can no longer trust the metric to pick the best  \\nalgorithm. It is time to change evaluation metrics. For example, you can change the metric to  \\nheavily penalize letting through pornographic images.  I would strongly recommend picking  \\na new metric and using the new metric to explicitly define a new goal for the team, rather  \\nthan proceeding for too long without a trusted metric and reverting to manually choosing  \\namong classifiers.  \\n \\nIt is quite common to change dev/test sets or evaluation metrics during a project. Having an  \\ninitial dev/test set and metric helps you iterate quickly. If you ever find that the dev/test sets  \\nor metric are no longer pointing your team in the right direction, it’s not a big deal! Just  \\nchange them and make sure your team knows about the new direction.   \\n \\n \\xa0\\nPage 26 Machine Learning Yearning-Draft Andrew Ng  \\n12 Takeaways: Setting up development and  \\ntest sets  \\n \\n•Choose dev and test sets from a distribution that reflects what data you expect to get in  \\nthe future and want to do well on. This may not be the same as your training data’s  \\ndistribution.   \\n•Choose dev and test sets from the same distribution if possible.   \\n•Choose a single-number evaluation metric for your team to optimize. If there are multiple  \\ngoals that you care about, consider combining them into a single formula (such as  \\naveraging multiple error metrics) or defining satisficing and optimizing metrics.   \\n•Machine learning is a highly iterative process: You may try many dozens of ideas before  \\nfinding one that you’re satisfied with.   \\n•Having dev/test sets and a single-number evaluation metric helps you quickly evaluate  \\nalgorithms, and therefore iterate faster.   \\n•When starting out on a brand new application, try to establish dev/test sets and a metric  \\nquickly, say in less than a week. It might be okay to take longer on mature applications.   \\n•The old heuristic of a 70%/30% train/test split does not apply for problems where you  \\nhave a lot of data; the dev and test sets can be much less than 30% of the data.   \\n•Your dev set should be large enough to detect meaningful changes in the accuracy of your  \\nalgorithm, but not necessarily much larger. Your test set should be big enough to give you  \\na confident estimate of the final performance of your system.   \\n•If your dev set and metric are no longer pointing your team in the right direction, quickly  \\nchange them: (i) If you had overfit the dev set, get more dev set data. (ii) If the actual  \\ndistribution you care about is different from the dev/test set distribution, get new  \\ndev/test set data. (iii) If your metric is no longer measuring what is most important to  \\nyou, change the metric.   \\n \\n \\n \\nPage 27 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\nBasic Error \\nAnalysis \\n\\xa0\\n\\xa0 \\xa0\\nPage 28 Machine Learning Yearning-Draft Andrew Ng  \\n13 Build your first system quickly, then iterate  \\n \\nYou want to build a new email anti-spam system. Your team has several ideas:   \\n•Collect a huge training set of spam email. For example, set up a “honeypot”: deliberately  \\nsend fake email addresses to known spammers, so that you can automatically harvest the  \\nspam messages they send to those addresses.  \\n•Develop features for understanding the text content of the email.   \\n•Develop features for understanding the email envelope/header features to show what set  \\nof internet servers the message went through.   \\n•and more.  \\nEven though I have worked extensively on anti-spam, I would still have a hard time picking  \\none of these directions. It is even harder if you are not an expert in the application area.   \\nSo don’t start off trying to design and build the perfect system. Instead, build and train a  \\nbasic system quickly—perhaps in just a few days.  Even if the basic system is far from the  5\\n“best” system you can build, it is valuable to examine how the basic system functions: you  \\nwill  quickly find clues that show you the most promising directions in which to invest your  \\ntime. These next few chapters will show you how to read these clues.  \\n \\n\\xa0\\n\\xa0 \\xa0\\n5 This adv ice is meant for readers wanting to build AI applications,  rather  than those whose goal is to  \\npublish academic papers.  I will later  return to the topic of doing research.   \\nPage 29 Machine Learning Yearning-Draft Andrew Ng  \\n14 Error analysis: Look at dev set examples to  \\nevaluate ideas  \\nWhen you play with your cat app, you notice several examples where it mistakes dogs for  \\ncats. Some dogs do look like cats!   \\nA team member proposes incorporating 3rd party software that will make the system do  \\nbetter on dog images. These changes will take a month, and the team member is  \\nenthusiastic. Should you ask them to go ahead?   \\nBefore investing a month on this task, I recommend that you first estimate how much it will  \\nactually improve the system’s accuracy. Then you can more rationally decide if this is worth  \\nthe month of development time, or if you’re better off using that time on other tasks.   \\nIn detail, here’s what you can do:   \\n1.Gather a sample of 100 dev set examples that your system \\u200bmisclassified \\u200b. I.e., examples  \\nthat your system made an error on.   \\n2.Look at these examples manually, and count what fraction of them are dog images.   \\nThe process of looking at misclassified examples is called \\u200berror analysis \\u200b. In this example, if \\nyou find that only 5% of the misclassified images are dogs, then no matter how much you  \\nimprove your algorithm’s performance on dog images, you won’t get rid of more than 5% of  \\nyour errors. In other words, 5% is a “ceiling” (meaning maximum possible amount) for how  \\nmuch the proposed project could help. Thus, if your overall system is currently 90% accurate  \\n(10% error), this improvement is likely to result in at best 90.5% accuracy (or 9.5% error,  \\nwhich is 5% less error than the original 10% error).   \\nPage 30 Machine Learning Yearning-Draft Andrew Ng \\n \\nIn contrast, if you find that 50% of the mistakes are dogs, then you can be more confident  \\nthat the proposed project will have a big impact. It could boost accuracy from 90% to 95% (a  \\n50% relative reduction in error, from 10% down to 5%).   \\nThis simple counting procedure of error analysis gives you a quick way to estimate the  \\npossible value of incorporating the 3rd party software for dog images. It provides a  \\nquantitative basis on which to decide whether to make this investment.   \\nError analysis can often help you figure out how promising different directions are. I’ve seen  \\nmany engineers reluctant to carry out error analysis. It often feels more exciting to just jump  \\nin and implement some idea, rather than question if the idea is worth the time investment.  \\nThis is a common mistake: It might result in your team spending a month only to realize  \\nafterward that it resulted in little benefit.   \\nManually examining 100 examples does not take long. Even if you take one minute per  \\nimage, you’d be done in under two hours. These two hours could save you a month of wasted  \\neffort.  \\nError Analysis \\u200b refers to the process of examining dev set examples that your algorithm  \\nmisclassified, so that you can understand the underlying causes of the errors. This can help  \\nyou prioritize projects—as in this example—and inspire new directions, which we will discuss  \\nnext. The next few chapters will also present best practices for carrying out error analyses.   \\n \\n \\n  \\nPage 31 Machine Learning Yearning-Draft Andrew Ng  \\n15 Evaluating multiple ideas in parallel during  \\nerror analysis \\n \\nYour team has several ideas for improving the cat detector:  \\n•Fix the problem of your algorithm recognizing \\u200bdogs\\u200b as cats. \\n•Fix the problem of your algorithm recognizing \\u200bgreat cats \\u200b (lions, panthers, etc.) as house  \\ncats (pets).   \\n•Improve the system’s performance on \\u200bblurry\\u200b images.  \\n•… \\nYou can efficiently evaluate all of these ideas in parallel. I usually create a spreadsheet and  \\nfill it out while looking through ~100 misclassified dev set images. I also jot down comments  \\nthat might help me remember specific examples. To illustrate this process, let’s look at a  \\nspreadsheet you might produce with a small dev set of four examples:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments \\xa0\\n1 ✔ \\xa0   Unusual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken at \\xa0\\nzoo on rainy day\\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n% of total \\xa0 25%\\xa0 50%\\xa0 50%\\xa0  \\n \\nImage #3 above has both the Great Cat and the Blurry columns checked. Furthermore,  \\nbecause it is possible for one example to be associated with multiple categories, the  \\npercentages at the bottom may not add up to 100%.   \\nAlthough you may first formulate the categories (Dog, Great cat, Blurry) then categorize the  \\nexamples by hand, in practice, once you start looking through examples, you will probably be  \\ninspired to propose new error categories. For example, say you go through a dozen images  \\nand realize a lot of mistakes occur with Instagram-filtered pictures. You can go back and add  \\na new “Instagram” column to the spreadsheet. Manually looking at examples that the  \\nalgorithm misclassified and asking how/whether you as a human could have labeled the  \\nPage 32 Machine Learning Yearning-Draft Andrew Ng  \\npicture correctly will often inspire you to come up with new categories of errors and  \\nsolutions.  \\nThe most helpful error categories will be ones that you have an idea for improving. For  \\nexample, the Instagram category will be most helpful to add if you have an idea to “undo”  \\nInstagram filters and recover the original image. But you don’t have to restrict yourself only  \\nto error categories you know how to improve; the goal of this process is to build your  \\nintuition about the most promising areas to focus on.   \\nError analysis is an iterative process. Don’t worry if you start off with no categories in mind.  \\nAfter looking at a couple of images, you might come up with a few ideas for error categories.  \\nAfter manually categorizing some images, you might think of  new categories and re-examine  \\nthe images in light of the new categories, and so on.   \\nSuppose you finish carrying out error analysis on 100 misclassified dev set examples and get  \\nthe following:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments\\xa0\\n1 ✔ \\xa0   Usual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken \\xa0\\nat zoo on rainy day \\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n…\\xa0 …\\xa0 …\\xa0 …\\xa0 ...\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0  \\n \\nYou now know that working on a project to address the Dog mistakes can eliminate 8% of  \\nthe errors at most. Working on Great Cat or Blurry image errors could help eliminate more  \\nerrors. Therefore, you might pick one of the two latter categories to focus on. If your team  \\nhas enough people to pursue multiple directions in parallel, you can also ask some engineers  \\nto work on Great Cats and others to work on Blurry images.   \\nError analysis does not produce a rigid mathematical formula that tells you what the highest  \\npriority task should be. You also have to take into account how much progress you expect to  \\nmake on different categories and the amount of work needed to tackle each one.   \\n\\xa0\\n\\xa0\\nPage 33 Machine Learning Yearning-Draft Andrew Ng  \\n16 Cleaning up mislabeled dev and test set  \\nexamples  \\n \\nDuring error analysis, you might notice that some examples in your dev set are mislabeled.  \\nWhen I say “mislabeled” here, I mean that the pictures were already mislabeled by a human  \\nlabeler even before the algorithm encountered it. I.e., the class label in an example \\u200b(x,y)\\u200b has \\nan incorrect value for \\u200by\\u200b. For example, perhaps some pictures that are not cats are mislabeled  \\nas containing a cat, and vice versa. If you suspect the fraction of mislabeled images is  \\nsignificant, add a category to keep track of the fraction of examples mislabeled:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Mislabeled \\xa0 Comments \\xa0\\n…\\xa0     \\n98    ✔ \\xa0 Labeler missed cat \\xa0\\nin background \\xa0\\n99  ✔ \\xa0    \\n100    ✔ \\xa0 Drawing of a cat;\\xa0\\nnot a real cat. \\xa0\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0 6%\\xa0  \\n \\nShould you correct the labels in your dev set? Remember that the goal of the dev set is to  \\nhelp you quickly evaluate algorithms so that you can tell if Algorithm A or B is better. If the  \\nfraction of the dev set that is mislabeled impedes your ability to make these judgments, then  \\nit is worth spending time to fix the mislabeled dev set labels.   \\nFor example, suppose your classifier’s performance is:  \\n•Overall accuracy on dev set.………………. 90% (10% overall error.)  \\n•Errors due to mislabeled examples……. 0.6% (6% of dev set errors.)   \\n•Errors due to other causes………………… 9.4% (94% of dev set errors)  \\nHere, the 0.6% inaccuracy due to mislabeling might not be significant enough relative to the  \\n9.4% of errors you could be improving. There is no harm in manually fixing the mislabeled  \\nimages in the dev set, but it is not crucial to do so: It might be fine not knowing whether your  \\nsystem has 10% or 9.4% overall error. \\nSuppose you keep improving the cat classifier and reach the following performance:   \\nPage 34 Machine Learning Yearning-Draft Andrew Ng  \\n•Overall accuracy on dev set.………………. 98.0% (2.0% overall error.) \\n•Errors due to mislabeled examples……. 0.6%. (30% of dev set errors.)   \\n•Errors due to other causes………………… 1.4% (70% of dev set errors)  \\n30% of your errors are due to the mislabeled dev set images, adding significant error to your  \\nestimates of accuracy. It is now worthwhile to improve the quality of the labels in the dev set.  \\nTackling the mislabeled examples will help you figure out if a classifier’s error is closer to  \\n1.4% or 2%—a significant relative difference.   \\nIt is not uncommon to start off tolerating some mislabeled dev/test set examples, only later  \\nto change your mind as your system improves so that the fraction of mislabeled examples  \\ngrows relative to the total set of errors.   \\nThe last chapter explained how you can improve error categories such as Dog, Great Cat and  \\nBlurry through algorithmic improvements. You have learned in this chapter that you can  \\nwork on the Mislabeled category as well—through improving the data’s labels.   \\nWhatever process you apply to fixing dev set labels, remember to apply it to the test set  \\nlabels too so that your dev and test sets continue to be drawn from the same distribution.  \\nFixing your dev and test sets together would prevent the problem we discussed in Chapter 6,  \\nwhere your team optimizes for dev set performance only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\n',\n",
       " ' only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\nto use the result in an academic research paper or need a completely unbiased measure of  \\ntest set accuracy.  \\xa0\\n\\xa0\\n  \\nPage 35 Machine Learning Yearning-Draft Andrew Ng  \\n17 If you have a large dev set, split it into two  \\nsubsets, only one of which you look at   \\n \\nSuppose you have a large dev set of 5,000 examples in which you have a 20% error rate.  \\nThus, your algorithm is misclassifying ~1,000 dev images. It takes a long time to manually  \\nexamine 1,000 images, so we might decide not to use all of them in the error analysis.   \\nIn this case, I would explicitly split the dev set into two subsets, one of which you look at, and  \\none of which you don’t. You will more rapidly overfit the portion that you are manually  \\nlooking at. You can use the portion you are not manually looking at to tune parameters.   \\nLet\\u200b’\\u200bs continue our example above, in which the algorithm is misclassifying 1,000 out of  \\n5,000 dev set examples. Suppose we want to manually examine about 100 errors for error  \\nanalysis (10% of the errors). You should randomly select 10% of the dev set and place that  \\ninto what we’ll call an \\u200bEyeball dev set \\u200b to remind ourselves that we are looking at it with our  \\neyes. (For a project on speech recognition, in which you would be listening to audio clips,  \\nperhaps you would call this set an Ear dev set instead). The Eyeball dev set therefore has 500  \\nexamples, of which we would expect our algorithm to misclassify about 100.   \\nThe second subset of the dev set, called the \\u200bBlackbox dev set \\u200b, will have the remaining  \\n4500 examples. You can use the Blackbox dev set to evaluate classifiers automatically by  \\nmeasuring their error rates. You can also use it to select among algorithms or tune  \\nhyperparameters. However, you should avoid looking at it with your eyes. We use the term  \\n“Blackbox” because we will only use this subset of the data to obtain “Blackbox” evaluations  \\nof classifiers.   \\nPage 36 Machine Learning Yearning-Draft Andrew Ng \\n \\nWhy do we explicitly separate the dev set into Eyeball and Blackbox dev sets? Since you will  \\ngain intuition about the examples in the Eyeball dev set, you will start to overfit the Eyeball  \\ndev set faster. If you see the performance on the Eyeball dev set improving much more  \\nrapidly than the performance on the Blackbox dev set, you have overfit the Eyeball dev set.  \\nIn this case, you might need to discard it and find a new Eyeball dev set by moving more  \\nexamples from the Blackbox dev set into the Eyeball dev set or by acquiring new labeled  \\ndata.   \\nExplicitly splitting your dev set into Eyeball and Blackbox dev sets allows you to tell when  \\nyour manual error analysis process is causing you to overfit the Eyeball portion of your data.   \\n \\n  \\nPage 37 Machine Learning Yearning-Draft Andrew Ng \\n \\n18 How big should the Eyeball and Blackbox  \\ndev sets be?  \\nYour Eyeball dev set should be large enough to give you a sense of your algorithm’s major  \\nerror categories. If you are working on a task that humans do well (such as recognizing cats  \\nin images), here are some rough guidelines:   \\n•An eyeball dev set in which your classifier makes 10 mistakes would be considered very  \\nsmall. With just 10 errors, it’s hard to accurately estimate the impact of different error  \\ncategories. But if you have very little data and cannot afford to put more into the Eyeball  \\ndev set, it \\u200b’\\u200bs better than nothing and will help with project prioritization.   \\n•If your classifier makes ~20 mistakes on eyeball dev examples, you would start to get a  \\nrough sense of the major error sources.   \\n•With ~50 mistakes, you would get a good sense of the major error sources.  \\n•With ~100 mistakes, you would get a very good sense of the major sources of errors. I’ve  \\nseen people manually analyze even more errors—sometimes as many as 500. There is no  \\nharm in this as long as you have enough data.   \\nSay your classifier has a 5% error rate. To make sure you have ~100 misclassified examples  \\nin the Eyeball dev set, the Eyeball dev set would have to have about 2,000 examples (since  \\n0.05*2,000 = 100). The lower your classifier’s error rate, the larger your Eyeball dev set  \\nneeds to be in order to get a large enough set of errors to analyze.   \\nIf you are working on a task that even humans cannot do well, then the exercise of examining  \\nan Eyeball dev set will not be as helpful because it is harder to figure out why the algorithm  \\ndidn’t classify an example correctly. In this case, you might omit having an Eyeball dev set.  \\nWe discuss guidelines for such problems in a later chapter.   \\nPage 38 Machine Learning Yearning-Draft Andrew Ng \\n \\nHow about the Blackbox dev set? We previously said that dev sets of around 1,000-10,000  \\nexamples are common. To refine that statement, a Blackbox dev set of 1,000-10,000  \\nexamples will often give you enough data to tune hyperparameters and select among models,  \\nthough there is little harm in having even more data. A Blackbox dev set of 100 would be  \\nsmall but still useful.   \\nIf you have a small dev set, then you might not have enough data to split into Eyeball and  \\nBlackbox dev sets that are both large enough to serve their purposes. Instead, your entire dev  \\nset might have to be used as the Eyeball dev set—i.e., you would manually examine all the  \\ndev set data.   \\nBetween the Eyeball and Blackbox dev sets, I consider the Eyeball dev set more important  \\n(assuming that you are working on a problem that humans can solve well and that examining  \\nthe examples helps you gain insight). If you only have an Eyeball dev set, you can perform  \\nerror analyses, model selection and hyperparameter tuning all on that set. The downside of  \\nhaving only an Eyeball dev set is that the risk of overfitting the dev set is greater.   \\nIf you have plentiful access to data, then the size of the Eyeball dev set would be determined  \\nmainly by how many examples you have time to manually analyze. For example, I’ve rarely  \\nseen anyone manually analyze more than 1,000 errors.  \\n \\n \\n \\xa0\\nPage 39 Machine Learning Yearning-Draft Andrew Ng \\n \\n19 Takeaways: Basic error analysis  \\n \\n•When you start a new project, especially if it is in an area in which you are not an expert,  \\nit is hard to correctly guess the most promising directions.   \\n•So don’t start off trying to design and build the perfect system. Instead build and train a  \\nbasic system as quickly as possible—perhaps in a few days. Then use error analysis to  \\nhelp you identify the most promising directions and iteratively improve your algorithm  \\nfrom there.  \\n•Carry out error analysis by manually examining ~100 dev set examples the algorithm  \\nmisclassifies and counting the major categories of errors. Use this information to  \\nprioritize what types of errors to work on fixing.   \\n•Consider splitting the dev set into an Eyeball dev set, which you will manually examine,  \\nand a Blackbox dev set, which you will not manually examine. If performance on the  \\nEyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev  \\nset and should consider acquiring more data for it.   \\n•The Eyeball dev set should be big enough so that your algorithm misclassifies enough  \\nexamples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is sufficient  \\nfor many applications.  \\n•If your dev set is not big enough to split this way, just use the entire dev set as an Eyeball  \\ndev set for manual error analysis, model selection, and hyperparameter tuning.   \\n \\n \\n \\n  \\nPage 40 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBias and Variance  \\n \\n \\n \\n \\n \\n \\xa0\\nPage 41 Machine Learning Yearning-Draft Andrew Ng  \\n20 Bias and Variance: The two big sources of  \\nerror  \\n \\nSuppose your training, dev and test sets all come from the same distribution. Then you  \\nshould always try to get more training data, since that can only improve performance, right?  \\nEven though having more data can’t hurt, unfortunately it doesn’t always help as much as  \\nyou might hope. It could be a waste of time to work on getting more data. So, how do you  \\ndecide when to add data, and when not to bother?  \\nThere are two major sources of error in machine learning: bias and variance. Understanding  \\nthem will help you decide whether adding data, as well as other tactics to improve  \\nperformance, are a good use of time.   \\nSuppose you hope to build a cat recognizer that has 5% error. Right now, your training set  \\nhas an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding  \\ntraining data probably won’t help much. You should focus on other changes. Indeed, adding  \\nmore examples to your training set only makes it harder for your algorithm to do well on the  \\ntraining set. (We explain why in a later chapter.)   \\nIf your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error  \\n(95% accuracy), then the first problem to solve is to improve your algorithm \\u200b’\\u200bs performance  \\non your training set. Your dev/test set performance is usually worse than your training set  \\nperformance. So if you are getting 85% accuracy on the examples your algorithm has seen,  \\nthere’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen.   \\nSuppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break  \\nthe 16% error into two components:   \\n•First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of  \\nthis informally as the algorithm’s \\u200bbias\\u200b.  \\n•Second, how much worse the algorithm does on the dev (or test) set than the training set.  \\nIn this example, it does 1% worse on the dev set than the training set. We think of this  \\ninformally as the algorithm’s \\u200bvariance \\u200b. 6\\n6 The field of statistics has mor e for mal definitions of bias and v ariance that we won’ t worr y about.  \\nRoughly,  the bias is the err or rate of your algor ithm on your  training set when you hav e a v ery large  \\ntraining set.  The variance is how much worse you do on the test set compar ed to the tr aining set in  \\nPage 42 Machine Learning Yearning-Draft Andrew Ng  \\nSome changes to a learning algorithm can address the first component of error— \\u200bbias\\u200b—and \\nimprove its performance on the training set. Some changes address the second  \\ncomponent— \\u200bvariance \\u200b—and help it generalize better from the training set to the dev/test  \\nsets.  To select the most promising changes, it is incredibly useful to understand which of  7\\nthese two components of error is more pressing to address.  \\nDeveloping good intuition about Bias and Variance will help you choose effective changes for  \\nyour algorithm.  \\n\\xa0\\n\\xa0  \\nthis setting.  When your err or metr ic is mean sq uared er ror, you can write down formulas specifying \\nthese two q uantities,  and pr ove that T otal Err or = Bias + V ariance.  But for  our purposes of deciding  \\nhow to mak e progress on an ML  problem,  the more informal definition of bias and v ariance giv en \\nhere will suffice.   \\n7 There are also some methods that can simultaneously red uce bias and v ariance,  by mak ing major  \\nchanges to the system architecture.  But these tend to be harder to identify and implement.   \\nPage 43 Machine Learning Yearning-Draft Andrew Ng  \\n21 Examples of Bias and Variance \\n \\nConsider our cat classification task. An “ideal” classifier (such as a human) might achieve  \\nnearly perfect performance in this task.   \\nSuppose your algorithm performs as follows:  \\n•Training error = 1%  \\n•Dev error = 11%  \\nWhat problem does it have? Applying the definitions from the previous chapter, we estimate  \\nthe bias as 1%, and the variance as 10% (=11%-1%). Thus, it has \\u200bhigh variance \\u200b. The \\nclassifier has very low training error, but it is failing to generalize to the dev set. This is also  \\ncalled \\u200boverfitting \\u200b. \\nNow consider this:  \\n•Training error = 15%  \\n•Dev error = 16% \\nWe estimate the bias as 15%, and variance as 1%. This classifier is fitting the training set  \\npoorly with 15% error, but its error on the dev set is barely higher than the training error.  \\nThis classifier therefore has \\u200bhigh bias \\u200b, but low variance. We say that this algorithm is  \\nunderfitting \\u200b. \\nNow, consider this:  \\n•Training error = 15%  \\n•Dev error = 30% \\nWe estimate the bias as 15%, and variance as 15%. This classifier has \\u200bhigh bias and high  \\nvariance \\u200b: It is doing poorly on the training set, and therefore has high bias, and its  \\nperformance on the dev set is even worse, so it also has high variance. The  \\noverfitting/underfitting terminology is hard to apply here since the classifier is  \\nsimultaneously overfitting and underfitting.   \\n \\n \\nPage 44 Machine Learning Yearning-Draft Andrew Ng  \\nFinally, consider this:  \\n•Training error = 0.5%  \\n•Dev error = 1% \\nThis classifier is doing well, as it has low bias and low variance. Congratulations on achieving  \\nthis great performance!  \\xa0\\n  \\nPage 45 Machine Learning Yearning-Draft Andrew Ng  \\n22 Comparing to the optimal error rate  \\n \\nIn our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal”  \\nclassifier—is nearly 0%. A human looking at a picture would be able to recognize if it  \\ncontains a cat almost all the time; thus, we can hope for a machine that would do just as well.   \\nOther problems are harder. For example, suppose that you are building a speech recognition  \\nsystem, and find that 14% of the audio clips have so much background noise or are so  \\nunintelligible that even a human cannot recognize what was said. In this case, even the most  \\n“optimal” speech recognition system might have error around 14%.   \\nSuppose that on this speech recognition problem, your algorithm achieves:   \\n•Training error = 15%  \\n•Dev error = 30% \\nThe training set performance is already close to the optimal error rate of 14%. Thus, there is  \\nnot much room for improvement in terms of bias or in terms of training set performance.  \\nHowever, this algorithm is not generalizing well to the dev set; thus there is ample room for  \\nimprovement in the errors due to variance.   \\nThis example is similar to the third example from the previous chapter, which also had a  \\ntraining error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training  \\nerror of 15% leaves much room for improvement. This suggests bias-reducing changes might  \\nbe fruitful. But if the optimal error rate is 14%, then the same training set performance tells  \\nus that there’s little room for improvement in the classifier’s bias.   \\nFor problems where the optimal error rate is far from zero, here \\u200b’\\u200bs a more detailed  \\nbreakdown of an algorithm \\u200b’\\u200bs error. Continuing with our speech recognition example above,  \\nthe total dev set error of 30% can be broken down as follows (a similar analysis can be  \\napplied to the test set error):   \\n•Optimal error rate (“unavoidable bias”) \\u200b: 14%. Suppose we decide that, even with the  \\nbest possible speech system in the world, we would still suffer 14% error. We can think of  \\nthis as the “unavoidable” part of a learning algorithm \\u200b’\\u200bs bias.   \\nPage 46 Machine Learning Yearning-Draft Andrew Ng  \\n•Avoidable bias \\u200b: 1%. This is calculated as the difference between the training error and  \\nthe optimal error rate.  8\\n•Variance \\u200b: 15%. The difference between the dev error and the training error.   \\nTo relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:   9\\nBias = Optimal error rate (“unavoidable bias”) + Avoidable bias  \\nThe “avoidable bias” reflects how much worse your algorithm performs on the training set  \\nthan the “optimal classifier.”   \\nThe concept of variance remains the same as before. In theory, we can always reduce  \\nvariance to nearly zero by training on a massive training set. Thus, all variance is “avoidable”  \\nwith a sufficiently large dataset, so there is no such thing as “unavoidable variance.”   \\nConsider one more example, where the optimal error rate is 14%, and we have:   \\n•Training error = 15%  \\n•Dev error = 16% \\nWhereas in the previous chapter we called this a high bias classifier, now we would say that  \\nerror from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm  \\nis already doing well, with little room for improvement. It is only 2% worse than the optimal  \\nerror rate.   \\nWe see from these examples that knowing the optimal error rate is helpful for guiding our  \\nnext steps. In statistics, the optimal error rate is also called \\u200bBayes error rate \\u200b, or Bayes \\nrate.  \\nHow do we know what the optimal error rate is? For tasks that humans are reasonably good  \\nat, such as recognizing pictures or transcribing audio clips, you can ask a human to provide  \\nlabels then measure the accuracy of the human labels relative to your training set. This  \\nwould give an estimate of the optimal error rate. If you are working on a problem that even  \\n8 If this number  is negativ e, you are doing better  on the tr aining set than the optimal er ror rate.  This  \\nmeans you are ov erfitting on the training set,  and the algor ithm has ov er-memorized the tr aining set.  \\nYou should focus on v ariance r eduction methods rather than on fur ther  bias reduction methods.  \\n9 These definitions ar e chosen  to conv ey insight on how to impr ove your  lear ning algorithm.  These \\ndefinitions ar e different than how statisticians define Bias and V ariance.  Technically,  what I define \\nhere as “Bias” should be called “Error  we attribute to bias”;  and “Av oidable bias” should be “err or we \\nattribute to the lear ning algorithm’ s bias that is ov er the optimal er ror rate. ”  \\nPage 47 Machine Learning Yearning-Draft Andrew Ng  \\nhumans have a hard time solving (e.g., predicting what movie to recommend, or what ad to  \\nshow to a user) it can be hard to estimate the optimal error rate.   \\nIn the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss  \\nin more detail the process of comparing a learning algorithm’s performance to human-level  \\nperformance.  \\nIn the last few chapters, you learned how to estimate avoidable/unavoidable bias and  \\nvariance by looking at training and dev set error rates. The next chapter will discuss how you  \\ncan use insights from such an analysis to prioritize techniques that reduce bias vs.  \\ntechniques that reduce variance. There are very different techniques that you should apply  \\ndepending on whether your project’s current problem is high (avoidable) bias or high  \\nvariance. Read on!   \\n  \\nPage 48 Machine Learning Yearning-Draft Andrew Ng  \\n23 Addressing Bias and Variance \\n \\nHere is the simplest formula for addressing bias and variance issues:  \\n•If you have high avoidable bias, increase the size of your model (for example, increase the  \\nsize of your neural network by adding layers/neurons).  \\n•If you have high variance, add data to your training set.  \\nIf you are able to increase the neural network size and increase training data without limit, it  \\nis possible to do very well on many learning problems.   \\nIn practice, increasing the size of your model will eventually cause you to run into  \\ncomputational problems because training very large models is slow. You might also exhaust  \\nyour ability to acquire more training data. (Even on the internet, there is only a finite  \\nnumber of cat pictures!)   \\nDifferent model architectures—for example, different neural network architectures—will  \\nhave different amounts of bias/variance for your problem. A lot of recent deep learning  \\nresearch has developed many innovative model architectures. So if you are using neural  \\nnetworks, the academic literature can be a great source of inspiration. There are also many  \\ngreat open-source implementations on github. But the results of trying new architectures are  \\nless predictable than the simple formula of increasing the model size and adding data.   \\nIncreasing the model size generally reduces bias, but it might also increase variance and the  \\nrisk of overfitting. However, this overfitting problem usually arises only when you are not  \\nusing regularization. If you include a well-designed regularization method, then you can  \\nusually safely increase the size of the model without increasing overfitting.   \\nSuppose you are applying deep learning, with L2 regularization or dropout, with the  \\nregularization parameter that performs best on the dev set. If you increase the model size,  \\nusually your performance will stay the same or improve; it is unlikely to worsen significantly.  \\nThe only reason to avoid using a bigger model is the increased computational cost.   \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 49 Machine Learning Yearning-Draft Andrew Ng  \\n24 Bias vs. Variance tradeoff \\n \\nYou might have heard of the “Bias vs. Variance tradeoff.” Of the changes you could make to  \\nmost learning algorithms, there are some that reduce bias errors but at the cost of increasing  \\nvariance, and vice versa. This creates a “trade off” between bias and variance.   \\nFor example, increasing the size of your model—adding neurons/layers in a neural network,  \\nor adding input features—generally reduces bias but could increase variance. Alternatively,  \\nadding regularization generally increases bias but reduces variance.   \\nIn the modern era, we often have access to plentiful data and can use very large neural  \\nnetworks (deep learning). Therefore, there is less of a tradeoff, and there are now more  \\noptions for reducing bias without hurting variance, and vice versa.   \\nFor example, you can usually increase a neural network size and tune the regularization  \\nmethod to reduce bias without noticeably increasing variance. By adding training data, you  \\ncan also usually reduce variance without affecting bias.   \\nIf you select a model architecture that is well suited for your task, you might also reduce bias  \\nand variance simultaneously. Selecting such an architecture can be difficult.   \\nIn the next few chapters, we discuss additional specific techniques for addressing bias and  \\nvariance.   \\n \\n  \\nPage 50 Machine Learning Yearning-Draft Andrew Ng  \\n25 Techniques for reducing avoidable bias  \\n \\nIf your learning algorithm suffers from high avoidable bias, you might try the following  \\ntechniques:  \\n•Increase the model size \\u200b(such as number of neurons/layers): This technique reduces  \\nbias, since it should allow you to fit the training set better. If you find that this increases  \\nvariance, then use regularization, which will usually eliminate the increase in variance.  \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm eliminate a  \\nparticular category of errors. (We discuss this further in the next chapter.) These new  \\nfeatures could help with both bias and variance. In theory, adding more features could  \\nincrease the variance; but if you find this to be the case, then use regularization, which will  \\nusually eliminate the increase in variance.   \\n•Reduce or eliminate regularization \\u200b (L2 regularization, L1 regularization, dropout):  \\nThis will reduce avoidable bias, but increase variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\nOne method that is not helpful:   \\n•Add more training data \\u200b: This technique helps with variance problems, but it usually  \\nhas no significant effect on bias.   \\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 51 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n \\nYour algorithm must perform well on the training set before you can expect it to perform  \\nwell on the dev/test sets.   \\nIn addition to the techniques described earlier to address high bias, I sometimes also carry  \\nout an error analysis on the \\u200btraining data \\u200b, following a protocol similar to error analysis on  \\nthe Eyeball dev set. This can be useful if your algorithm has high bias—i.e., if it is not fitting  \\nthe training set well.   \\nFor example, suppose you are building a speech recognition system for an app and have  \\ncollected a training set of audio clips from volunteers. If your system is not doing well on the  \\ntraining set, you might consider listening to a set of ~100 examples that the algorithm is  \\ndoing poorly on to understand the major categories of training set errors. Similar to the dev  \\nset error analysis, you can count the errors in different categories:   \\nAudio clip \\xa0 Loud background \\xa0\\nnoise\\xa0User spoke\\xa0\\nquickly\\xa0Far from \\xa0\\nmicrophone \\xa0Comments\\xa0\\n1 ✔ \\xa0   Car noise \\xa0\\n2 ✔ \\xa0  ✔ \\xa0 Restaurant noise \\xa0\\n3  ✔ \\xa0 ✔ \\xa0 User shouting\\xa0\\nacross living room?\\xa0\\n4 ✔ \\xa0 \\xa0\\xa0  Coffeeshop \\xa0\\n% of total \\xa0 75%\\xa0 25%\\xa0 50%\\xa0  \\n \\nIn this example, you might realize that your algorithm is having a particularly hard time with  \\ntraining examples that have a lot of background noise. Thus, you might focus on techniques  \\nthat allow it to better fit training examples with background noise.   \\nYou might also double-check whether it is possible for a person to transcribe these audio  \\nclips, given the same input audio as your learning algorithm. If there is so much background  \\nnoise that it is simply impossible for anyone to make out what was said, then it might be  \\nunreasonable to expect any algorithm to correctly recognize such utterances. We will discuss  \\nthe benefits of comparing your algorithm to human-level performance in a later section.  \\n\\xa0\\n\\xa0\\nPage 52 Machine Learning Yearning-Draft Andrew Ng  \\n27 Techniques for reducing variance  \\n \\nIf your learning algorithm suffers from high variance, you might try the following  \\ntechniques:  \\n•Add more training data \\u200b: This is the simplest and most reliable way to address variance,  \\nso long as you have access to significantly more data and enough computational power to  \\nprocess the data.   \\n•Add regularization \\u200b (L2 regularization, L1 regularization, dropout): This technique \\nreduces variance but increases bias.   \\n•Add early stopping \\u200b (i.e., stop gradient descent early, based on dev set error): This  \\ntechnique reduces variance but increases bias. Early stopping behaves a lot like  \\nregularization methods, and some authors call it a regularization technique.   \\n•Feature selection to decrease number/type of input features: \\u200b This technique  \\nmight help with variance problems, but it might also increase bias. Reducing the number  \\nof features slightly (say going from 1,000 features to 900) is unlikely to have a huge effect  \\non bias. Reducing it significantly (say going from 1,000 features to 100—a 10x reduction)  \\nis more likely to have a significant effect, so long as you are not excluding too many useful  \\nfeatures. In modern deep learning, when data is plentiful, there has been a shift away from  \\nfeature selection, and we are now more likely to give all the features we have to the  \\nalgorithm and let the algorithm sort out which ones to use based on the data. But when  \\nyour training set is small, feature selection can be very useful.   \\n•Decrease the model size \\u200b(such as number of neurons/layers): \\u200bUse with caution. \\u200b This \\ntechnique could decrease variance, while possibly increasing bias. However, I don’t  \\nrecommend this technique for addressing variance. Adding regularization usually gives  \\nbetter classification performance. The advantage of reducing the model size is reducing  \\nyour computational cost and thus speeding up how quickly you can train models. If  \\nspeeding up model training is useful, then by all means consider decreasing the model size.  \\nBut if your goal is to reduce variance, and you are not concerned about the computational  \\ncost, consider adding regularization instead.   \\nHere are two additional tactics, repeated from the previous chapter on addressing bias:   \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm to eliminate a  \\nparticular category of errors. These new features could help with both bias and variance. In  \\nPage 53 Machine Learning Yearning-Draft Andrew Ng  \\ntheory, adding more features could increase the variance; but if you find this to be the case,  \\nthen use regularization, which will usually eliminate the increase in variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\n \\n \\n  \\nPage 54 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLearning curves  \\n\\xa0\\n\\xa0 \\xa0\\nPage 55 Machine Learning Yearning-Draft Andrew Ng  \\n28 Diagnosing bias and variance: Learning  \\ncurves  \\n \\nWe’ve seen some ways to estimate how much error can be attributed to avoidable bias vs.  \\nvariance. We did so by estimating the optimal error rate and computing the algorithm’s  \\ntraining set and dev set errors. Let’s discuss a technique that is even more informative:  \\nplotting a learning curve.  \\nA learning curve plots your dev set error against the number of training examples. To plot it,  \\nyou would run your algorithm using different training set sizes. For example, if you have  \\n1,000 examples, you might train separate copies of the algorithm on 100, 200, 300, …, 1000 \\nexamples. Then you could plot how dev set error varies with the training set size. Here is an  \\nexample: \\nAs the training set size increases, the dev set error should decrease.   \\nWe will often have some “desired error rate” that we hope our learning algorithm will  \\neventually achieve. For example:   \\n•If we hope for human-level performance, then the human error rate could be the “desired  \\nerror rate.”  \\n•If our learning algorithm serves some product (such as delivering cat pictures), we might  \\nhave an intuition about what level of performance is needed to give users a great  \\nexperience.  \\nPage 56 Machine Learning Yearning-Draft Andrew Ng \\n \\n•If you have worked on a important application for a long time, then you might have  \\nintuition about how much more progress you can reasonably make in the next  \\nquarter/year.   \\nAdd the desired level of performance to your learning curve:   \\n \\n \\n \\n \\n \\n \\n \\nYou can visually extrapolate the red “dev error” curve  to guess how much closer you could  \\nget to the desired level of performance by adding more data. In the example above, it looks  \\nplausible that doubling the training set size might allow you to reach the desired  \\nperformance.  \\nBut if the dev error curve has “plateaued” (i.e. flattened out), then you can immediately tell  \\nthat adding more data won’t get you to your goal:   \\nLooking at the learning curve might therefore help you avoid spending months collecting  \\ntwice as much training data, only to realize it does not help.  \\nPage 57 Machine Learning Yearning-Draft Andrew Ng \\n \\nOne downside of this process is that if you only look at the dev error curve, it can be hard to  \\nextrapolate and predict exactly where the red curve will go if you had more data. There is one  \\nadditional plot that can help you estimate the impact of adding more data: the training error.  \\nPage 58 Machine Learning Yearning-Draft Andrew Ng  \\n29 Plotting training error \\n \\nYour dev set (and test set) error should decrease as the training set size grows. But your  \\ntraining set error usually \\u200bincreases \\u200b as the training set size grows.   \\nLet’s illustrate this effect with an example. Suppose your training set has only 2 examples:  \\nOne cat image and one non-cat image. Then it is easy for the learning algorithms to  \\n“memorize” both examples in the training set, and get 0% training set error. Even if either or  \\nboth of the training examples were mislabeled, it is still easy for the algorithm to memorize  \\nboth labels.   \\nNow suppose your training set has 100 examples. Perhaps even a few examples are  \\nmislabeled, or ambiguous—some images are very blurry, so even humans cannot tell if there  \\nis a cat. Perhaps the learning algorithm can still “memorize” most or all of the training set,  \\nbut it is now harder to obtain 100% accuracy. By increasing the training set from 2 to 100  \\nexamples, you will find that the training set accuracy will drop slightly.   \\nFinally, suppose your training set has 10,000 examples. In this case, it becomes even harder  \\nfor the algorithm to perfectly fit all 10,000 examples, especially if some are ambiguous or  \\nmislabeled. Thus, your learning algorithm will do even worse on this training set.   \\nLet’s add a plot of training error to our earlier figures:   \\n \\n \\n \\n \\n \\n \\n \\nYou can see that the blue “training error” curve increases with the size of the training set.  \\nFurthermore, your algorithm usually does better on the training set than on the dev set; thus  \\nthe red dev error curve usually lies strictly above the blue training error curve.   \\nLet’s discuss next how to interpret these plots. \\xa0\\nPage 59 Machine Learning Yearning-Draft Andrew Ng \\n \\n30 Interpreting learning curves: High bias  \\n \\nSuppose your dev error curve looks like this:   \\n \\n \\n \\n \\n \\n \\nWe previously said that, if your dev error curve plateaus, you are unlikely to achieve the  \\ndesired performance just by adding data.   \\nBut it is hard to know exactly what an extrapolation of the red dev error curve will look like.  \\nIf the dev set was small, you would be even less certain because the curves could be noisy.  \\nSuppose we add the training error curve to this plot and get the following:   \\n \\n \\n \\n \\n \\n \\n \\nNow, you can be absolutely sure that adding more data will not, by itself, be sufficient. Why  \\nis that? Remember our two observations:  \\nPage 60 Machine Learning Yearning-Draft Andrew Ng \\n \\n•As we add more training data, training error can only get worse. Thus, the blue training  \\nerror curve can only stay the same or go higher, and thus it can only get further away from  \\nthe (green line) level of desired performance.   \\n•The red dev error curve is usually higher than the blue training error. Thus, there’s almost  \\nno way that adding more data would allow the red dev error curve to drop down to the  \\ndesired level of performance when even the training error is higher than the desired level  \\nof performance.  \\nExamining both the dev error curve and the training error curve on the same plot allows us  \\nto more confidently extrapolate the dev error curve.   \\nSuppose, for the sake of discussion, that the desired performance is our estimate of the  \\noptimal error rate. The figure above is then the standard “textbook” example of what a  \\nlearning curve with high avoidable bias looks like: At the largest training set  \\nsize—presumably corresponding to all the training data we have—there is a large gap  \\nbetween the training error and the desired performance, indicating large avoidable bias.  \\nFurthermore, the gap between the training and dev curves is small, indicating small  \\nvariance.   \\nPreviously, we were measuring training and dev set error only at the rightmost point of this  \\nplot, which corresponds to using all the available training data. Plotting the full learning  \\ncurve gives us a more comprehensive picture of the algorithms’ performance on different  \\ntraining set sizes.   \\n \\n  \\nPage 61 Machine Learning Yearning-Draft Andrew Ng  \\n31 Interpreting learning curves: Other cases   \\n \\nConsider this learning curve:  \\n \\nDoes this plot indicate high bias, high variance, or both?  \\nThe blue training error curve is relatively low, and the red dev error curve is much higher  \\nthan the blue training error. Thus, the bias is small, but the variance is large. Adding more  \\ntraining data will probably help close the gap between dev error and training error.  \\nNow, consider this: \\n \\nThis time, the training error is large, as it is much higher than the desired level of  \\nperformance. The dev error is also much larger than the training error. Thus, you have  \\nsignificant bias and significant variance. You will have to find a way to reduce both bias and  \\nvariance in your algorithm.   \\nPage 62 Machine Learning Yearning-Draft Andrew Ng  \\n32 Plotting learning curves  \\n \\nSuppose you have a very small training set of 100 examples. You train your algorithm using a  \\nrandomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing  \\nthe number of examples by intervals of ten. You then use these 10 data points to plot your  \\nlearning curve. You might find that the curve looks slightly noisy (meaning that the values  \\nare higher/lower than expected) at the smaller training set sizes.   \\nWhen training on just 10 randomly chosen examples, you might be unlucky and have a  \\nparticularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or,  \\nyou might get lucky and get a particularly “good” training set. Having a small training set  \\nmeans that the dev and training errors may randomly fluctuate.   \\nIf your machine learning application is heavily skewed toward one class (such as a cat  \\nclassification task where the fraction of negative examples is much larger than positive  \\nexamples), or if it has a huge number of classes (such as recognizing 100 different animal  \\nspecies), then the chance of selecting an especially “unrepresentative” or bad training set is  \\nalso larger. For example, if 80% of your examples are negative examples (y=0), and only  \\n20% are positive examples (y=1), then there is a chance that a training set of 10 examples  \\ncontains only negative examples, thus making it very difficult for the algorithm to learn  \\nsomething meaningful.   \\nIf the noise in the training curve makes it hard to see the true trends, here are two solutions:   \\n•Instead of training just one model on 10 examples, instead select several (say 3-10)  \\ndifferent randomly chosen training sets of 10 examples by sampling with replacement  10\\nfrom your original set of 100. Train a different model on each of these, and compute the  \\ntraining and dev set error of each of the resulting models. Compute and plot the average  \\ntraining error and average dev set error.  \\n•If your training set is skewed towards one class, or if it has many classes, choose a  \\n“balanced” subset instead of 10 training examples at random out of the set of 100. For  \\nexample, you can make sure that 2/10 of the examples are positive examples, and 8/10 are  \\n10 Here’s what sampling \\u200bwith replacement \\u200b means: You would randomly pick 10 different examples out of the 100 to form  \\nyour first training set. Then to form the second training set, you would again pick 10 examples, but without taking into \\naccount what had been chosen in the first training set. Thus, it is possible for one specific exa mple to appear in both the  \\nfirst and second training sets. In contrast, if you were sampling \\u200bwithout replacement \\u200b, the second training set would be  \\nchosen from just the 90 examples that had not been chosen the first time around. In practice, sampling with or without \\nreplacement shouldn’t make a huge difference, but the former is common practice.   \\nPage 63 Machine Learning Yearning-Draft Andrew Ng  \\nnegative. More generally, you can make sure the fraction of examples from each class is as  \\nclose as possible to the overall fraction in the original training set.   \\nI would not bother with either of these techniques unless you have already tried plotting  \\nlearning curves and concluded that the curves are too noisy to see the underlying trends. If  \\nyour training set is large—say over 10,000 examples—and your class distribution is not very  \\nskewed, you probably won’t need these techniques.   \\nFinally, plotting a learning curve may be computationally expensive: For example, you might  \\nhave to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training  \\nmodels with small datasets is much faster than training models with large datasets. Thus,  \\ninstead of evenly spacing out the training set sizes on a linear scale as above, you might train  \\nmodels with 1,000, 2,000, 4,000, 6,000, and 10,000 examples. This should still give you a  \\nclear sense of the trends in the learning curves. Of course, this technique is relevant only if  \\nthe computational cost of training all the additional models is significant.   \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 64 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nComparing to \\nhuman-level \\nperformance \\n\\xa0\\n\\xa0\\n  \\nPage 65 Machine Learning Yearning-Draft Andrew Ng  \\n33 Why we compare to human-level  \\nperformance  \\n \\nMany machine learning systems aim to automate things that humans do well. Examples  \\ninclude image recognition, speech recognition, and email spam classification. Learning  \\nalgorithms have also improved so much that we are now surpassing human-level  \\nperformance on more and more of these tasks.   \\nFurther, there are several reasons building an ML system is easier if you are trying to do a  \\ntask that people can do well:   \\n1. Ease of obtaining data from human labelers. \\u200b For example, since people recognize  \\ncat images well, it is straightforward for people to provide high accuracy labels for your  \\nlearning algorithm.  \\n2. Error analysis can draw on human intuition. \\u200b Suppose a speech recognition  \\nalgorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\naudio clip as “This recipe calls for a \\u200bpear\\u200b of apples,” mistaking “pair” for “pear.” You can  \\ndraw on human intuition and try to understand what information a person uses to get the  \\ncorrect transcription, and use this knowledge to modify the learning algorithm.   \\n3. Use human-level performance to estimate the optimal error rate and also set  \\na “desired error rate.” \\u200b Suppose your algorithm achieves 10% error on a task, but a person  \\nachieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\navoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\nEven though item #3 might not sound important, I find that having a reasonable and  \\nachievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\nhigh avoidable bias is incredibly valuable and opens up a menu of options to try.   \\nThere are some tasks that even humans aren’t good at. For example, picking a book to  \\nrecommend to you; or picking an ad to show a user on a website; or predicting the stock  \\nmarket. Computers already surpass the performance of most people on these tasks. With  \\nthese applications, we run into the following problems:   \\n•It is harder to obtain labels. \\u200b For example, it’s hard for human labelers to annotate a  \\ndatabase of users with the “optimal” book recommendation. If you operate a website or  \\napp that sells books, you can obtain data by showing books to users and seeing what they  \\nbuy. If you do not operate such a site, you need to find more creative ways to get data.  \\nPage 66 Machine Learning Yearning-Draft Andrew Ng  \\n•Human intuition is harder to count on. \\u200b For example, pretty much no one can  \\npredict the stock market. So if our stock prediction algorithm does no better than random  \\nguessing, it is hard to figure out how to improve it.   \\n•It is hard to know what the optimal error rate and reasonable desired error  \\nrate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\n',\n",
       " \"rate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\nprevious chapter for comparing to human-level performance apply:   \\n•Ease of obtaining labeled data from human labelers. \\u200b You can get a team of doctors  \\nto provide labels to you with a 2% error rate.  \\n•Error analysis can draw on human intuition. \\u200bBy discussing images with a team of  \\ndoctors, you can draw on their intuitions.  \\n•Use human-level performance to estimate the optimal error rate and also set  \\nachievable “desired error rate.” \\u200b It is reasonable to use 2% error as our estimate of the  \\noptimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\nhigher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\nreasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\nestimates are necessarily too high.  \\nWhen it comes to obtaining labeled data, you might not want to discuss every image with an  \\nentire team of doctors since their time is expensive. Perhaps you can have a single junior  \\ndoctor label the vast majority of cases and bring only the harder cases to more experienced  \\ndoctors or to the team of doctors.   \\nIf your system is currently at 40% error, then it doesn’t matter much whether you use a  \\njunior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\nintuitions. But if your system is already at 10% error, then defining the human-level  \\nreference as 2% gives you better tools to keep improving your system.   \\n \\xa0\\nPage 68 Machine Learning Yearning-Draft Andrew Ng  \\n35 Surpassing human-level performance   \\n \\nYou are working on speech recognition and have a dataset of audio clips. Suppose your  \\ndataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\nsystem already achieves 8% error. Can you use any of the three techniques described in  \\nChapter 33 to continue making rapid progress?  \\nIf you can identify a subset of data in which humans significantly surpass your system, then  \\nyou can still use those techniques to drive rapid progress. For example, suppose your system  \\nis much better than people at recognizing speech in noisy audio, but humans are still better  \\nat transcribing very rapidly spoken speech.  \\nFor the subset of data with rapidly spoken speech:  \\n1.You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\noutput.  \\n2.You can draw on human intuition to understand why they correctly heard a rapidly  \\nspoken utterance when your system didn’t.  \\n3.You can use human-level performance on rapidly spoken speech as a desired performance  \\ntarget. \\nMore generally, so long as there are dev set examples where humans are right and your  \\nalgorithm is wrong, then many of the techniques described earlier will apply. This is true  \\neven if, averaged over the entire dev/test set, your performance is already surpassing  \\nhuman-level performance.  \\nThere are many important machine learning applications where machines surpass human  \\nlevel performance. For example, machines are better at predicting movie ratings, how long it  \\ntakes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\nsubset of techniques apply once humans have a hard time identifying examples that the  \\nalgorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\nwhere machines already surpass human-level performance, while progress is faster when  \\nmachines are still trying to catch up to humans.   \\n \\n  \\nPage 69 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTraining and  \\ntesting on different  \\ndistributions  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 70 Machine Learning Yearning-Draft Andrew Ng  \\n36 When you should train and test on  \\ndifferent distributions  \\n \\nUsers of your cat pictures app have uploaded 10,000 images, which you have manually  \\nlabeled as containing cats or not. You also have a larger set of 200,000 images that you  \\ndownloaded off the internet. How should you define train/dev/test sets?   \\nSince the 10,000 user images closely reflect the actual probability distribution of data you  \\nwant to do well on, you might use that for your dev and test sets. If you are training a  \\ndata-hungry deep learning algorithm, you might give it the additional 200,000 internet  \\nimages for training. Thus, your training and dev/test sets come from different probability  \\ndistributions. How does this affect your work?   \\nInstead of partitioning our data into train/dev/test sets, we could take all 210,000 images we  \\nhave, and randomly shuffle them into train/dev/test sets. In this case, all the data comes  \\nfrom the same distribution. But I recommend against this method, because about  \\n205,000/210,000 ≈ 97.6% of your dev/test data would come from internet images, which  \\ndoes not reflect the actual distribution you want to do well on. Remember our  \\nrecommendation on choosing dev/test sets:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nMost of the academic literature on machine learning assumes that the training set, dev set  \\nand test set all come from the same distribution.  In the early days of machine learning, data  11\\nwas scarce. We usually only had one dataset drawn from some probability distribution. So  \\nwe would randomly split that data into train/dev/test sets, and the assumption that all the  \\ndata was coming from the same source was usually satisfied.   \\n11 There is some academic research on training and testing on differ ent distributions.  Examples  \\ninclude “domain adaptation, ” “tr ansfer lear ning” and “multitask  lear ning. ” But ther e is still a huge \\ngap between theory and practice.  If you tr ain on dataset A and test on some v ery differ ent type of data \\nB, luck could hav e a huge effect on how well  your  algor ithm performs.  (Her e, “luck ” includes the \\nresearcher’ s hand-designed features for  the particular task , as well as other factors that we just don’ t \\nunder stand yet. ) This mak es the academic study of training and testing on differ ent distributions  \\ndifficult to carr y out in a systematic way.   \\nPage 71 Machine Learning Yearning-Draft Andrew Ng  \\nBut in the era of big data, we now have access to huge training sets, such as cat internet  \\nimages. Even if the training set comes from a different distribution than the dev/test set, we  \\nstill want to use it for learning since it can provide a lot of information.  \\nFor the cat detector example, instead of putting all 10,000 user-uploaded images into the  \\ndev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining  \\n5,000 user-uploaded examples into the training set. This way, your training set of 205,000  \\nexamples contains some data that comes from your dev/test distribution along with the  \\n200,000 internet images. We will discuss in a later chapter why this method is helpful.   \\nLet’s consider a second example. Suppose you are building a speech recognition system to  \\ntranscribe street addresses for a voice-controlled mobile map/navigation app. You have  \\n20,000 examples of users speaking street addresses. But you also have 500,000 examples of  \\nother audio clips with users speaking about other topics. You might take 10,000 examples of  \\nstreet addresses for the dev/test sets, and use the remaining 10,000, plus the additional  \\n500,000 examples, for training.   \\nWe will continue to assume that your dev data and your test data come from the same  \\ndistribution. But it is important to understand that different training and dev/test  \\ndistributions offer some special challenges.   \\n  \\nPage 72 Machine Learning Yearning-Draft Andrew Ng  \\n37 How to decide whether to use all your data  \\n \\nSuppose your cat detector’s training set includes 10,000 user-uploaded images. This data  \\ncomes from the same distribution as a separate dev/test set, and represents the distribution  \\nyou care about doing well on. You also have an additional 20,000 images downloaded from  \\nthe internet. Should you provide all 20,000+10,000=30,000 images to your learning \\nalgorithm as its training set, or discard the 20,000 internet images for fear of it biasing your  \\nlearning algorithm?   \\nWhen using earlier generations of learning algorithms (such as hand-designed computer  \\nvision features, followed by a simple linear classifier) there was a real risk that merging both  \\ntypes of data would cause you to perform worse. Thus, some engineers will warn you against  \\nincluding the 20,000 internet images. \\nBut in the modern era of powerful, flexible learning algorithms—such as large neural  \\nnetworks—this risk has greatly diminished. If you can afford to build a neural network with a  \\nlarge enough number of hidden units/layers, you can safely add the 20,000 images to your  \\ntraining set. Adding the images is more likely to increase your performance.   \\nThis observation relies on the fact that there is some x —> y mapping that works well for  \\nboth types of data. In other words, there exists some system that inputs either an internet  \\nimage or a mobile app image and reliably predicts the label, even without knowing the  \\nsource of the image.   \\nAdding the additional 20,000 images has the following effects:  \\n1.It gives your neural network more examples of what cats do/do not look like. This is  \\nhelpful, since internet images and user-uploaded mobile app images do share some  \\nsimilarities. Your neural network can apply some of the knowledge acquired from internet  \\nimages to mobile app images.   \\n2.It forces the neural network to expend some of its capacity to learn about properties that  \\nare specific to internet images (such as higher resolution, different distributions of how  \\nthe images are framed, etc.) If these properties differ greatly from mobile app images, it  \\nwill “use up” some of the representational capacity of the neural network. Thus there is  \\nless capacity for recognizing data drawn from the distribution of mobile app images,  \\nwhich is what you really care about. Theoretically, this could hurt your algorithms’  \\nperformance.  \\nPage 73 Machine Learning Yearning-Draft Andrew Ng  \\nTo describe the second effect in different terms, we can turn to the fictional character  \\nSherlock Holmes, who says that your brain is like an attic; it only has a finite amount of  \\nspace. He says that “for every addition of knowledge, you forget something that you knew  \\nbefore. It is of the highest importance, therefore, not to have useless facts elbowing out the  \\nuseful ones.”   12\\nFortunately, if you have the computational capacity needed to build a big enough neural  \\nnetwork—i.e., a big enough attic—then this is not a serious concern. You have enough  \\ncapacity to learn from both internet and from mobile app images, without the two types of  \\ndata competing for capacity. Your algorithm’s “brain” is big enough that you don’t have to  \\nworry about running out of attic space.   \\nBut if you do not have a big enough neural network (or another highly flexible learning  \\nalgorithm), then you should pay more attention to your training data matching your dev/test  \\nset distribution.  \\nIf you think you have data that has no benefit,you should just leave out that data for  \\ncomputational reasons. For example, suppose your dev/test sets contain mainly casual  \\npictures of people, places, landmarks, animals. Suppose you also have a large collection of  \\nscanned historical documents:   \\n \\n \\n \\n \\n \\n \\n \\nThese documents don’t contain anything resembling a cat. They also look completely unlike  \\nyour dev/test distribution. There is no point including this data as negative examples,  \\nbecause the benefit from the first effect above is negligible—there is almost nothing your  \\nneural network can learn from this data that it can apply to your dev/test set distribution.  \\nIncluding them would waste computation resources and representation capacity of the  \\nneural network. \\xa0\\n12 \\u200bA Study  in Sc arlet\\u200b \\u200bby Arthur Conan Doyle \\nPage 74 Machine Learning Yearning-Draft Andrew Ng \\n \\n38 How to decide whether to include  \\ninconsistent data  \\n \\nSuppose you want to learn to predict housing prices in New York City. Given the size of a  \\nhouse (input feature x), you want to predict the price (target label y).   \\nHousing prices in New York City are very high. Suppose you have a second dataset of  \\nhousing prices in Detroit, Michigan, where housing prices are much lower. Should you  \\ninclude this data in your training set?   \\nGiven the same size x, the price of a house y is very different depending on whether it is in  \\nNew York City or in Detroit. If you only care about predicting New York City housing prices,  \\nputting the two datasets together will hurt your performance.  In this case, it would be better  \\nto leave out the inconsistent Detroit data.  13\\nHow is this New York City vs. Detroit example different from the  mobile app vs. internet cat  \\nimages example?  \\nThe cat image example is different because, given an input picture x, one can reliably predict  \\nthe label y indicating whether there is a cat, even without knowing if the image is an internet  \\nimage or a mobile app image. I.e., there is a function f(x) that reliably maps from the input x  \\nto the target output y, even without knowing the origin of x. Thus, the task of recognition  \\nfrom internet images is “consistent” with the task of recognition from mobile app images.  \\nThis means there was little downside (other than computational cost) to including all the  \\ndata, and some possible significant upside. In contrast, New York City and Detroit, Michigan  \\ndata are not consistent. Given the same x (size of house), the price is very different  \\ndepending on where the house is.   \\n \\n \\n  \\n13 There is one way to address the problem of Detr oit data being inconsistent with New Yor k City  \\ndata,  which is to add an extra featur e to each training example indicating the city.  Given an input \\nx—which now specifies the city—the target v alue of y is now unambiguous.  Howev er, in practice I  do \\nnot see this done fr equently.   \\nPage 75 Machine Learning Yearning-Draft Andrew Ng  \\n39 Weighting data   \\n \\nSuppose you have 200,000 images from the internet and 5,000 images from your mobile  \\napp users. There is a 40:1 ratio between the size of these datasets. In theory, so long as you  \\nbuild a huge neural network and train it long enough on all 205,000 images, there is no  \\nharm in trying to make the algorithm do well on both internet images and mobile images.   \\nBut in practice, having 40x as many internet images as mobile app images might mean you  \\nneed to spend 40x (or more) as much computational resources to model both, compared to if  \\nyou trained on only the 5,000 images.   \\nIf you don’t have huge computational resources, you could  give the internet images a much  \\nlower weight as a compromise.   \\nFor example, suppose your optimization objective is squared error (This is not a good choice  \\nfor a classification task, but it will simplify our explanation.) Thus, our learning algorithm  \\ntries to optimize:  \\n  \\nThe first sum above is over the 5,000 mobile images, and the second sum is over the  \\n200,000 internet images. You can instead optimize with an additional parameter \\u200b𝛽\\u200b:  \\n \\n If you set \\u200b𝛽\\u200b=1/40, the algorithm would give equal weight to th e 5,000 mobile images and the  \\n200,000 internet images. You can also set the parameter \\u200b𝛽\\u200b to other values, perhaps by  \\ntuning to the dev set.   \\nBy weighting the additional Internet images less, you don’t have to build as massive a neural  \\nnetwork to make sure the algorithm does well on both types of tasks. This type of  \\nre-weighting is needed only when you suspect the additional data (Internet Images) has a  \\nvery different distribution than the dev/test set, or if the additional data is much larger than  \\nthe data that came from the same distribution as the dev/test set (mobile images).   \\nPage 76 Machine Learning Yearning-Draft Andrew Ng  \\n40 Generalizing from the training set to the  \\ndev set  \\n \\nSuppose you are applying ML in a setting where the training and the dev/test distributions  \\nare different. Say, the training set contains Internet images + Mobile images, and the  \\ndev/test sets contain only Mobile images. However, the algorithm is not working well: It has  \\na much higher dev/test set error than you would like. Here are some possibilities of what  \\nmight be wrong:  \\n1.It does not do well on the training set. This is the problem of high (avoidable) bias on the  \\ntraining set distribution.   \\n2.It does well on the training set, but does not generalize well to previously unseen data  \\ndrawn from the same distribution as the training set \\u200b. This is high variance.   \\n3.It generalizes well to new data drawn from the same distribution as the training set, but  \\nnot to data drawn from the dev/test set distribution. We call this problem \\u200bdata  \\nmismatch \\u200b, since it is because the training set data is a poor match for the dev/test set  \\ndata.   \\nFor example, suppose that humans achieve near perfect performance on the cat recognition  \\ntask. Your algorithm achieves this:  \\n•1% error on the training set  \\n•1.5% error on data drawn from the same distribution as the training set that the algorithm  \\nhas not seen  \\n•10% error on the dev set   \\nIn this case, you clearly have a data mismatch problem. To address this, you might try to  \\nmake the training data more similar to the dev/test data. We discuss some techniques for  \\nthis later.  \\nIn order to diagnose to what extent an algorithm suffers from each of the problems 1-3  \\nabove, it will be useful to have another dataset. Specifically, rather than giving the algorithm  \\nall the available training data, you can split it into two subsets: The actual training set which  \\nthe algorithm will train on, and a separate set, which we will call the “Training dev” set, that  \\nwe will not train on.   \\nYou now have four subsets of data:  \\nPage 77 Machine Learning Yearning-Draft Andrew Ng  \\n•Training set. This is the data that the algorithm will learn from (e.g., Internet images +  \\nMobile images). This does not have to be drawn from the same distribution as what we  \\nreally care about (the dev/test set distribution).  \\n•Training dev set: This data is drawn from the same distribution as the training set (e.g.,  \\nInternet images + Mobile images). This is usually smaller than the training set; it only  \\nneeds to be large enough to evaluate and track the progress of our learning algorithm.   \\n•Dev set: This is drawn from the same distribution as the test set, and it reflects the  \\ndistribution of data that we ultimately care about doing well on. (E.g., mobile images.)   \\n•Test set: This is drawn from the same distribution as the dev set. (E.g., mobile images.)  \\nArmed with these four separate datasets, you can now evaluate:  \\n•Training error, by evaluating on the training set.   \\n•The algorithm’s ability to generalize to new data drawn from the training set distribution,  \\nby evaluating on the training dev set.  \\n•The algorithm’s performance on the task you care about, by evaluating on the dev and/or  \\ntest sets.   \\nMost of the guidelines in Chapters 5-7 for picking the size of the dev set also apply to the  \\ntraining dev set.  \\n \\n \\n \\n \\n \\n \\n  \\nPage 78 Machine Learning Yearning-Draft Andrew Ng  \\n41 Identifying Bias, Variance, and Data  \\nMismatch Errors  \\n \\nSuppose humans achieve almost perfect performance (≈0% error) on the cat detection task,  \\nand thus the optimal error rate is about 0%. Suppose you have:  \\n•1% error on the training set. \\n•5% error on training dev set.   \\n•5% error on the dev set.   \\nWhat does this tell you? Here, you know that you have high variance. The variance reduction  \\ntechniques described earlier should allow you to make progress.   \\nNow, suppose your algorithm achieves:   \\n•10% error on the training set.  \\n•11% error on training dev set.  \\n•12% error on the dev set.   \\nThis tells you that you have high avoidable bias on the training set. I.e., the algorithm is  \\ndoing poorly on the training set. Bias reduction techniques should help.  \\nIn the two examples above, the algorithm suffered from only high avoidable bias or high  \\nvariance. It is possible for an algorithm to suffer from any subset of high avoidable bias, high  \\nvariance, and data mismatch. For example:   \\n•10% error on the training set.   \\n•11% error on training dev set.  \\n•20% error on the dev set.   \\nThis algorithm suffers from high avoidable bias and from data mismatch. It does not,  \\nhowever, suffer from high variance on the training set distribution.   \\nIt might be easier to understand how the different types of errors relate to each other by  \\ndrawing them as entries in a table:   \\nPage 79 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\nContinuing with the example of th \\u200be cat image detector, you can see that there are two  \\ndifferent distributions of data on the x-axis. On the y-axis, we ha \\u200bve three types of error:  \\nhuman level error, error on examples the algorithm has trained on, and error on examples  \\nthe algorithm has not trained on. We can fill in the boxes with the different types of errors we  \\nidentified in the previous chapter.   \\nIf you wish, you can also fill in the remaining two boxes in this table: You can fill in the  \\nupper-right box (Human level performance on Mobile Images) by asking some humans to  \\nlabel your mobile cat images data and measure their error. You can fill in the next box by  \\ntaking the mobile cat images (Distribution B) and putting a small fraction of into the training  \\nset so that the neural network learns on it too. Then you measure the learned model’s error  \\non that subset of data. Filling in these two additional entries may sometimes give additional  \\ninsight about what the algorithm is doing on the two different distributions (Distribution A  \\nand B) of data.  \\nBy understanding which types of error the algorithm suffers from the most, you will be better  \\npositioned to decide whether to focus on reducing bias, reducing variance, or reducing data  \\nmismatch.  \\n  \\nPage 80 Machine Learning Yearning-Draft Andrew Ng  \\n42 Addressing data mismatch  \\n \\nSuppose you have developed a speech recognition system that does very well on the training  \\nset and on the training dev set. However, it does poorly on your dev set: You have a data  \\nmismatch problem. What can you do?  \\nI recommend that you: (i) Try to understand what properties of the data differ between the  \\ntraining and the dev set distributions. (ii) Try to find more training data that better matches  \\nthe dev set examples that your algorithm has trouble with.  14\\nFor example, suppose you carry out an error analysis on the speech recognition dev set: You  \\nmanually go through 100 examples, and try to understand where the algorithm is making  \\nmistakes. You find that your system does poorly because most of the audio clips in the dev  \\nset are taken within a car, whereas most of the training examples were recorded against a  \\nquiet background. The engine and road noise dramatically worsen the performance of your  \\nspeech system. In this case, you might try to acquire more training data comprising audio  \\nclips that were taken in a car. The purpose of the error analysis is to understand the  \\nsignificant differences between the training and the dev set, which is what leads to the data  \\nmismatch.  \\nIf your training and training dev sets include audio recorded within a car, you should also  \\ndouble-check your system’s performance on this subset of data. If it is doing well on the car  \\ndata in the training set but not on car data in the training dev set, then this further validates  \\nthe hypothesis that getting more car data would help. This is why we discussed the  \\npossibility of including in your training set some data drawn from the same distribution as  \\nyour dev/test set in the previous chapter. Doing so allows you to compare your performance  \\non the car data in the training set vs. the dev/test set.   \\nUnfortunately, there are no guarantees in this process. For example, if you don't have any  \\nway to get more training data that better match the dev set data, you might not have a clear  \\npath towards improving performance.  \\n \\n \\xa0\\n14There is also some research on “domain adaptation”—how to tr ain an algorithm on one distribution  \\nand hav e it gener alize to a differ ent distribution.  These methods ar e typically applicable only in  \\nspecial types of problems and are much less widely used than the ideas descr ibed in this chapter .  \\nPage 81 Machine Learning Yearning-Draft Andrew Ng  \\n43 Artificial data synthesis  \\n \\nYour speech system needs more data that sounds as if it were taken from within a car. Rather  \\nthan collecting a lot of data while driving around, there might be an easier way to get this  \\ndata: By artificially synthesizing it.  \\nSuppose you obtain a large quantity of car/road noise audio clips. You can download this  \\ndata from several websites. Suppose you also have a large training set of people speaking in a  \\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip  \\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in  \\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it  \\nwere collected inside a car.   \\nMore generally, there are several circumstances where artificial data synthesis allows you to  \\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as  \\na second example. You notice that dev set images have much more motion blur because they  \\ntend to come from cellphone users who are moving their phone slightly while taking the  \\npicture. You can take non-blurry images from the training set of internet images, and add  \\nsimulated motion blur to them, thus making them more similar to the dev set.   \\nKeep in mind that artificial data synthesis has its challenges: it is sometimes easier to create  \\nsynthetic data that appears realistic to a person than it is to create data that appears realistic  \\nto a computer. For example, suppose you have 1,000 hours of speech training data, but only  \\n1 hour of car noise. If you repeatedly use the same 1 hour of car noise with different portions  \\nfrom the original 1,000 hours of training data, you will end up with a synthetic dataset where  \\nthe same car noise is repeated over and over. While a person listening to this audio probably  \\nwould not be able to tell—all car noise sounds the same to most of us—it is possible that a  \\nlearning algorithm would “overfit” to the 1 hour of car noise. Thus, it could generalize poorly  \\nto a new audio clip where the car noise happens to sound different.   \\nAlternatively, suppose you have 1,000 unique hours of car noise, but all of it was taken from  \\njust 10 different cars. In this case, it is possible for an algorithm to “overfit” to these 10 cars  \\nand perform poorly if tested on audio from a different car. Unfortunately, these problems  \\ncan be hard to spot.   \\n \\n \\n \\nPage 82 Machine Learning Yearning-Draft Andrew Ng  \\n \\nTo take one more example, suppose you are building a computer vision system to recognize  \\ncars. Suppose you partner with a video gaming company, which has computer graphics  \\nmodels of several cars. To train your algorithm, you use the models to generate synthetic  \\nimages of cars. Even if the synthesized images look very realistic, this approach (which has  \\nbeen independently proposed by many people) will probably not work well. The video game  \\nmight have ~20 car designs in the entire video game. It is very expensive to build a 3D car  \\nmodel of a car; if you were playing the game, you probably wouldn’t notice that you’re seeing  \\nthe same cars over and over, perhaps only painted differently. I.e., this data looks very  \\nrealistic to you. But compared to the set of all cars out on roads—and therefore what you’re  \\nlikely to see in the dev/test sets—this set of 20 synthesized cars captures only a minuscule  \\nfraction of the world’s distribution of cars. Thus if your 100,000 training examples all come  \\nfrom these 20 cars, your system will “overfit” to these 20 specific car designs, and it will fail  \\nto generalize well to dev/test sets that include other car designs.   \\nWhen synthesizing data, put some thought into whether you’re really synthesizing a  \\nrepresentative set of examples. Try to avoid giving the synthesized data properties that  \\nmakes it possible for a learning algorithm to distinguish synthesized from non-synthesized  \\nexamples—such as if all the synthesized data comes from one of 20 car designs, or all the  \\nsynthesized audio comes from only 1 hour of car noise. This advice can be hard to follow.   \\nWhen working on data synthesis, my teams have sometimes taken weeks before we produced  \\ndata with details that are close enough to the actual distribution for the synthesized data to  \\nhave a significant effect. But if you are able to get the details right, you can suddenly access a  \\nfar larger training set than before.   \\n \\n \\nPage 83 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\nDebugging  \\ninference  \\nalgorithms \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 84 Machine Learning Yearning-Draft Andrew Ng  \\n44 The Optimization Verification test  \\n \\nSuppose you are building a speech recognition system. Your system works by inputting an  \\naudio clip \\u200bA\\u200b, and computing some Score \\u200bA\\u200b(\\u200bS\\u200b) for each possible  output sentence \\u200bS\\u200b. For  \\nexample, you might try to estimate Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b), the pro bability that the correct  \\noutput transcription is the sentence \\u200bS\\u200b,  given that the input audio was \\u200bA.   \\nGiven a way to compute Score \\u200bA\\u200b(\\u200bS\\u200b), you still have to find the En glish sentence \\u200bS\\u200b that \\nmaximizes it:  \\n \\nHow do you compute the “arg max” above? If the English language has 50,000 words, then  \\nthere are (50,000)\\u200bN \\u200bpossi ble sentences of length \\u200bN\\u200b—far too many to exhaustively enumerate.  \\nSo, you need to apply an approximate search algorithm, to try to find the value of \\u200bS\\u200b that  \\noptimizes (maximizes) Score \\u200bA\\u200b(\\u200bS\\u200b). One example search algorith m is “beam search,” which  \\nkeeps only \\u200bK\\u200b top candidates during the search process. (For the purposes of this chapter, you  \\ndon’t need to understand the details of beam search.) Algorithms like this are not guaranteed  \\nto find the value of \\u200bS \\u200bthat maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nSuppose that an audio clip \\u200bA\\u200b records someone saying “I love machine learning.” But instead  \\nof outputting the correct transcription, your system outputs the incorrect “I love robots.”  \\nThere are now two possibilities for what went wrong:   \\n1.Search algorithm problem \\u200b. The approximate search algorithm (beam search) failed  \\nto find the value of \\u200bS\\u200b that maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\n2.Objective (scoring function) problem. \\u200b Our estimates for Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b) were \\ninaccurate. In particular, our choice of Score \\u200bA\\u200b(\\u200bS\\u200b) failed to r ecognize that “I love machine  \\nlearning” is the correct transcription.   \\nDepending on which of these was the cause of the failure, you should prioritize your efforts  \\nvery differently. If #1 was the problem, you should work on improving the search algorithm.  \\nIf #2 was the problem, you should work on the learning algorithm that estimates Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nFacing this situation, some researchers will randomly decide to work on the search  \\nalgorithm; others will randomly work on a better way to learn values for Score \\u200bA\\u200b(S). But \\nunless you know which of these is the underlying cause of the error, your efforts could be  \\nwasted. How can you decide more systematically what to work on?   \\nPage 85 Machine Learning Yearning-Draft Andrew Ng  \\nLet S\\u200bout\\u200b be the output tran scription (“I love robots”). Let S* be the correct transcription (“I  \\nlove machine learning”). In order to understand whether #1 or #2 above is the problem, you  \\ncan perform the \\u200bOptimization Verification test \\u200b: First, compute Score \\u200bA\\u200b(\\u200bS\\u200b*) and \\nScore\\u200bA\\u200b(\\u200bS\\u200bout\\u200b). Then check w hether Score \\u200bA\\u200b(\\u200bS\\u200b*) > Score \\u200bA\\u200b(\\u200bS\\u200bout\\u200b). There are two possibilities:   \\nCase 1: Score\\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, your learning algorithm has correctly given S* a higher score than S \\u200bout\\u200b. \\nNevertheless, our approximate search algorithm chose S \\u200bout \\u200brather than S*. This tells you that  \\nyour approximate search algorithm is failing to choose the value of S that maximizes  \\nScore\\u200bA\\u200b(\\u200bS\\u200b). In this case, the Optimization Verification test tells  you that you have a search  \\nalgorithm problem and should focus on that. For example, you could try increasing the beam  \\nwidth of beam search.   \\nCase 2: Score\\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, you know that the way you’re computing Score \\u200bA\\u200b(.) is at fault: It is failing to give a  \\nstrictly higher score to the correct output \\u200bS\\u200b* than the incorrect \\u200bS\\u200bout\\u200b. The Optimization \\nVerification test tells you that you have an objective (scoring) function problem. Thus, you  \\nshould focus on improving how you learn or approximate Score \\u200bA\\u200b(\\u200bS\\u200b) for different sentences \\u200bS\\u200b.  \\nOur discussion has focused on a single example. To apply the Optimization Verification test  \\nin practice, you should examine the errors in your dev set. For each error, you would test  \\nwhether Score \\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b). Each dev example for which this inequality holds will get  \\nmarked as an error caused by the optimization algorithm. Each example for which this does  \\nnot hold (Score \\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)) gets counted as a mistake due to the way you’re  \\ncomputing Score \\u200bA\\u200b(.).  \\nFor example, suppose you find that 95% of the errors were due to the scoring function  \\nScore\\u200bA\\u200b(.), and only 5% due to the optimization algorithm. Now  you know that no matter how  \\nmuch you improve your optimization procedure, you would realistically eliminate only ~5%  \\nof our errors. Thus, you should instead focus on improving how you estimate Score \\u200bA\\u200b(.).  \\n \\n \\n \\n \\xa0\\nPage 86 Machine Learning Yearning-Draft Andrew Ng  \\n45 General form of Optimization Verification  \\ntest \\n \\nYou can apply the Optimization Verification test when, given some input \\u200bx\\u200b, you know how  to  \\ncompute Score \\u200bx\\u200b(\\u200by\\u200b) that indicates how good a response \\u200by\\u200b is to a n input \\u200bx\\u200b. Furthermore, you \\nare using an approximate algorithm to try to find arg max \\u200by\\u200b Score\\u200bx\\u200b(\\u200by\\u200b), but suspect that the  \\nsearch algorithm is sometimes failing to find the maximum. In our previous speech  \\nrecognition example, \\u200bx=A\\u200b was an audio clip, and \\u200by=S\\u200b was the output transcript.   \\nSuppose y* is the “correct” output but the algorithm instead outputs y \\u200bout\\u200b. Then the key test is  \\nto measure whether Score \\u200bx\\u200b(y*) > Score \\u200bx\\u200b(y\\u200bout\\u200b). If this inequality holds, then we blame the  \\noptimization algorithm for the mistake. Refer to the previous chapter to make sure you  \\nunderstand the logic behind this. Otherwise, we blame the computation of Score \\u200bx\\u200b(y).  \\nLet’s look at one more example. Suppose you are building a Chinese-to-English machine  \\ntranslation system. Your system works by inputting a Chinese sentence \\u200bC\\u200b, and computing \\nsome Score \\u200bC\\u200b(\\u200bE\\u200b) for each p ossible translation \\u200bE\\u200b. For example, you might use Score \\u200bC\\u200b(\\u200bE\\u200b) = \\nP(\\u200bE\\u200b|\\u200bC\\u200b), the probability of the translation being E given that the input sentence was \\u200bC\\u200b.  \\nYour algorithm translates sentences by trying to compute:   \\n \\nHowever, the set of all possible English sentences \\u200bE \\u200bis too large, so you rely on a heuristic  \\nsearch algorithm.  \\nSuppose your algorithm outputs an incorrect translation \\u200bE\\u200bout\\u200b rather than some correct  \\ntranslation \\u200bE \\u200b*. Then the Optimization Verification test would ask you to compute whether  \\nScore\\u200bC\\u200b(\\u200bE*\\u200b) > Score \\u200bC\\u200b(\\u200bE\\u200bout\\u200b). If this inequality holds, then the Score \\u200bC\\u200b(.) correctly recognized E*  \\nas a superior output to \\u200bE\\u200bout\\u200b; thus, you would attribute this error to the approximate search  \\nalgorithm. Otherwise, you attribute this error to the computation of Score \\u200bC\\u200b(.).  \\nIt is a very common “design pattern” in AI to first learn an approximate scoring function  \\nScore\\u200bx\\u200b(.), then use an approximate maximization algorithm. If you are able to spot this  \\npattern, you will be able to use the Optimization Verification test to understand your source  \\nof errors.  \\n  \\nPage 87 Machine Learning Yearning-Draft Andrew Ng  \\n46 Reinforcement learning example  \\n \\nSuppose you are using machine learning to teach a helicopter to fly complex maneuvers.  \\nHere is a time-lapse photo of a computer-controller helicopter executing a landing with the  \\nengine turned off.  \\nThis is called an “autorotation” maneuver. It allows helicopters to land even if their engine  \\nunexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal  \\nis to use a learning algorithm to fly the helicopter through a trajectory \\u200bT \\u200bthat ends in a safe  \\nlanding.   \\nTo apply reinforcement learning, you have to develop a “Reward function” \\u200bR\\u200b(.) that gives a  \\nscore measuring how good each possible trajectory \\u200bT\\u200b is. For example, if \\u200bT \\u200bresults in the  \\nhelicopter crashing, then perhaps the reward is \\u200bR(T)\\u200b = -1,000—a huge negative reward. A \\ntrajectory \\u200bT\\u200b resulting in a safe landing might result in a positive \\u200bR(T) \\u200bwith the exact value  \\ndepending on how smooth the landing was. The reward function \\u200bR\\u200b(.) is typically chosen by  \\nhand to quantify how desirable different trajectories \\u200bT\\u200b are. It has to trade off how bumpy the  \\nlanding was, whether the helicopter landed in exactly the desired spot, how rough the ride  \\ndown was for passengers, and so on. It is not easy to design good reward functions.   \\nPage 88 Machine Learning Yearning-Draft Andrew Ng \\n \\nGiven a reward function \\u200bR(T), \\u200bthe job of the reinforcement learning algorithm is to control  \\nthe helicopter so that it  achieves max \\u200bT\\u200b R(T). \\u200bHowever, reinforcement learning algorithms  \\nmake many approximations and may not succeed in achieving this maximization.   \\nSuppose you have picked some reward \\u200bR(.)\\u200b and have run your learning algorithm. However, \\nits performance appears far worse than your human pilot—the landings are bumpier and  \\nseem less safe than what a human pilot achieves. How can you tell if the fault is with the  \\nreinforcement learning algorithm—which is trying to carry out a trajectory that achieves  \\nmax\\u200bT\\u200b \\u200bR(T)\\u200b—or if the fault  is with the reward function—which is trying to measure as well as  \\nspecify the ideal tradeoff between ride bumpiness and accuracy of landing spot?   \\nTo apply the Optimization Verification test, let \\u200bT\\u200bhum an\\u200b be the trajectory achieved by the  \\nhuman pilot, and let \\u200bT\\u200bout \\u200bbe the trajectory achieved by the algorithm. According to our  \\ndescription above, \\u200bT\\u200bhum an \\u200bis a superior trajectory to \\u200bT\\u200bout\\u200b. Thus, the key test is the following:  \\nDoes it hold true that \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) > \\u200bR\\u200b(\\u200bT\\u200bout\\u200b)?  \\nCase 1: If this inequality holds, then the reward function \\u200bR\\u200b(.) is correctly rating \\u200bT\\u200bhum an \\u200bas \\nsuperior to \\u200bT\\u200bout\\u200b. But our reinforcement learning algorithm is finding the inferior \\u200bT\\u200bout. \\u200bThis  \\nsuggests that working on improving our reinforcement learning algorithm is worthwhile.   \\nCase 2: The inequality does not hold: \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) ≤ \\u200bR\\u200b(\\u200bT\\u200bout\\u200b). This means \\u200bR \\u200b(.) assigns a worse  \\nscore to \\u200bT\\u200bhum an \\u200beven though it is the superior trajectory. You sh ould work on improving \\u200bR \\u200b(.) to \\nbetter capture the tradeoffs that correspond to a good landing.   \\nMany machine learning applications have this “pattern” of optimizing an approximate  \\nscoring function Score \\u200bx\\u200b(.) using an approximate search algorit hm. Sometimes, there is no  \\nspecified input \\u200bx\\u200b, so this reduces to just Score(.). In our example above, the scoring function  \\nwas the reward function Score( \\u200bT\\u200b)=R(\\u200bT\\u200b), and the optimization algorithm was the  \\nreinforcement learning algorithm trying to execute a good trajectory \\u200bT\\u200b.  \\nOne difference between this and earlier examples is that, rather than comparing to an  \\n“optimal” output, you were instead comparing to human-level performance \\u200bT\\u200bhum an\\u200b.We \\nassumed \\u200bT\\u200bhum an\\u200b is pretty good, even if not optimal. In general, so long as you have some y* (in  \\nthis example, \\u200bT\\u200bhum an\\u200b) that is a superior output to the performance of your current learning  \\nalgorithm—even if it is not the “optimal” output—then the Optimization Verification test can  \\nindicate whether it is more promising to improve the optimization algorithm or  the scoring  \\nfunction.  \\n \\n\\xa0\\nPage 89 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nEnd-to-end  \\ndeep learning  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 90 Machine Learning Yearning-Draft Andrew Ng  \\n47 The rise of end-to-end learning  \\n \\nSuppose you want to build a system to examine online product reviews and automatically tell  \\nyou if the writer liked or disliked that product. For example, you hope to recognize the  \\nfollowing review as highly positive:   \\nThis is a great mop!   \\nand the following as highly negative:  \\nThis mop is low quality--I regret buying it.   \\nThe problem of recognizing positive vs. negative opinions is called “sentiment classification.”  \\nTo build this system, you might build a “pipeline” of two components:  \\n1.Parser: A system that annotates the text with information identifying the most  \\nimportant words.  For example, you might use the parser to label all the adjectives  15\\nand nouns. You would therefore get the following annotated text:   \\nThis is a great \\u200bAdjectiv e\\u200b mop\\u200bNoun\\u200b! \\n2.Sentiment classifier: A learning algorithm that takes as input the annotated text and  \\npredicts the overall sentiment. The parser’s annotation could help this learning  \\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to  \\nquickly hone in on the important words such as “great,” and ignore less important  \\nwords such as “this.”   \\nWe can visualize your “pipeline” of two components as follows:   \\n \\n \\n  \\nThere has been a recent trend toward replacing pipeline systems with a single learning  \\nalgorithm. An \\u200bend-to-end learning algorithm \\u200b for this task would simply take as input  \\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:   \\n \\n15 A parser giv es a much r icher annotation of the text than this,  but this simplified descr iption will  \\nsuffice for explaining end-to-end deep lear ning.   \\nPage 91 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\n \\n \\n \\nNeural networks are commonly used in end-to-end learning systems. The term “end-to-end”  \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to  \\nthe desired output. I.e., the learning algorithm directly connects the “input end” of the  \\nsystem to the “output end.”  \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same\",\n",
       " ' abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c”  \\nsound in “cake.” This system tries to recognize the phonemes in the audio clip.  \\n3.Final recognizer: Take the sequence of recognized phonemes, and try to string them  \\ntogether into an output transcript.   \\nIn contrast, an end-to-end system might input an audio clip, and try to directly output the  \\ntranscript:   \\n \\n \\n \\nSo far, we have only described machine learning “pipelines” that are completely linear: the  \\noutput is sequentially passed from one staged to the next. Pipelines can be more complex.  \\nFor example, here is a simple architecture for an autonomous car:   \\n \\n \\n \\n \\nPage 93 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nIt has three components: One detects other cars using the camera images; one detects  \\npedestrians; then a final component plans a path for our own car that avoids the cars and  \\npedestrians.   \\nNot every component in a pipeline has to be learned. For example, the literature on “robot  \\nmotion planning” has numerous algorithms for the final path planning step for the car. Many  \\nof these algorithms do not involve learning.   \\nIn contrast, and end-to-end approach might try to take in the sensor inputs and directly  \\noutput the steering direction:   \\n \\n \\n \\nEven though end-to-end learning has seen many successes, it is not always the best  \\napproach. For example, end-to-end speech recognition works well. But I’m skeptical about  \\nend-to-end learning for autonomous driving. The next few chapters explain why.   \\n  \\nPage 94 Machine Learning Yearning-Draft Andrew Ng \\n \\n49 Pros and cons of end-to-end learning   \\n \\nConsider the same speech pipeline from our earlier example:  \\nMany parts of this pipeline were “hand-engineered”:  \\n•MFCCs are a set of hand-designed audio features. Although they provide a reasonable  \\nsummary of the audio input, they also simplify the input signal by throwing some  \\ninformation away.  \\n•Phonemes are an invention of linguists. They are an imperfect representation of speech  \\nsounds. To the extent that phonemes are a poor approximation of reality, forcing an  \\nalgorithm to use a phoneme representation will limit the speech system’s performance.  \\nThese hand-engineered components limit the potential performance of the speech system.  \\nHowever, allowing hand-engineered components also has some advantages:  \\n•The MFCC features are robust to some properties of speech that do not affect the content,  \\nsuch as speaker pitch. Thus, they help simplify the problem for the learning algorithm.  \\n•To the extent that phonemes are a reasonable representation of speech, they can also help  \\nthe learning algorithm understand basic sound components and therefore improve its  \\nperformance. \\nHaving more hand-engineered components generally allows a speech system to learn with  \\nless data. The hand-engineered knowledge captured by MFCCs and phonemes  \\n“supplements” the knowledge our algorithm acquires from data. When we don’t have much  \\ndata, this knowledge is useful.   \\nNow, consider the end-to-end system:   \\n \\n \\n \\nPage 95 Machine Learning Yearning-Draft Andrew Ng \\n \\nThis system lacks the hand-engineered knowledge. Thus, when the training set is small, it  \\nmight do worse than the hand-engineered pipeline.   \\nHowever, when the training set is large, then it is not hampered by the limitations of an  \\nMFCC or phoneme-based representation. If the learning algorithm is a large-enough neural  \\nnetwork and if it is trained with enough training data, it has the potential to do very well, and  \\nperhaps even approach the optimal error rate.   \\nEnd-to-end learning systems tend to do well when there is a lot of labeled data for “both  \\nends”—the input end and the output end. In this example, we require a large dataset of  \\n(audio, transcript) pairs. When this type of data is not available, approach end-to-end  \\nlearning with great caution.   \\nIf you are working on a machine learning problem where the training set is very small, most  \\nof your algorithm’s knowledge will have to come from your human insight. I.e., from your  \\n“hand engineering” components.   \\nIf you choose not to use an end-to-end system, you will have to decide what are the steps in  \\nyour pipeline, and how they should plug together. In the next few chapters, we’ll give some  \\nsuggestions for designing such pipelines.   \\n \\n \\n \\n  \\nPage 96 Machine Learning Yearning-Draft Andrew Ng  \\n50 Choosing pipeline components: Data  \\navailability \\n \\nWhen building a non-end-to-end pipeline system, what are good candidates for the  \\ncomponents of the pipeline? How you design the pipeline will greatly impact the overall  \\nsystem’s performance. One important factor is whether you can easily collect data to train  \\neach of the components.   \\n \\nFor example, consider this autonomous driving architecture:  \\n \\n \\n \\n \\n \\nYou can use machine learning to detect cars and pedestrians. Further, it is not hard to obtain  \\ndata for these: There are numerous computer vision datasets with large numbers of labeled  \\ncars and pedestrians. You can also use crowdsourcing (such as Amazon Mechanical Turk) to  \\nobtain even larger datasets. It is thus relatively easy to obtain training data to build a car  \\ndetector and a pedestrian detector.   \\nIn contrast, consider a pure end-to-end approach:   \\n \\n \\n \\nTo train this system, we would need a large dataset of (Image, Steering Direction) pairs. It is  \\nvery time-consuming and expensive to have people drive cars around and record their  \\nsteering direction to collect such data. You need a fleet of specially-instrumented cars, and a  \\nhuge amount of driving to cover a wide range of possible scenarios. This makes an  \\nend-to-end system difficult to train. It is much easier to obtain a large dataset of labeled car  \\nor pedestrian images.   \\nMore generally, if there is a lot of data available for training “intermediate modules” of a  \\npipeline (such as a car detector or a pedestrian detector), then you might consider using a  \\nPage 97 Machine Learning Yearning-Draft Andrew Ng \\n \\npipeline with multiple stages. This structure could be superior because you could use all that  \\navailable data to train the intermediate modules.   \\nUntil more end-to-end data becomes available, I believe the non-end-to-end approach is  \\nsignificantly more promising for autonomous driving: Its architecture better matches the  \\navailability of data.  \\n \\n \\n \\n  \\nPage 98 Machine Learning Yearning-Draft Andrew Ng  \\n51 Choosing pipeline components: Task  \\nsimplicity   \\n \\nOther than data availability, you should also consider a second factor when picking  \\ncomponents of a pipeline: How simple are the tasks solved by the individual components?  \\nYou should try to choose pipeline components that are individually easy to build or learn.  \\nBut what does it mean for a component to be “easy” to learn?   \\n \\nConsider these machine learning tasks, listed in order of increasing difficulty:   \\n1.Classifying whether an image is overexposed (like the example above)   \\n2.Classifying whether an image was taken indoor or outdoor  \\n3.Classifying whether an image contains a cat  \\n4.Classifying whether an image contains a cat with both black and white fur  \\n5.Classifying whether an image contains a Siamese cat (a particular breed of cat)  \\n \\nEach of these is a binary image classification task: You have to input an image, and output  \\neither 0 or 1. But the tasks earlier in the list seem much “easier” for a neural network to  \\nlearn. You will be able to learn the easier tasks with fewer training examples.   \\nMachine learning does not yet have a good formal definition of what makes a task easy or  \\nhard.  With the rise of deep learning and multi-layered neural networks, we sometimes say a  16\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a  \\nshallow neural network), and “hard” if it requires more computation steps (requiring a  \\ndeeper neural network). But these are informal definitions.   \\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function  \\nis the length of the shortest computer program that can produce that function. However, this theoretical concept has found  \\nfew practical applications in AI. See also: https://en.wikipedia.org/wiki/Kolmogorov_complexity  \\nPage 99 Machine Learning Yearning-Draft Andrew Ng  \\nIf you are able to take a complex task, and break it down into simpler sub-tasks, then by  \\ncoding in the steps of the sub-tasks explicitly, you are giving the algorithm prior knowledge  \\nthat can help it learn a task more efficiently.   \\n \\nSuppose you are building a Siamese cat detector. This is the pure end-to-end architecture:  \\n \\nIn contrast, you can alternatively use a pipeline with two steps:   \\n \\nThe first step (cat detector) detects all the cats in the image.   \\nPage 100 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThe second step then passes cropped images of each of the detected cats (one at a time) to a  \\ncat species classifier, and finally outputs 1 if any of the cats detected is a Siamese cat.  \\n \\nCompared to training a purely end-to-end classifier using just labels 0/1, each of the two  \\ncomponents in the pipeline--the cat detector and the cat breed classifier--seem much easier  \\nto learn and will require significantly less data.  17\\n \\n17 If you are familiar with practical object detection algorithms, you will recognize that they do not learn just with 0/1 \\nimage labels, but are instead trained with bounding boxes provided as part of the training data. A discussion of them is  \\nbeyond the scope of this chapter. See the Deep Learning specialization on Coursera (\\u200bhttp://deeplearning.ai\\u200b) if you would \\nlike to learn more about such algorithms.   \\nPage 101 Machine Learning Yearning-Draft Andrew Ng  \\nAs one final example, let’s revisit the autonomous driving pipeline.   \\n \\n \\n \\n \\n \\n \\n \\nBy using this pipeline, you are telling the algorithm that there are 3 key steps to driving: (1)  \\nDetect other cars, (2) Detect pedestrians, and (3) Plan a path for your car. Further, each of  \\nthese is a relatively simpler function--and can thus be learned with less data--than the  \\npurely end-to-end approach.  \\n \\nIn summary, when deciding what should be the components of a pipeline, try to build a  \\npipeline where each component is a relatively “simple” function that can therefore be learned  \\nfrom only a modest amount of data.   \\n \\n \\n \\n \\n  \\nPage 102 Machine Learning Yearning-Draft Andrew Ng \\n \\n52 Directly learning rich outputs  \\n \\nAn image classification algorithm will input an image \\u200bx\\u200b, and output an integer indicating the  \\nobject category. Can an algorithm instead output an entire sentence describing the image?   \\nFor example:  \\n \\n \\nx\\u200b =   \\n \\ny\\u200b = “A yellow bus driving down a road with  \\ngreen trees and green grass in the  \\nbackground.” \\nTraditional applications of supervised learning learned a function \\u200bh\\u200b:\\u200bX\\u200b→\\u200bY\\u200b, where the output  \\ny\\u200b was usually an integer or a real number. For example:   \\nProblem \\xa0 X\\xa0 Y\\xa0\\nSpam classification \\xa0 Email\\xa0\\xa0 Spam/Not spam (0/1) \\xa0\\nImage recognition \\xa0 Image\\xa0 Integer label \\xa0\\nHousing price prediction \\xa0Features of house\\xa0 Price in dollars \\xa0\\nProduct recommendation \\xa0Product & user features\\xa0 Chance of purchase \\xa0\\n \\nOne of the most exciting developments in end-to-end deep learning is that it is letting us  \\ndirectly learn \\u200by\\u200b that are much more complex than a number. In the image-captioning  \\nexample above, you can have a neural network input an image ( \\u200bx\\u200b) and directly output a  \\ncaption ( \\u200by\\u200b).  \\n \\n \\n \\n \\n \\nPage 103 Machine Learning Yearning-Draft Andrew Ng  \\nHere are more examples:  \\nProblem \\xa0 X\\xa0 Y\\xa0 Example Citation \\xa0\\nImage captioning \\xa0 Image\\xa0 Text\\xa0 Mao et al., 2014\\xa0\\nMachine translation \\xa0 English text \\xa0 French text\\xa0 Suskever et al., 2014\\xa0\\nQuestion answering \\xa0 (Text,Question) pair \\xa0\\xa0 Answer text \\xa0 Bordes et al., 2015 \\xa0\\nSpeech recognition \\xa0 Audio\\xa0 Transcription \\xa0 Hannun et al., 2015 \\xa0\\nTTS\\xa0 Text features \\xa0 Audio\\xa0 van der Oord et al., 2016\\xa0\\n \\nThis is an accelerating trend in deep learning: When you have the right (input,output)  \\nlabeled pairs, you can sometimes learn end-to-end even when the output is a sentence, an  \\nimage, audio, or other outputs that are richer than a single number.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 104 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n\\xa0\\nError analysis \\nby parts \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 105 Machine Learning Yearning-Draft Andrew Ng  \\n53 Error analysis by parts   \\n \\nSuppose your system is built using a complex machine learning pipeline, and you would like  \\nto improve the system’s performance. Which part of the pipeline should you work on  \\nimproving? By attributing errors to specific parts of the pipeline, you can decide how to  \\nprioritize your work.   \\n \\nLet’s use our Siamese cat classifier example:   \\n \\n \\nThe first part, the cat detector, detects cats and crops them out of the image. The second  \\npart, the cat breed classifier, decides if it is a Siamese cat. It is possible to spend years  \\nworking on improving either of these two pipeline components. How do you decide which  \\ncomponent(s) to focus on?   \\nBy carrying out \\u200berror analysis by parts \\u200b, you can try to attribute each mistake the  \\nalgorithm makes to one (or sometimes both) of the two parts of the pipeline. For example,  \\nthe algorithm misclassifies this image as not containing a Siamese cat (y=0) even though the  \\ncorrect label is y=1.   \\n \\nLet’s manually examine what the two steps of the algorithm did. Suppose the Siamese cat  \\ndetector had detected a cat as follows:   \\nPage 106 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThis means that the cat breed classifier is given the following image:   \\n \\n The cat breed classifier then correctly classifies this image as not containing a Siamese cat.  \\nThus, the cat breed classifier is blameless: It was given of a pile of rocks and outputted a very  \\nreasonable label y=0. Indeed, a human classifying the cropped image above would also have  \\npredicted y=0. Thus, you can clearly attribute this error to the cat detector.   \\nIf, on the other hand, the cat detector had outputted the following bounding box:  \\n \\nthen you would conclude that the cat detector had done its job, and that it was the cat breed  \\nclassifier that is at fault.   \\n \\nSay you go through 100 misclassified dev set images and find that 90 of the errors are  \\nattributable to the cat detector, and only 10 errors are attributable to the cat breed classifier.  \\nYou can safely conclude that you should focus more attention on improving the cat detector.   \\nPage 107 Machine Learning Yearning-Draft Andrew Ng  \\n \\nFurther, you have now also conveniently found 90 examples where the cat detector  \\noutputted incorrect bounding boxes. You can use these 90 examples to carry out a deeper  \\nlevel of error analysis on the cat detector to see how to improve that.   \\n \\nOur description of how you attribute error to one part of the pipeline has been informal so  \\nfar: you look at the output of each of the parts and see if you can decide which one made a  \\nmistake. This informal method could be all you need. But in the next chapter, you’ll also see  \\na more formal way of attributing error.   \\n \\n \\n  \\nPage 108 Machine Learning Yearning-Draft Andrew Ng  \\n54 Attributing error to one part   \\n \\nLet’s continue to use this example:  \\n  \\nSuppose the cat detector outputted this bounding box:   \\n \\n \\n \\nThe cat breed classifier is thus given this cropped image, whereupon it incorrectly outputs  \\ny=0, or that there is no cat in the picture.   \\n \\nThe cat detector did its job poorly. However, a highly skilled human could arguably still  \\nrecognize the Siamese cat from the poorly cropped image. So do we attribute this error to the  \\ncat detector, or the cat breed classifier, or both? It is ambiguous.  \\n \\nIf the number of ambiguous cases like these is small, you can make whatever decision you  \\nwant and get a similar result. But here is a more formal test that lets you more definitively  \\nattribute the error to exactly one part:  \\n \\n1.Replace the cat detector output with a hand-labeled bounding box.  \\nPage 109 Machine Learning Yearning-Draft Andrew Ng  \\n \\n2.Run the corresponding cropped image through the cat breed classifier. If the cat breed  \\nclassifier still misclassifies it, attribute the error to the cat breed classifier. Otherwise,  \\nattribute the error to the cat detector.   \\n \\nIn other words, run an experiment in which you give the cat breed classifier a “perfect” input.  \\nThere are two cases:  \\n \\n●Case 1: Even given a “perfect” bounding box, the cat breed classifier still incorrectly  \\noutputs y=0. In this case, clearly the cat breed classifier is at fault.  \\n●Case 2: Given a “perfect” bounding box, the breed classifier now correctly outputs  \\ny=1. This shows that if only the cat detector had given a more perfect bounding box,  \\nthen the overall system’s output would have been correct. Thus, attribute the error to  \\nthe cat detector.   \\n \\nBy carrying out this analysis on the misclassified dev set images, you can now  \\nunambiguously attribute each error to one component. This allows you to estimate the  \\nfraction of errors due to each component of the pipeline, and therefore decide where to focus  \\nyour attention.  \\n \\n \\n \\n \\n  \\nPage 110 Machine Learning Yearning-Draft Andrew Ng  \\n55 General case of error attribution   \\n \\nHere are the general steps for error attribution. Suppose the pipeline has three steps A, B  \\nand C, where A feeds directly into B, and B feeds directly into C.   \\n \\n \\n \\nFor each mistake the system makes on the dev set:  \\n \\n1.Try manually modifying A’s output to be a “perfect” output (e.g.,  the “perfect”  \\nbounding box for the cat), and run the rest of the pipeline B, C on this output. If the  \\nalgorithm now gives a correct output, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been correct; thus, you can  \\nattribute this error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B’s output to be the “perfect” output for B. If the algorithm  \\nnow gives a correct output, then attribute the error to component B. Otherwise, go on  \\nto Step 3. \\n3.Attribute the error to component C.   \\n \\nLet’s look at a more complex example:  \\nYour self-driving car uses this pipeline. How do you use error analysis by parts to decide  \\nwhich component(s) to focus on?   \\n \\nYou can map the three components to A, B, C as follows:  \\nA: Detect cars  \\nB: Detect pedestrians  \\nC: Plan path for car  \\n \\n \\nPage 111 Machine Learning Yearning-Draft Andrew Ng \\n \\nFollowing the procedure described above, suppose you test out your car on a closed track  \\nand find a case where the car chooses a more jarring steering direction than a skilled driver  \\nwould. In the self-driving world, such a case is usually called a \\u200bscenario \\u200b. You would then:  \\n  \\n1.Try manually modifying A (detecting cars)’s output to be a “perfect” output (e.g.,  \\nmanually go in and tell it where the other cars are). Run the rest of the pipeline B, C as  \\nbefore, but allow C (plan path) to use A’s now perfect output. If the algorithm now  \\nplans a much better path for the car, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been better; Thus, you can attribute  \\nthis error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B (detect pedestrian)’s output to be the “perfect” output for  \\nB. If the algorithm now gives a correct output, then attribute the error to component  \\nB. Otherwise, go on to Step 3.   \\n3.Attribute the error to component C.   \\n \\nThe components of an ML pipeline should be ordered according to a Directed Acyclic Graph  \\n(DAG), meaning that you should be able to compute them in some fixed left-to-right order,  \\nand later components should depend only on earlier components’ outputs. So long as the  \\nmapping of the components to the A->B->C order follows the DAG ordering, then the error  \\nanalysis will be fine. You might get slightly different results if you swap A and B:  \\n \\nA: Detect pedestrians (was previously \\u200bDetect cars \\u200b)  \\nB: Detect cars (was previously \\u200bDetect pedestrians \\u200b) \\nC: Plan path for car  \\n \\nBut the results of this analysis would still be valid and give good guidance for where to focus  \\nyour attention.  \\n \\n \\n  \\nPage 112 Machine Learning Yearning-Draft Andrew Ng  \\n56 Error analysis by parts and comparison to  \\nhuman-level performance   \\n \\nCarrying out error analysis on a learning algorithm is like using data science to analyze an  \\nML system’s mistakes in order to derive insights about what to do next. At its most basic,  \\nerror analysis by parts tells us what component(s) performance is (are) worth the greatest  \\neffort to improve.  \\n \\nSay you have a dataset about customers buying things on a website. A data scientist may  \\nhave many different ways of analyzing the data. She may draw many different conclusions  \\nabout whether the website should raise prices, about the lifetime value of customers acquired  \\nthrough different marketing campaigns, and so on. There is no one “right” way to analyze a  \\ndataset, and there are many possible useful insights one could draw. Similarly, there is no  \\none “right” way to carry out error analysis. Through these chapters you have learned many of  \\nthe most common design patterns for drawing useful insights about your ML system, but you  \\nshould feel free to experiment with other ways of analyzing errors as well.  \\n \\nLet’s return to the self-driving application, where a car detection algorithm outputs the  \\nlocation (and perhaps velocity) of the nearby cars, a pedestrian detection algorithm outputs  \\nthe location of the nearby pedestrians, and these two outputs are finally used to plan a path  \\nfor the car.  \\n \\nTo debug this pipeline, rather than rigorously following the procedure you saw in the  \\nprevious chapter, you could more informally ask:  \\n \\n1.How far is the Detect cars component from human-level performance at detecting  \\ncars?  \\n2.How far is the Detect pedestrians component from human-level performance?  \\nPage 113 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.How far is the overall system’s performance from human-level performance? Here,  \\nhuman-level performance assumes the human has to plan a path for the car given  \\nonly the outputs from the previous two pipeline components (rather than access to  \\nthe camera images). In other words, how does the Plan path component’s  \\nperformance compare to that of a human’s, when the human is given only the same  \\ninput?   \\n \\nIf you find that one of the components is far from human-level performance, you now have a  \\ngood case to focus on improving the performance of that component.   \\n \\nMany error analysis processes work best when we are trying to automate something humans  \\ncan do and can thus benchmark against human-level performance. Most of our preceding  \\nexamples had this implicit assumption. If you are building an ML system where the final  \\noutput or some of the intermediate components are doing things that even humans cannot  \\ndo well, then some of these procedures will not apply.  \\n \\nThis is another advantage of working on problems that humans can solve--you have more  \\npowerful error analysis tools, and thus you can prioritize your team’s work more efficiently.   \\n \\n \\n \\n  \\nPage 114 Machine Learning Yearning-Draft Andrew Ng  \\n57 Spotting a flawed ML pipeline   \\n \\nWhat if each individual component of your ML pipeline is performing at human-level  \\nperformance or near-human-level performance, but the overall pipeline falls far short of  \\nhuman-level? This usually means that the pipeline is flawed and needs to be redesigned.  \\nError analysis can also help you understand if you need to redesign your pipeline.   \\n \\n \\nIn the previous chapter, we posed the question of whether each of the three components’  \\nperformance is at human level. Suppose the answer to all three questions is yes. That is:  \\n \\n1.The Detect cars component is at (roughly) human-level performance for detecting  \\ncars from the camera images.  \\n2.The Detect pedestrians component is at (roughly) human-level performance for  \\ndetecting cars from the camera images.   \\n3.Compared to a human that has to plan a path for the car given only the outputs  \\nfrom the previous two pipeline components (rather than access to the camera  \\nimages), \\u200b the Plan path component’s performance is at a similar level.   \\n \\nHowever, your overall self-driving car is performing significantly below human-level  \\nperformance. I.e., humans given access to the camera images can plan significantly better  \\npaths for the car. What conclusion can you draw?   \\n \\nThe only possible conclusion is that the ML pipeline is flawed. In this case, the Plan path  \\ncomponent is doing as well as it can \\u200bgiven its inputs \\u200b, but the inputs do not contain enough  \\ninformation. You should ask yourself what other information, other than the outputs from  \\nthe two earlier pipeline components, is needed to plan paths very well for a car to drive. In  \\nother words, what other information does a skilled human driver need?   \\n \\nPage 115 Machine Learning Yearning-Draft Andrew Ng  \\nFor example, suppose you realize that a human driver also needs to know the location of the  \\nlane markings. This suggests that you should redesign the pipeline as follows : 18\\n \\n \\n \\n \\nUltimately, if you don’t think your pipeline as a whole will achieve human-level performance,  \\neven if every individual component has human-level performance (remember that you are  \\ncomparing to a human who is given the same input as the component), then the pipeline is  \\nflawed and should be redesigned.  \\n  \\n18 In the self-driv ing example abov e, in theory one could solv e this problem by also feeding the raw camera  \\nimage into the planning component.  Howev er, this would v iolate the design principle of “Task  simplicity”  \\ndescribed in Chapter 51,  because the path planning module now needs to input a raw image and has a v ery \\ncomplex task  to solv e. That’ s why adding a Detect lane markings component  is a better choice--it helps get the \\nimportant and prev iously missing information about lane markings to the path planning module,  but you av oid \\nmak ing any particular module ov erly complex to build/train.   \\n \\nPage 116 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xa0\\nConclusion \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 117 Machine Learning Yearning-Draft Andrew Ng  \\n58 Building a superhero team - Get your  \\nteammates to read this  \\n \\nCongratulations on finishing this book!   \\nIn Chapter 2, we talked about how this book can help you become the superhero of your  \\nteam.  \\n \\nThe only thing better than being a superhero is being part of a superhero team. I hope you’ll  \\ngive copies of this book to your friends and teammates and help create other superheroes!  \\n \\n  \\nPage 118 Machine Learning Yearning-Draft Andrew Ng ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_ques_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to document\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(page_content=t) for t in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' \\n \\n \\n \\n \\n \\n \\nMachine L earning Year ning is a   \\ndeeplear ning. ai pr oject. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n© 2018 Andrew Ng.  All Rights R eserv ed. \\n \\n  \\nPage 2 Machine Learning Yearning-Draft Andrew Ng Deeplearning.AI \\nTable of Contents \\n \\n1 Why Machine Learning Strategy  \\n2 How to use this book to help your team  \\n3 Prerequisites and Notation  \\n4 Scale drives machine learning progress  \\n5 Your development and test sets  \\n6 Your dev and test sets should come from the same distribution  \\n7 How large do the dev/test sets need to be?  \\n8 Establish a single-number evaluation metric for your team to optimize  \\n9 Optimizing and satisficing metrics  \\n10 Having a dev set and metric speeds up iterations  \\n11 When to change dev/test sets and metrics  \\n12 Takeaways: Setting up development and test sets  \\n13 Build your first system quickly, then iterate  \\n14 Error analysis: Look at dev set examples to evaluate ideas  \\n15 Evaluating multiple ideas in parallel during error analysis  \\n16 Cleaning up mislabeled dev and test set examples  \\n17 If you have a large dev set, split it into two subsets, only one of which you look at  \\n18 How big should the Eyeball and Blackbox dev sets be?  \\n19 Takeaways: Basic error analysis  \\n20 Bias and Variance: The two big sources of error  \\n21 Examples of Bias and Variance  \\n22 Comparing to the optimal error rate  \\n23 Addressing Bias and Variance  \\n24 Bias vs. Variance tradeoff  \\n25 Techniques for reducing avoidable bias  \\nPage 3 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n27 Techniques for reducing variance  \\n28 Diagnosing bias and variance: Learning curves  \\n29 Plotting training error  \\n30 Interpreting learning curves: High bias  \\n31 Interpreting learning curves: Other cases  \\n32 Plotting learning curves  \\n33 Why we compare to human-level performance  \\n34 How to define human-level performance  \\n35 Surpassing human-level performance  \\n36 When you should train and test on different distributions  \\n37 How to decide whether to use all your data  \\n38 How to decide whether to include inconsistent data  \\n39 Weighting data  \\n40 Generalizing from the training set to the dev set  \\n41 Identifying Bias, Variance, and Data Mismatch Errors  \\n42 Addressing data mismatch  \\n43 Artificial data synthesis  \\n44 The Optimization Verification test  \\n45 General form of Optimization Verification test  \\n46 Reinforcement learning example  \\n47 The rise of end-to-end learning  \\n48 More end-to-end learning examples  \\n49 Pros and cons of end-to-end learning  \\n50 Choosing pipeline components: Data availability  \\n51 Choosing pipeline components: Task simplicity  \\nPage 4 Machine Learning Yearning-Draft Andrew Ng  \\n52 Directly learning rich outputs  \\n53 Error analysis by parts  \\n54 Attributing error to one part  \\n55 General case of error attribution  \\n56 Error analysis by parts and comparison to human-level performance  \\n57 Spotting a flawed ML pipeline  \\n58 Building a superhero team - Get your teammates to read this  \\n \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 5 Machine Learning Yearning-Draft Andrew Ng  \\n1 Why Machine Learning Strategy  \\n \\nMachine learning is the foundation of countless important applications, including web  \\nsearch, email anti-spam, speech recognition, product recommendations, and more. I assume  \\nthat you or your team is working on a machine learning application, and that you want to  \\nmake rapid progress. This book will help you do so.   \\nExample:  Building a cat picture startup  \\nSay you’re building a startup that will provide an endless stream of cat pictures to cat lovers.  \\nYou use a neural network to build a computer vision system for detecting cats in pictures.  \\nBut tragically, your learning algorithm’s accuracy is not yet good enough. You are under  \\ntremendous pressure to improve your cat detector. What do you do?   \\nYour team has a lot of ideas, such as:  \\n•Get more data: Collect more pictures of cats.   \\n•Collect a more diverse training set. For example, pictures of cats in unusual positions; cats  \\nwith unusual coloration; pictures shot with a variety of camera settings; ….   \\n•Train the algorithm longer, by running more gradient descent iterations.  \\n•Try a bigger neural network, with more layers/hidden units/parameters.   \\nPage 6 Machine Learning Yearning-Draft Andrew Ng \\n \\n•Try a smaller neural network. \\n•Try adding regularization (such as L2 regularization).  \\n•Change the neural network architecture (activation function, number of hidden units, etc.)  \\n•…  \\nIf you choose well among these possible directions, you’ll build the leading cat picture  \\nplatform, and lead your company to success. If you choose poorly, you might waste months.  \\nHow do you proceed?   \\nThis book will tell you how. Most machine learning problems leave clues that tell you what’s  \\nuseful to try, and what’s not useful to try. Learning to read those clues will save you months  \\nor years of development time.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 7 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n2 How to use this book to help your team  \\n \\nAfter finishing this book, you will have a deep understanding of how to set technical  \\ndirection for a machine learning project.   \\nBut your teammates might not understand why you’re recommending a particular direction.  \\nPerhaps you want your team to define a single-number evaluation metric, but they aren’t  \\nconvinced. How do you persuade them?   \\nThat’s why I made the chapters short: So that you can print them out and get your  \\nteammates to read just the 1-2 pages you need them to know.   \\nA few changes in prioritization can have a huge effect on your team’s productivity. By helping  \\nyour team with a few such changes, I hope that you can become the superhero of your team!   \\n \\n \\n \\xa0\\nPage 8 Machine Learning Yearning-Draft Andrew Ng \\n \\n3 Prerequisites and Notation  \\n \\nIf you have taken a Machine Learning course such as my machine learning MOOC on  \\nCoursera, or if you have experience applying supervised learning, you will be able to  \\nunderstand this text.   \\nI assume you are familiar with \\u200bsupervised learning \\u200b: learning a function that maps from x  \\nto y, using labeled training examples (x,y). Supervised learning algorithms include linear  \\nregression, logistic regression, and neural networks. There are many forms of machine  \\nlearning, but the majority of Machine Learning’s practical value today comes from  \\nsupervised learning.   \\nI will frequently refer to neural networks (also known as “deep learning”). You’ll only need a  \\nbasic understanding of what they are to follow this text.   \\nIf you are not familiar with the concepts mentioned here, watch the first three weeks of  \\nvideos in the Machine Learning course on Coursera at \\u200bhttp://ml-class.org  \\n  \\nPage 9 Machine Learning Yearning-Draft Andrew Ng  \\n4 Scale drives machine learning progress  \\n \\nMany of the ideas of deep learning (neural networks) have been around for decades. Why are  \\nthese ideas taking off now?   \\nTwo of the biggest drivers of recent progress have been:  \\n•Data availability. \\u200b People are now spending more time on digital devices (laptops, mobile  \\ndevices). Their digital activities generate huge amounts of data that we can feed to our  \\nlearning algorithms.  \\n•Computational scale. \\u200bWe started just a few years ago to be able to train neural  \\nnetworks that are big enough to take advantage of the huge datasets we now have.   \\nIn detail, even as you accumulate more data, usually the performance of older learning  \\nalgorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens  \\nout,” and the algorithm stops improving even as you give it more data:   \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIt was as if the older algorithms didn’t know what to do with all the data we now have.   \\nIf you train a small neutral network (NN) on the same supervised learning task, you might  \\nget slightly better performance:   \\n \\n \\n \\nPage 10 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nHere, by “Small NN” we mean a neural network with only a small number of hidden  \\nunits/layers/parameters. Finally, if you train larger and larger neural networks, you can  \\nobtain even better performance:  1\\n \\nThus, you obtain the best performance when you (i) Train a very large neural network, so  \\nthat you are on the green curve above; (ii) Have a huge amount of data.   \\nMany other details such as neural network architecture are also important, and there has  \\nbeen much innovation here. But one of the more reliable ways to improve an algorithm’s  \\nperformance today is still to (i) train a bigger network and (ii) get more data.   \\n1 This diagram shows NNs doing better in the r egime of small datasets.  This effect is less consistent \\nthan the effect of NNs doing well in the r egime of huge datasets.  In the small data regime,  depending  \\non how the features ar e hand-engineer ed, traditional algor ithms may or  may not do better.  For \\nexample,  if you hav e 20 training examples,  it might not matter  much whether you use logist ic \\nregr ession or  a neur al network ; the hand-engineering of featur es will hav e a bigger effect than the  \\nchoice of algorithm.  But if you hav e 1 million examples,  I would fav or the neur al network .  \\nPage 11 Machine Learning Yearning-Draft Andrew Ng  \\nThe process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss  \\nthe details at length. We will start with general strategies that are useful for both traditional  \\nlearning algorithms and neural networks, and build up to the most modern strategies for  \\nbuilding deep learning systems.   \\nPage 12 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSetting up \\ndevelopment and \\ntest sets  \\nPage 13 Machine Learning Yearning-Draft Andrew Ng  \\n5 Your development and test sets  \\n \\nLet’s return to our earlier cat pictures example: You run a mobile app, and users are  \\nuploading pictures of many different things to your app. You want to automatically find the  \\ncat pictures.   \\nYour team gets a large training set by downloading pictures of cats (positive examples) and  \\nnon-cats (negative examples) off of different websites. They split the dataset 70%/30% into  \\ntraining and test sets. Using this data, they build a cat detector that works well on the  \\ntraining and test sets.   \\nBut when you deploy this classifier into the mobile app, you find that the performance is  \\nreally poor!   \\n         \\nWhat happened?  \\nYou figure out that the pictures users are uploading have a different look than the website  \\nimages that make up your training set: Users are uploading pictures taken with mobile  \\nphones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test  \\nsets were made of website images, your algorithm did not generalize well to the actual  \\ndistribution you care about: mobile phone pictures.   \\nBefore the modern era of big data, it was a common rule in machine learning to use a  \\nrandom 70%/30% split to form your training and test sets. This practice can work, but it’s a  \\nbad idea in more and more applications where the training distribution (website images in  \\nPage 14 Machine Learning Yearning-Draft Andrew Ng  \\nour example above) is different from the distribution you ultimately care about (mobile  \\nphone images).  \\nWe usually define:  \\n•Training set \\u200b — Which you run your learning algorithm on.  \\n•Dev (development) set \\u200b — Which you use to tune parameters, select features, and  \\nmake other decisions regarding the learning algorithm. Sometimes also called the  \\nhold-out cross validation set \\u200b.  \\n•Test set\\u200b — which you use to evaluate the performance of the algorithm, but not to make  \\nany decisions regarding what learning algorithm or parameters to use.   \\nOnce you define a dev set (development set) and test set, your team will try a lot of ideas,  \\nsuch as different learning algorithm parameters, to see what works best. The dev and test  \\nsets allow your team to quickly see how well your algorithm is doing.   \\nIn other words, \\u200bthe purpose of the dev and test sets are to direct your team toward  \\nthe most important changes to make to the machine learning system \\u200b.  \\nSo, you should do the following:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nIn other words, your test set should not simply be 30% of the available data, especially if you  \\nexpect your future data (mobile phone images) to be different in nature from your training  \\nset (website images).   \\nIf you have not yet launched your mobile app, you might not have any users yet, and thus  \\nmight not be able to get data that accurately reflects what you have to do well on in the  \\nfuture. But you might still try to approximate this. For example, ask your friends to take  \\nmobile phone pictures of cats and send them to you. Once your app is launched, you can  \\nupdate your dev/test sets using actual user data.   \\nIf you really don’t have any way of getting data that approximates what you expect to get in  \\nthe future, perhaps you can start by using website images. But you should be aware of the  \\nrisk of this leading to a system that doesn’t generalize well.   \\nIt requires judgment to decide how much to invest in developing great dev and test sets. But  \\ndon’t assume your training distribution is the same as your test distribution. Try to pick test  \\nPage 15 Machine Learning Yearning-Draft Andrew Ng  \\nexamples that reflect what you ultimately want to perform well on, rather than whatever data  \\nyou happen to have for training.   \\n \\n  \\nPage 16 Machine Learning Yearning-Draft Andrew Ng  \\n6 Your dev and test sets should come from the  \\nsame distribution  \\nYou have your cat app image data segmented into four regions, based on your largest  \\nmarkets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a dev set and a test  \\nset, say we put US and India in the dev set; China and Other in the test set. In other words,  \\nwe can randomly assign two of these segments to the dev set, and the other two to the test  \\nset, right?  \\nOnce you define the dev and test sets, your team will be focused on improving dev set  \\nperformance. Thus, the dev set should reflect the task you want to improve on the most: To  \\ndo well on all four geographies, and not only two.   \\nThere is a second problem with having different dev and test set distributions: There is a  \\nchance that your team will build something that works well on the dev set, only to find that it  \\ndoes poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid  \\nletting this happen to you.   \\nAs an example, suppose your team develops a system that works well on the dev set but not  \\nthe test set. If your dev and test sets had come from the same distribution, then you would  \\nhave a very clear diagnosis of what went wrong: You have overfit the dev set. The obvious  \\ncure is to get more dev set data.   \\nBut if the dev and test sets come from different distributions, then your options are less  \\nclear. Several things could have gone wrong:   \\n1.You had overfit to the dev set.   \\n2.The test set is harder than the dev set. So your algorithm might be doing as well as could  \\nbe expected, and no further significant improvement is possible.   \\nPage 17 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.The test set is not necessarily harder, but just different, from the dev set. So what works  \\nwell on the dev set just does not work well on the test set. In this case, a lot of your work  \\nto improve dev set performance might be wasted effort.   \\nWorking on machine learning applications is hard enough. Having mismatched dev and test  \\nsets introduces additional uncertainty about whether improving on the dev set distribution  \\nalso improves test set performance. Having mismatched dev and test sets makes it harder to  \\nfigure out what is and isn’t working, and thus makes it harder to prioritize what to work on.   \\nIf you are working on a 3rd party benchmark problem, their creator might have specified dev  \\nand test sets that come from different distributions. Luck, rather than skill, will have a  \\ngreater impact on your performance on such benchmarks compared to if the dev and test  \\nsets come from the same distribution. It is an important research problem to develop  \\nlearning algorithms that are trained on one distribution and generalize well to another. But if  \\nyour goal is to make progress on a specific machine learning application rather than make  \\nresearch progress, I  recommend trying to choose dev and test sets that are drawn from the  \\nsame distribution. This will make your team more efficient.   \\n \\n \\n \\n \\xa0\\nPage 18 Machine Learning Yearning-Draft Andrew Ng  \\n7 How large do the dev/test sets need to be?  \\n \\nThe dev set should be large enough to detect differences between algorithms that you are  \\ntrying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an  \\naccuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1%  \\ndifference. Compared to other machine learning problems I’ve seen, a 100 example dev set is  \\nsmall. Dev sets with sizes from 1,000 to 10,000 examples are common. With 10,000  \\nexamples, you will have a good chance of detecting an improvement of 0.1%.  2\\nFor mature and important applications—for example, advertising, web search, and product  \\nrecommendations—I have also seen teams that are highly motivated to eke out even a 0.01%  \\nimprovement, since it has a direct impact on the company’s profits. In this case, the dev set  \\ncould be much larger than 10,000, in order to detect even smaller improvements.   \\nHow about the size of the test set? It should be large enough to give high confidence in the  \\noverall performance of your system. One popular heuristic had been to use 30% of your data  \\nfor your test set. This works well when you have a modest number of examples—say 100 to  \\n10,000 examples. But in the era of big data where we now have machine learning problems  \\nwith sometimes more than a billion examples, the fraction of data allocated to dev/test sets  \\nhas been shrinking, even as the absolute number of examples in the dev/test sets has been  \\ngrowing. There is no need to have excessively large dev/test sets beyond what is needed to  \\nevaluate the performance of your algorithms.   \\n2 In theory,  one could also test if a change to an algorithm mak es a statistically significant difference \\non the dev  set. In pr actice,  most teams don’t bother  with this (unless they are publishing academic  \\nresearch papers),  and I usually do not find statistical significance tests useful for measuring inter im \\nprogr ess.  \\nPage 19 Machine Learning Yearning-Draft Andrew Ng  \\n8 Establish a single-number evaluation metric  \\nfor your team to optimize   \\n \\nClassification accuracy is an example of a \\u200bsingle-number evaluation metric \\u200b: You run \\nyour classifier on the dev set (or test set), and get back a single number about what fraction  \\nof examples it classified correctly. According to this metric, if classifier A obtains 97%  \\naccuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.  \\nIn contrast, Precision and Recall  is not a single-number evaluation metric: It gives two  3\\nnumbers for assessing your classifier. Having multiple-number evaluation metrics makes it  \\nharder to compare algorithms. Suppose your algorithms perform as follows:   \\n \\nClassifier \\xa0 Precision \\xa0 Recall\\xa0\\nA\\xa0 95% 90% \\nB\\xa0 98% 85% \\n \\nHere, neither classifier is obviously superior, so it doesn’t immediately guide you toward  \\npicking one. \\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\n \\nDuring development, your team will try a lot of ideas about algorithm architecture, model  \\nparameters, choice of features, etc. Having a \\u200bsingle-number evaluation metric \\u200b such as  \\naccuracy allows you to sort all your models according to their performance on this metric,  \\nand quickly decide what is working best.   \\nIf you really care about both Precision and Recall, I recommend using one of the standard  \\nways to combine them into a single number. For example, one could take the average of  \\nprecision and recall, to end up with a single number.  Alternatively, you can compute the “F1  \\n3 The Precision of a cat classifier  is the fraction of images in the dev  (or test) set it labeled as cats that \\nreally are cats.  Its Recall is the percentage of all cat images in the dev  (or test) set that it cor rectly  \\nlabeled as a cat.  There is often a tradeoff between hav ing high pr ecision and high recall.   \\nPage 20 Machine Learning Yearning-Draft Andrew Ng  \\nscore,” which is a modified way of computing their average, and works better than simply  \\ntaking the mean. 4\\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\nB\\xa0 98% 85% 91.0%  \\n \\nHaving a single-number evaluation metric speeds up your ability to make a decision when  \\nyou are selecting among a large number of classifiers. It gives a clear preference ranking  \\namong all of them, and therefore a clear direction for progress.   \\nAs a final example, suppose you are separately tracking the accuracy of your cat classifier in  \\nfour key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By  \\ntaking an average or weighted average of these four numbers, you end up with a single  \\nnumber metric. Taking an average or weighted average is one of the most common ways to  \\ncombine multiple metrics into one.   \\n \\n \\n \\xa0\\n4 If you want to learn more about the F1  scor e, see \\u200bhttps: //en. wikipedia. org/wik i/F1_score\\u200b.  It is the  \\n“harmonic mean” between Precision and R ecall,  and is calculated as 2 /((1/Pr ecision)+(1/R ecall)).   \\nPage 21 Machine Learning Yearning-Draft Andrew Ng  \\n9 Optimizing and satisficing metrics   \\n \\nHere’s another way to combine multiple evaluation metrics.   \\nSuppose you care about both the accuracy and the running time of a learning algorithm. You  \\nneed to choose from these three classifiers:   \\nClassifier \\xa0 Accuracy \\xa0 Running time\\xa0\\nA\\xa0 90%\\xa0 80ms\\xa0\\nB\\xa0 92% 95ms\\xa0\\nC\\xa0 95% 1,500ms \\xa0\\n \\nIt seems unnatural to derive a single metric by putting accuracy and running time into a  \\nsingle formula, such as:  \\nAccuracy - 0.5*RunningTime \\nHere’s what you can do instead: First, define what is an “acceptable” running time. Lets say  \\nanything that runs in 100ms is acceptable. Then, maximize accuracy, subject to your  \\nclassifier meeting the running time criteria. Here, running time is a “satisficing  \\nmetric”—your classifier just has to be “good enough” on this metric, in the sense that it  \\nshould take at most 100ms. Accuracy is the “optimizing metric.”  \\nIf you are trading off N different criteria, such as binary file size of the model (which is  \\nimportant for mobile apps, since users don’t want to download large apps), running time,  \\nand accuracy, you might consider setting N-1 of the criteria as “satisficing” metrics. I.e., you  \\nsimply require that they meet a certain value. Then define the final one as the “optimizing”  \\nmetric. For example, set a threshold for what is acceptable for binary file size and running  \\ntime, and try to optimize accuracy given those constraints.  \\nAs a final example, suppose you are building a hardware device that uses a microphone to  \\nlisten for the user saying a particular “wakeword,” that then causes the system to wake up.  \\nExamples include Amazon Echo listening for “Alexa”; Apple Siri listening for “Hey Siri”;  \\nAndroid listening for “Okay Google”; and Baidu apps listening for “Hello Baidu.” You care  \\nabout both the false positive rate—the frequency with which the system wakes up even when  \\nno one said the wakeword—as well as the false negative rate—how often it fails to wake up  \\nwhen someone says the wakeword. One reasonable goal for the performance of this system is  \\nPage 22 Machine Learning Yearning-Draft Andrew Ng  \\nto minimize the false negative rate (optimizing metric), subject to there being no more than  \\none false positive every 24 hours of operation (satisficing metric).   \\nOnce your team is aligned on the evaluation metric to optimize, they will be able to make  \\nfaster progress.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 23 Machine Learning Yearning-Draft Andrew Ng  \\n10 Having a dev set and metric speeds up  \\niterations \\n \\nIt is very difficult to know in advance what approach will work best for a new problem. Even  \\nexperienced machine learning researchers will usually try out many dozens of ideas before  \\nthey discover something satisfactory. When building a machine learning system, I will often:   \\n1.Start off with some \\u200bidea\\u200b on how to build the system.   \\n2.Implement the idea in \\u200bcode\\u200b.  \\n3.Carry out an \\u200bexperiment \\u200b which tells me how well the idea worked. (Usually my first few  \\nideas don’t work!) Based on these learnings, go back to generate more ideas, and keep on  \\niterating.  \\nThis is an iterative process. The faster you can go round this loop, the faster you will make  \\nprogress. This is why having dev/test sets and a metric are important: Each time you try an  \\nidea, measuring your idea’s performance on the dev set lets you quickly decide if you’re  \\nheading in the right direction.   \\nIn contrast, suppose you don’t have a specific dev set and metric. So each time your team  \\ndevelops a new cat classifier, you have to incorporate it into your app, and play with the app  \\nfor a few hours to get a sense of whether the new classifier is an improvement. This would be  \\nincredibly slow! Also, if your team improves the classifier’s accuracy from 95.0% to 95.1%,  \\nyou might not be able to detect that 0.1% improvement from playing with the app. Yet a lot  \\nof progress in your system will be made by gradually accumulating dozens of these 0.1%  \\nimprovements. Having a dev set and metric allows you to very quickly detect which ideas are  \\nsuccessfully giving you small (or large) improvements, and therefore lets you quickly decide  \\nwhat ideas to keep refining, and which ones to discard.   \\n \\xa0\\nPage 24 Machine Learning Yearning-Draft Andrew Ng \\n \\n11 When to change dev/test sets and metrics   \\n \\nWhen starting out on a new project, I try to quickly choose dev/test sets, since this gives the  \\nteam a well-defined target to aim for.   \\nI typically ask my teams to come up with an initial dev/test set and an initial metric in less  \\nthan one week—rarely longer. It is better to come up with something imperfect and get going  \\nquickly, rather than overthink this. But this one week timeline does not apply to mature  \\napplications. For example, anti-spam is a mature deep learning application. I have seen  \\nteams working on already-mature systems spend months to acquire even better dev/test  \\nsets.  \\nIf you later realize that your initial dev/test set or metric missed the mark, by all means  \\nchange them quickly. For example, if your dev set + metric ranks classifier A above classifier  \\nB, but your team thinks that classifier B is actually superior for your product, then this might  \\nbe a sign that you need to change your dev/test sets or your evaluation metric.   \\nThere are three main possible causes of the dev set/metric incorrectly rating classifier A  \\nhigher:  \\n1. The actual distribution you need to do well on is different from the dev/test sets.  \\nSuppose your initial dev/test set had mainly pictures of adult cats. You ship your cat app,  \\nand find that users are uploading a lot more kitten images than expected. So, the dev/test set  \\ndistribution is not representative of the actual distribution you need to do well on. In this  \\ncase, update your dev/test sets to be more representative.   \\n \\nPage 25 Machine Learning Yearning-Draft Andrew Ng \\n \\n2. You have overfit to the dev set.  \\nThe process of repeatedly evaluating ideas on the dev set causes your algorithm to gradually  \\n“overfit” to the dev set. When you are done developing, you will evaluate your system on the  \\ntest set. If you find that your dev set performance is much better than your test set  \\nperformance, it is a sign that you have overfit to the dev set. In this case, get a fresh dev set.   \\nIf you need to track your team’s progress, you can also evaluate your system regularly—say  \\nonce per week or once per month—on the test set. But do not use the test set to make any  \\ndecisions regarding the algorithm, including whether to roll back to the previous week’s  \\nsystem. If you do so, you will start to overfit to the test set, and can no longer count on it to  \\ngive a completely unbiased estimate of your system’s performance (which you would need if  \\nyou’re publishing research papers, or perhaps using this metric to make important business  \\ndecisions).  \\n3. The metric is measuring something other than what the project needs to optimize.  \\nSuppose that for your cat application, your metric is classification accuracy. This metric  \\ncurrently ranks classifier A as superior to classifier B. But suppose you try out both  \\nalgorithms, and find classifier A is allowing occasional pornographic images to slip through.  \\nEven though classifier A is more accurate, the bad impression left by the occasional  \\npornographic image means its performance is unacceptable. What do you do?   \\nHere, the metric is failing to identify the fact that Algorithm B is in fact better than  \\nAlgorithm A for your product. So, you can no longer trust the metric to pick the best  \\nalgorithm. It is time to change evaluation metrics. For example, you can change the metric to  \\nheavily penalize letting through pornographic images.  I would strongly recommend picking  \\na new metric and using the new metric to explicitly define a new goal for the team, rather  \\nthan proceeding for too long without a trusted metric and reverting to manually choosing  \\namong classifiers.  \\n \\nIt is quite common to change dev/test sets or evaluation metrics during a project. Having an  \\ninitial dev/test set and metric helps you iterate quickly. If you ever find that the dev/test sets  \\nor metric are no longer pointing your team in the right direction, it’s not a big deal! Just  \\nchange them and make sure your team knows about the new direction.   \\n \\n \\xa0\\nPage 26 Machine Learning Yearning-Draft Andrew Ng  \\n12 Takeaways: Setting up development and  \\ntest sets  \\n \\n•Choose dev and test sets from a distribution that reflects what data you expect to get in  \\nthe future and want to do well on. This may not be the same as your training data’s  \\ndistribution.   \\n•Choose dev and test sets from the same distribution if possible.   \\n•Choose a single-number evaluation metric for your team to optimize. If there are multiple  \\ngoals that you care about, consider combining them into a single formula (such as  \\naveraging multiple error metrics) or defining satisficing and optimizing metrics.   \\n•Machine learning is a highly iterative process: You may try many dozens of ideas before  \\nfinding one that you’re satisfied with.   \\n•Having dev/test sets and a single-number evaluation metric helps you quickly evaluate  \\nalgorithms, and therefore iterate faster.   \\n•When starting out on a brand new application, try to establish dev/test sets and a metric  \\nquickly, say in less than a week. It might be okay to take longer on mature applications.   \\n•The old heuristic of a 70%/30% train/test split does not apply for problems where you  \\nhave a lot of data; the dev and test sets can be much less than 30% of the data.   \\n•Your dev set should be large enough to detect meaningful changes in the accuracy of your  \\nalgorithm, but not necessarily much larger. Your test set should be big enough to give you  \\na confident estimate of the final performance of your system.   \\n•If your dev set and metric are no longer pointing your team in the right direction, quickly  \\nchange them: (i) If you had overfit the dev set, get more dev set data. (ii) If the actual  \\ndistribution you care about is different from the dev/test set distribution, get new  \\ndev/test set data. (iii) If your metric is no longer measuring what is most important to  \\nyou, change the metric.   \\n \\n \\n \\nPage 27 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\nBasic Error \\nAnalysis \\n\\xa0\\n\\xa0 \\xa0\\nPage 28 Machine Learning Yearning-Draft Andrew Ng  \\n13 Build your first system quickly, then iterate  \\n \\nYou want to build a new email anti-spam system. Your team has several ideas:   \\n•Collect a huge training set of spam email. For example, set up a “honeypot”: deliberately  \\nsend fake email addresses to known spammers, so that you can automatically harvest the  \\nspam messages they send to those addresses.  \\n•Develop features for understanding the text content of the email.   \\n•Develop features for understanding the email envelope/header features to show what set  \\nof internet servers the message went through.   \\n•and more.  \\nEven though I have worked extensively on anti-spam, I would still have a hard time picking  \\none of these directions. It is even harder if you are not an expert in the application area.   \\nSo don’t start off trying to design and build the perfect system. Instead, build and train a  \\nbasic system quickly—perhaps in just a few days.  Even if the basic system is far from the  5\\n“best” system you can build, it is valuable to examine how the basic system functions: you  \\nwill  quickly find clues that show you the most promising directions in which to invest your  \\ntime. These next few chapters will show you how to read these clues.  \\n \\n\\xa0\\n\\xa0 \\xa0\\n5 This adv ice is meant for readers wanting to build AI applications,  rather  than those whose goal is to  \\npublish academic papers.  I will later  return to the topic of doing research.   \\nPage 29 Machine Learning Yearning-Draft Andrew Ng  \\n14 Error analysis: Look at dev set examples to  \\nevaluate ideas  \\nWhen you play with your cat app, you notice several examples where it mistakes dogs for  \\ncats. Some dogs do look like cats!   \\nA team member proposes incorporating 3rd party software that will make the system do  \\nbetter on dog images. These changes will take a month, and the team member is  \\nenthusiastic. Should you ask them to go ahead?   \\nBefore investing a month on this task, I recommend that you first estimate how much it will  \\nactually improve the system’s accuracy. Then you can more rationally decide if this is worth  \\nthe month of development time, or if you’re better off using that time on other tasks.   \\nIn detail, here’s what you can do:   \\n1.Gather a sample of 100 dev set examples that your system \\u200bmisclassified \\u200b. I.e., examples  \\nthat your system made an error on.   \\n2.Look at these examples manually, and count what fraction of them are dog images.   \\nThe process of looking at misclassified examples is called \\u200berror analysis \\u200b. In this example, if \\nyou find that only 5% of the misclassified images are dogs, then no matter how much you  \\nimprove your algorithm’s performance on dog images, you won’t get rid of more than 5% of  \\nyour errors. In other words, 5% is a “ceiling” (meaning maximum possible amount) for how  \\nmuch the proposed project could help. Thus, if your overall system is currently 90% accurate  \\n(10% error), this improvement is likely to result in at best 90.5% accuracy (or 9.5% error,  \\nwhich is 5% less error than the original 10% error).   \\nPage 30 Machine Learning Yearning-Draft Andrew Ng \\n \\nIn contrast, if you find that 50% of the mistakes are dogs, then you can be more confident  \\nthat the proposed project will have a big impact. It could boost accuracy from 90% to 95% (a  \\n50% relative reduction in error, from 10% down to 5%).   \\nThis simple counting procedure of error analysis gives you a quick way to estimate the  \\npossible value of incorporating the 3rd party software for dog images. It provides a  \\nquantitative basis on which to decide whether to make this investment.   \\nError analysis can often help you figure out how promising different directions are. I’ve seen  \\nmany engineers reluctant to carry out error analysis. It often feels more exciting to just jump  \\nin and implement some idea, rather than question if the idea is worth the time investment.  \\nThis is a common mistake: It might result in your team spending a month only to realize  \\nafterward that it resulted in little benefit.   \\nManually examining 100 examples does not take long. Even if you take one minute per  \\nimage, you’d be done in under two hours. These two hours could save you a month of wasted  \\neffort.  \\nError Analysis \\u200b refers to the process of examining dev set examples that your algorithm  \\nmisclassified, so that you can understand the underlying causes of the errors. This can help  \\nyou prioritize projects—as in this example—and inspire new directions, which we will discuss  \\nnext. The next few chapters will also present best practices for carrying out error analyses.   \\n \\n \\n  \\nPage 31 Machine Learning Yearning-Draft Andrew Ng  \\n15 Evaluating multiple ideas in parallel during  \\nerror analysis \\n \\nYour team has several ideas for improving the cat detector:  \\n•Fix the problem of your algorithm recognizing \\u200bdogs\\u200b as cats. \\n•Fix the problem of your algorithm recognizing \\u200bgreat cats \\u200b (lions, panthers, etc.) as house  \\ncats (pets).   \\n•Improve the system’s performance on \\u200bblurry\\u200b images.  \\n•… \\nYou can efficiently evaluate all of these ideas in parallel. I usually create a spreadsheet and  \\nfill it out while looking through ~100 misclassified dev set images. I also jot down comments  \\nthat might help me remember specific examples. To illustrate this process, let’s look at a  \\nspreadsheet you might produce with a small dev set of four examples:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments \\xa0\\n1 ✔ \\xa0   Unusual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken at \\xa0\\nzoo on rainy day\\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n% of total \\xa0 25%\\xa0 50%\\xa0 50%\\xa0  \\n \\nImage #3 above has both the Great Cat and the Blurry columns checked. Furthermore,  \\nbecause it is possible for one example to be associated with multiple categories, the  \\npercentages at the bottom may not add up to 100%.   \\nAlthough you may first formulate the categories (Dog, Great cat, Blurry) then categorize the  \\nexamples by hand, in practice, once you start looking through examples, you will probably be  \\ninspired to propose new error categories. For example, say you go through a dozen images  \\nand realize a lot of mistakes occur with Instagram-filtered pictures. You can go back and add  \\na new “Instagram” column to the spreadsheet. Manually looking at examples that the  \\nalgorithm misclassified and asking how/whether you as a human could have labeled the  \\nPage 32 Machine Learning Yearning-Draft Andrew Ng  \\npicture correctly will often inspire you to come up with new categories of errors and  \\nsolutions.  \\nThe most helpful error categories will be ones that you have an idea for improving. For  \\nexample, the Instagram category will be most helpful to add if you have an idea to “undo”  \\nInstagram filters and recover the original image. But you don’t have to restrict yourself only  \\nto error categories you know how to improve; the goal of this process is to build your  \\nintuition about the most promising areas to focus on.   \\nError analysis is an iterative process. Don’t worry if you start off with no categories in mind.  \\nAfter looking at a couple of images, you might come up with a few ideas for error categories.  \\nAfter manually categorizing some images, you might think of  new categories and re-examine  \\nthe images in light of the new categories, and so on.   \\nSuppose you finish carrying out error analysis on 100 misclassified dev set examples and get  \\nthe following:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments\\xa0\\n1 ✔ \\xa0   Usual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken \\xa0\\nat zoo on rainy day \\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n…\\xa0 …\\xa0 …\\xa0 …\\xa0 ...\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0  \\n \\nYou now know that working on a project to address the Dog mistakes can eliminate 8% of  \\nthe errors at most. Working on Great Cat or Blurry image errors could help eliminate more  \\nerrors. Therefore, you might pick one of the two latter categories to focus on. If your team  \\nhas enough people to pursue multiple directions in parallel, you can also ask some engineers  \\nto work on Great Cats and others to work on Blurry images.   \\nError analysis does not produce a rigid mathematical formula that tells you what the highest  \\npriority task should be. You also have to take into account how much progress you expect to  \\nmake on different categories and the amount of work needed to tackle each one.   \\n\\xa0\\n\\xa0\\nPage 33 Machine Learning Yearning-Draft Andrew Ng  \\n16 Cleaning up mislabeled dev and test set  \\nexamples  \\n \\nDuring error analysis, you might notice that some examples in your dev set are mislabeled.  \\nWhen I say “mislabeled” here, I mean that the pictures were already mislabeled by a human  \\nlabeler even before the algorithm encountered it. I.e., the class label in an example \\u200b(x,y)\\u200b has \\nan incorrect value for \\u200by\\u200b. For example, perhaps some pictures that are not cats are mislabeled  \\nas containing a cat, and vice versa. If you suspect the fraction of mislabeled images is  \\nsignificant, add a category to keep track of the fraction of examples mislabeled:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Mislabeled \\xa0 Comments \\xa0\\n…\\xa0     \\n98    ✔ \\xa0 Labeler missed cat \\xa0\\nin background \\xa0\\n99  ✔ \\xa0    \\n100    ✔ \\xa0 Drawing of a cat;\\xa0\\nnot a real cat. \\xa0\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0 6%\\xa0  \\n \\nShould you correct the labels in your dev set? Remember that the goal of the dev set is to  \\nhelp you quickly evaluate algorithms so that you can tell if Algorithm A or B is better. If the  \\nfraction of the dev set that is mislabeled impedes your ability to make these judgments, then  \\nit is worth spending time to fix the mislabeled dev set labels.   \\nFor example, suppose your classifier’s performance is:  \\n•Overall accuracy on dev set.………………. 90% (10% overall error.)  \\n•Errors due to mislabeled examples……. 0.6% (6% of dev set errors.)   \\n•Errors due to other causes………………… 9.4% (94% of dev set errors)  \\nHere, the 0.6% inaccuracy due to mislabeling might not be significant enough relative to the  \\n9.4% of errors you could be improving. There is no harm in manually fixing the mislabeled  \\nimages in the dev set, but it is not crucial to do so: It might be fine not knowing whether your  \\nsystem has 10% or 9.4% overall error. \\nSuppose you keep improving the cat classifier and reach the following performance:   \\nPage 34 Machine Learning Yearning-Draft Andrew Ng  \\n•Overall accuracy on dev set.………………. 98.0% (2.0% overall error.) \\n•Errors due to mislabeled examples……. 0.6%. (30% of dev set errors.)   \\n•Errors due to other causes………………… 1.4% (70% of dev set errors)  \\n30% of your errors are due to the mislabeled dev set images, adding significant error to your  \\nestimates of accuracy. It is now worthwhile to improve the quality of the labels in the dev set.  \\nTackling the mislabeled examples will help you figure out if a classifier’s error is closer to  \\n1.4% or 2%—a significant relative difference.   \\nIt is not uncommon to start off tolerating some mislabeled dev/test set examples, only later  \\nto change your mind as your system improves so that the fraction of mislabeled examples  \\ngrows relative to the total set of errors.   \\nThe last chapter explained how you can improve error categories such as Dog, Great Cat and  \\nBlurry through algorithmic improvements. You have learned in this chapter that you can  \\nwork on the Mislabeled category as well—through improving the data’s labels.   \\nWhatever process you apply to fixing dev set labels, remember to apply it to the test set  \\nlabels too so that your dev and test sets continue to be drawn from the same distribution.  \\nFixing your dev and test sets together would prevent the problem we discussed in Chapter 6,  \\nwhere your team optimizes for dev set performance only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\n'),\n",
       " Document(page_content=' only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\nto use the result in an academic research paper or need a completely unbiased measure of  \\ntest set accuracy.  \\xa0\\n\\xa0\\n  \\nPage 35 Machine Learning Yearning-Draft Andrew Ng  \\n17 If you have a large dev set, split it into two  \\nsubsets, only one of which you look at   \\n \\nSuppose you have a large dev set of 5,000 examples in which you have a 20% error rate.  \\nThus, your algorithm is misclassifying ~1,000 dev images. It takes a long time to manually  \\nexamine 1,000 images, so we might decide not to use all of them in the error analysis.   \\nIn this case, I would explicitly split the dev set into two subsets, one of which you look at, and  \\none of which you don’t. You will more rapidly overfit the portion that you are manually  \\nlooking at. You can use the portion you are not manually looking at to tune parameters.   \\nLet\\u200b’\\u200bs continue our example above, in which the algorithm is misclassifying 1,000 out of  \\n5,000 dev set examples. Suppose we want to manually examine about 100 errors for error  \\nanalysis (10% of the errors). You should randomly select 10% of the dev set and place that  \\ninto what we’ll call an \\u200bEyeball dev set \\u200b to remind ourselves that we are looking at it with our  \\neyes. (For a project on speech recognition, in which you would be listening to audio clips,  \\nperhaps you would call this set an Ear dev set instead). The Eyeball dev set therefore has 500  \\nexamples, of which we would expect our algorithm to misclassify about 100.   \\nThe second subset of the dev set, called the \\u200bBlackbox dev set \\u200b, will have the remaining  \\n4500 examples. You can use the Blackbox dev set to evaluate classifiers automatically by  \\nmeasuring their error rates. You can also use it to select among algorithms or tune  \\nhyperparameters. However, you should avoid looking at it with your eyes. We use the term  \\n“Blackbox” because we will only use this subset of the data to obtain “Blackbox” evaluations  \\nof classifiers.   \\nPage 36 Machine Learning Yearning-Draft Andrew Ng \\n \\nWhy do we explicitly separate the dev set into Eyeball and Blackbox dev sets? Since you will  \\ngain intuition about the examples in the Eyeball dev set, you will start to overfit the Eyeball  \\ndev set faster. If you see the performance on the Eyeball dev set improving much more  \\nrapidly than the performance on the Blackbox dev set, you have overfit the Eyeball dev set.  \\nIn this case, you might need to discard it and find a new Eyeball dev set by moving more  \\nexamples from the Blackbox dev set into the Eyeball dev set or by acquiring new labeled  \\ndata.   \\nExplicitly splitting your dev set into Eyeball and Blackbox dev sets allows you to tell when  \\nyour manual error analysis process is causing you to overfit the Eyeball portion of your data.   \\n \\n  \\nPage 37 Machine Learning Yearning-Draft Andrew Ng \\n \\n18 How big should the Eyeball and Blackbox  \\ndev sets be?  \\nYour Eyeball dev set should be large enough to give you a sense of your algorithm’s major  \\nerror categories. If you are working on a task that humans do well (such as recognizing cats  \\nin images), here are some rough guidelines:   \\n•An eyeball dev set in which your classifier makes 10 mistakes would be considered very  \\nsmall. With just 10 errors, it’s hard to accurately estimate the impact of different error  \\ncategories. But if you have very little data and cannot afford to put more into the Eyeball  \\ndev set, it \\u200b’\\u200bs better than nothing and will help with project prioritization.   \\n•If your classifier makes ~20 mistakes on eyeball dev examples, you would start to get a  \\nrough sense of the major error sources.   \\n•With ~50 mistakes, you would get a good sense of the major error sources.  \\n•With ~100 mistakes, you would get a very good sense of the major sources of errors. I’ve  \\nseen people manually analyze even more errors—sometimes as many as 500. There is no  \\nharm in this as long as you have enough data.   \\nSay your classifier has a 5% error rate. To make sure you have ~100 misclassified examples  \\nin the Eyeball dev set, the Eyeball dev set would have to have about 2,000 examples (since  \\n0.05*2,000 = 100). The lower your classifier’s error rate, the larger your Eyeball dev set  \\nneeds to be in order to get a large enough set of errors to analyze.   \\nIf you are working on a task that even humans cannot do well, then the exercise of examining  \\nan Eyeball dev set will not be as helpful because it is harder to figure out why the algorithm  \\ndidn’t classify an example correctly. In this case, you might omit having an Eyeball dev set.  \\nWe discuss guidelines for such problems in a later chapter.   \\nPage 38 Machine Learning Yearning-Draft Andrew Ng \\n \\nHow about the Blackbox dev set? We previously said that dev sets of around 1,000-10,000  \\nexamples are common. To refine that statement, a Blackbox dev set of 1,000-10,000  \\nexamples will often give you enough data to tune hyperparameters and select among models,  \\nthough there is little harm in having even more data. A Blackbox dev set of 100 would be  \\nsmall but still useful.   \\nIf you have a small dev set, then you might not have enough data to split into Eyeball and  \\nBlackbox dev sets that are both large enough to serve their purposes. Instead, your entire dev  \\nset might have to be used as the Eyeball dev set—i.e., you would manually examine all the  \\ndev set data.   \\nBetween the Eyeball and Blackbox dev sets, I consider the Eyeball dev set more important  \\n(assuming that you are working on a problem that humans can solve well and that examining  \\nthe examples helps you gain insight). If you only have an Eyeball dev set, you can perform  \\nerror analyses, model selection and hyperparameter tuning all on that set. The downside of  \\nhaving only an Eyeball dev set is that the risk of overfitting the dev set is greater.   \\nIf you have plentiful access to data, then the size of the Eyeball dev set would be determined  \\nmainly by how many examples you have time to manually analyze. For example, I’ve rarely  \\nseen anyone manually analyze more than 1,000 errors.  \\n \\n \\n \\xa0\\nPage 39 Machine Learning Yearning-Draft Andrew Ng \\n \\n19 Takeaways: Basic error analysis  \\n \\n•When you start a new project, especially if it is in an area in which you are not an expert,  \\nit is hard to correctly guess the most promising directions.   \\n•So don’t start off trying to design and build the perfect system. Instead build and train a  \\nbasic system as quickly as possible—perhaps in a few days. Then use error analysis to  \\nhelp you identify the most promising directions and iteratively improve your algorithm  \\nfrom there.  \\n•Carry out error analysis by manually examining ~100 dev set examples the algorithm  \\nmisclassifies and counting the major categories of errors. Use this information to  \\nprioritize what types of errors to work on fixing.   \\n•Consider splitting the dev set into an Eyeball dev set, which you will manually examine,  \\nand a Blackbox dev set, which you will not manually examine. If performance on the  \\nEyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev  \\nset and should consider acquiring more data for it.   \\n•The Eyeball dev set should be big enough so that your algorithm misclassifies enough  \\nexamples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is sufficient  \\nfor many applications.  \\n•If your dev set is not big enough to split this way, just use the entire dev set as an Eyeball  \\ndev set for manual error analysis, model selection, and hyperparameter tuning.   \\n \\n \\n \\n  \\nPage 40 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBias and Variance  \\n \\n \\n \\n \\n \\n \\xa0\\nPage 41 Machine Learning Yearning-Draft Andrew Ng  \\n20 Bias and Variance: The two big sources of  \\nerror  \\n \\nSuppose your training, dev and test sets all come from the same distribution. Then you  \\nshould always try to get more training data, since that can only improve performance, right?  \\nEven though having more data can’t hurt, unfortunately it doesn’t always help as much as  \\nyou might hope. It could be a waste of time to work on getting more data. So, how do you  \\ndecide when to add data, and when not to bother?  \\nThere are two major sources of error in machine learning: bias and variance. Understanding  \\nthem will help you decide whether adding data, as well as other tactics to improve  \\nperformance, are a good use of time.   \\nSuppose you hope to build a cat recognizer that has 5% error. Right now, your training set  \\nhas an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding  \\ntraining data probably won’t help much. You should focus on other changes. Indeed, adding  \\nmore examples to your training set only makes it harder for your algorithm to do well on the  \\ntraining set. (We explain why in a later chapter.)   \\nIf your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error  \\n(95% accuracy), then the first problem to solve is to improve your algorithm \\u200b’\\u200bs performance  \\non your training set. Your dev/test set performance is usually worse than your training set  \\nperformance. So if you are getting 85% accuracy on the examples your algorithm has seen,  \\nthere’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen.   \\nSuppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break  \\nthe 16% error into two components:   \\n•First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of  \\nthis informally as the algorithm’s \\u200bbias\\u200b.  \\n•Second, how much worse the algorithm does on the dev (or test) set than the training set.  \\nIn this example, it does 1% worse on the dev set than the training set. We think of this  \\ninformally as the algorithm’s \\u200bvariance \\u200b. 6\\n6 The field of statistics has mor e for mal definitions of bias and v ariance that we won’ t worr y about.  \\nRoughly,  the bias is the err or rate of your algor ithm on your  training set when you hav e a v ery large  \\ntraining set.  The variance is how much worse you do on the test set compar ed to the tr aining set in  \\nPage 42 Machine Learning Yearning-Draft Andrew Ng  \\nSome changes to a learning algorithm can address the first component of error— \\u200bbias\\u200b—and \\nimprove its performance on the training set. Some changes address the second  \\ncomponent— \\u200bvariance \\u200b—and help it generalize better from the training set to the dev/test  \\nsets.  To select the most promising changes, it is incredibly useful to understand which of  7\\nthese two components of error is more pressing to address.  \\nDeveloping good intuition about Bias and Variance will help you choose effective changes for  \\nyour algorithm.  \\n\\xa0\\n\\xa0  \\nthis setting.  When your err or metr ic is mean sq uared er ror, you can write down formulas specifying \\nthese two q uantities,  and pr ove that T otal Err or = Bias + V ariance.  But for  our purposes of deciding  \\nhow to mak e progress on an ML  problem,  the more informal definition of bias and v ariance giv en \\nhere will suffice.   \\n7 There are also some methods that can simultaneously red uce bias and v ariance,  by mak ing major  \\nchanges to the system architecture.  But these tend to be harder to identify and implement.   \\nPage 43 Machine Learning Yearning-Draft Andrew Ng  \\n21 Examples of Bias and Variance \\n \\nConsider our cat classification task. An “ideal” classifier (such as a human) might achieve  \\nnearly perfect performance in this task.   \\nSuppose your algorithm performs as follows:  \\n•Training error = 1%  \\n•Dev error = 11%  \\nWhat problem does it have? Applying the definitions from the previous chapter, we estimate  \\nthe bias as 1%, and the variance as 10% (=11%-1%). Thus, it has \\u200bhigh variance \\u200b. The \\nclassifier has very low training error, but it is failing to generalize to the dev set. This is also  \\ncalled \\u200boverfitting \\u200b. \\nNow consider this:  \\n•Training error = 15%  \\n•Dev error = 16% \\nWe estimate the bias as 15%, and variance as 1%. This classifier is fitting the training set  \\npoorly with 15% error, but its error on the dev set is barely higher than the training error.  \\nThis classifier therefore has \\u200bhigh bias \\u200b, but low variance. We say that this algorithm is  \\nunderfitting \\u200b. \\nNow, consider this:  \\n•Training error = 15%  \\n•Dev error = 30% \\nWe estimate the bias as 15%, and variance as 15%. This classifier has \\u200bhigh bias and high  \\nvariance \\u200b: It is doing poorly on the training set, and therefore has high bias, and its  \\nperformance on the dev set is even worse, so it also has high variance. The  \\noverfitting/underfitting terminology is hard to apply here since the classifier is  \\nsimultaneously overfitting and underfitting.   \\n \\n \\nPage 44 Machine Learning Yearning-Draft Andrew Ng  \\nFinally, consider this:  \\n•Training error = 0.5%  \\n•Dev error = 1% \\nThis classifier is doing well, as it has low bias and low variance. Congratulations on achieving  \\nthis great performance!  \\xa0\\n  \\nPage 45 Machine Learning Yearning-Draft Andrew Ng  \\n22 Comparing to the optimal error rate  \\n \\nIn our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal”  \\nclassifier—is nearly 0%. A human looking at a picture would be able to recognize if it  \\ncontains a cat almost all the time; thus, we can hope for a machine that would do just as well.   \\nOther problems are harder. For example, suppose that you are building a speech recognition  \\nsystem, and find that 14% of the audio clips have so much background noise or are so  \\nunintelligible that even a human cannot recognize what was said. In this case, even the most  \\n“optimal” speech recognition system might have error around 14%.   \\nSuppose that on this speech recognition problem, your algorithm achieves:   \\n•Training error = 15%  \\n•Dev error = 30% \\nThe training set performance is already close to the optimal error rate of 14%. Thus, there is  \\nnot much room for improvement in terms of bias or in terms of training set performance.  \\nHowever, this algorithm is not generalizing well to the dev set; thus there is ample room for  \\nimprovement in the errors due to variance.   \\nThis example is similar to the third example from the previous chapter, which also had a  \\ntraining error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training  \\nerror of 15% leaves much room for improvement. This suggests bias-reducing changes might  \\nbe fruitful. But if the optimal error rate is 14%, then the same training set performance tells  \\nus that there’s little room for improvement in the classifier’s bias.   \\nFor problems where the optimal error rate is far from zero, here \\u200b’\\u200bs a more detailed  \\nbreakdown of an algorithm \\u200b’\\u200bs error. Continuing with our speech recognition example above,  \\nthe total dev set error of 30% can be broken down as follows (a similar analysis can be  \\napplied to the test set error):   \\n•Optimal error rate (“unavoidable bias”) \\u200b: 14%. Suppose we decide that, even with the  \\nbest possible speech system in the world, we would still suffer 14% error. We can think of  \\nthis as the “unavoidable” part of a learning algorithm \\u200b’\\u200bs bias.   \\nPage 46 Machine Learning Yearning-Draft Andrew Ng  \\n•Avoidable bias \\u200b: 1%. This is calculated as the difference between the training error and  \\nthe optimal error rate.  8\\n•Variance \\u200b: 15%. The difference between the dev error and the training error.   \\nTo relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:   9\\nBias = Optimal error rate (“unavoidable bias”) + Avoidable bias  \\nThe “avoidable bias” reflects how much worse your algorithm performs on the training set  \\nthan the “optimal classifier.”   \\nThe concept of variance remains the same as before. In theory, we can always reduce  \\nvariance to nearly zero by training on a massive training set. Thus, all variance is “avoidable”  \\nwith a sufficiently large dataset, so there is no such thing as “unavoidable variance.”   \\nConsider one more example, where the optimal error rate is 14%, and we have:   \\n•Training error = 15%  \\n•Dev error = 16% \\nWhereas in the previous chapter we called this a high bias classifier, now we would say that  \\nerror from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm  \\nis already doing well, with little room for improvement. It is only 2% worse than the optimal  \\nerror rate.   \\nWe see from these examples that knowing the optimal error rate is helpful for guiding our  \\nnext steps. In statistics, the optimal error rate is also called \\u200bBayes error rate \\u200b, or Bayes \\nrate.  \\nHow do we know what the optimal error rate is? For tasks that humans are reasonably good  \\nat, such as recognizing pictures or transcribing audio clips, you can ask a human to provide  \\nlabels then measure the accuracy of the human labels relative to your training set. This  \\nwould give an estimate of the optimal error rate. If you are working on a problem that even  \\n8 If this number  is negativ e, you are doing better  on the tr aining set than the optimal er ror rate.  This  \\nmeans you are ov erfitting on the training set,  and the algor ithm has ov er-memorized the tr aining set.  \\nYou should focus on v ariance r eduction methods rather than on fur ther  bias reduction methods.  \\n9 These definitions ar e chosen  to conv ey insight on how to impr ove your  lear ning algorithm.  These \\ndefinitions ar e different than how statisticians define Bias and V ariance.  Technically,  what I define \\nhere as “Bias” should be called “Error  we attribute to bias”;  and “Av oidable bias” should be “err or we \\nattribute to the lear ning algorithm’ s bias that is ov er the optimal er ror rate. ”  \\nPage 47 Machine Learning Yearning-Draft Andrew Ng  \\nhumans have a hard time solving (e.g., predicting what movie to recommend, or what ad to  \\nshow to a user) it can be hard to estimate the optimal error rate.   \\nIn the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss  \\nin more detail the process of comparing a learning algorithm’s performance to human-level  \\nperformance.  \\nIn the last few chapters, you learned how to estimate avoidable/unavoidable bias and  \\nvariance by looking at training and dev set error rates. The next chapter will discuss how you  \\ncan use insights from such an analysis to prioritize techniques that reduce bias vs.  \\ntechniques that reduce variance. There are very different techniques that you should apply  \\ndepending on whether your project’s current problem is high (avoidable) bias or high  \\nvariance. Read on!   \\n  \\nPage 48 Machine Learning Yearning-Draft Andrew Ng  \\n23 Addressing Bias and Variance \\n \\nHere is the simplest formula for addressing bias and variance issues:  \\n•If you have high avoidable bias, increase the size of your model (for example, increase the  \\nsize of your neural network by adding layers/neurons).  \\n•If you have high variance, add data to your training set.  \\nIf you are able to increase the neural network size and increase training data without limit, it  \\nis possible to do very well on many learning problems.   \\nIn practice, increasing the size of your model will eventually cause you to run into  \\ncomputational problems because training very large models is slow. You might also exhaust  \\nyour ability to acquire more training data. (Even on the internet, there is only a finite  \\nnumber of cat pictures!)   \\nDifferent model architectures—for example, different neural network architectures—will  \\nhave different amounts of bias/variance for your problem. A lot of recent deep learning  \\nresearch has developed many innovative model architectures. So if you are using neural  \\nnetworks, the academic literature can be a great source of inspiration. There are also many  \\ngreat open-source implementations on github. But the results of trying new architectures are  \\nless predictable than the simple formula of increasing the model size and adding data.   \\nIncreasing the model size generally reduces bias, but it might also increase variance and the  \\nrisk of overfitting. However, this overfitting problem usually arises only when you are not  \\nusing regularization. If you include a well-designed regularization method, then you can  \\nusually safely increase the size of the model without increasing overfitting.   \\nSuppose you are applying deep learning, with L2 regularization or dropout, with the  \\nregularization parameter that performs best on the dev set. If you increase the model size,  \\nusually your performance will stay the same or improve; it is unlikely to worsen significantly.  \\nThe only reason to avoid using a bigger model is the increased computational cost.   \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 49 Machine Learning Yearning-Draft Andrew Ng  \\n24 Bias vs. Variance tradeoff \\n \\nYou might have heard of the “Bias vs. Variance tradeoff.” Of the changes you could make to  \\nmost learning algorithms, there are some that reduce bias errors but at the cost of increasing  \\nvariance, and vice versa. This creates a “trade off” between bias and variance.   \\nFor example, increasing the size of your model—adding neurons/layers in a neural network,  \\nor adding input features—generally reduces bias but could increase variance. Alternatively,  \\nadding regularization generally increases bias but reduces variance.   \\nIn the modern era, we often have access to plentiful data and can use very large neural  \\nnetworks (deep learning). Therefore, there is less of a tradeoff, and there are now more  \\noptions for reducing bias without hurting variance, and vice versa.   \\nFor example, you can usually increase a neural network size and tune the regularization  \\nmethod to reduce bias without noticeably increasing variance. By adding training data, you  \\ncan also usually reduce variance without affecting bias.   \\nIf you select a model architecture that is well suited for your task, you might also reduce bias  \\nand variance simultaneously. Selecting such an architecture can be difficult.   \\nIn the next few chapters, we discuss additional specific techniques for addressing bias and  \\nvariance.   \\n \\n  \\nPage 50 Machine Learning Yearning-Draft Andrew Ng  \\n25 Techniques for reducing avoidable bias  \\n \\nIf your learning algorithm suffers from high avoidable bias, you might try the following  \\ntechniques:  \\n•Increase the model size \\u200b(such as number of neurons/layers): This technique reduces  \\nbias, since it should allow you to fit the training set better. If you find that this increases  \\nvariance, then use regularization, which will usually eliminate the increase in variance.  \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm eliminate a  \\nparticular category of errors. (We discuss this further in the next chapter.) These new  \\nfeatures could help with both bias and variance. In theory, adding more features could  \\nincrease the variance; but if you find this to be the case, then use regularization, which will  \\nusually eliminate the increase in variance.   \\n•Reduce or eliminate regularization \\u200b (L2 regularization, L1 regularization, dropout):  \\nThis will reduce avoidable bias, but increase variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\nOne method that is not helpful:   \\n•Add more training data \\u200b: This technique helps with variance problems, but it usually  \\nhas no significant effect on bias.   \\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 51 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n \\nYour algorithm must perform well on the training set before you can expect it to perform  \\nwell on the dev/test sets.   \\nIn addition to the techniques described earlier to address high bias, I sometimes also carry  \\nout an error analysis on the \\u200btraining data \\u200b, following a protocol similar to error analysis on  \\nthe Eyeball dev set. This can be useful if your algorithm has high bias—i.e., if it is not fitting  \\nthe training set well.   \\nFor example, suppose you are building a speech recognition system for an app and have  \\ncollected a training set of audio clips from volunteers. If your system is not doing well on the  \\ntraining set, you might consider listening to a set of ~100 examples that the algorithm is  \\ndoing poorly on to understand the major categories of training set errors. Similar to the dev  \\nset error analysis, you can count the errors in different categories:   \\nAudio clip \\xa0 Loud background \\xa0\\nnoise\\xa0User spoke\\xa0\\nquickly\\xa0Far from \\xa0\\nmicrophone \\xa0Comments\\xa0\\n1 ✔ \\xa0   Car noise \\xa0\\n2 ✔ \\xa0  ✔ \\xa0 Restaurant noise \\xa0\\n3  ✔ \\xa0 ✔ \\xa0 User shouting\\xa0\\nacross living room?\\xa0\\n4 ✔ \\xa0 \\xa0\\xa0  Coffeeshop \\xa0\\n% of total \\xa0 75%\\xa0 25%\\xa0 50%\\xa0  \\n \\nIn this example, you might realize that your algorithm is having a particularly hard time with  \\ntraining examples that have a lot of background noise. Thus, you might focus on techniques  \\nthat allow it to better fit training examples with background noise.   \\nYou might also double-check whether it is possible for a person to transcribe these audio  \\nclips, given the same input audio as your learning algorithm. If there is so much background  \\nnoise that it is simply impossible for anyone to make out what was said, then it might be  \\nunreasonable to expect any algorithm to correctly recognize such utterances. We will discuss  \\nthe benefits of comparing your algorithm to human-level performance in a later section.  \\n\\xa0\\n\\xa0\\nPage 52 Machine Learning Yearning-Draft Andrew Ng  \\n27 Techniques for reducing variance  \\n \\nIf your learning algorithm suffers from high variance, you might try the following  \\ntechniques:  \\n•Add more training data \\u200b: This is the simplest and most reliable way to address variance,  \\nso long as you have access to significantly more data and enough computational power to  \\nprocess the data.   \\n•Add regularization \\u200b (L2 regularization, L1 regularization, dropout): This technique \\nreduces variance but increases bias.   \\n•Add early stopping \\u200b (i.e., stop gradient descent early, based on dev set error): This  \\ntechnique reduces variance but increases bias. Early stopping behaves a lot like  \\nregularization methods, and some authors call it a regularization technique.   \\n•Feature selection to decrease number/type of input features: \\u200b This technique  \\nmight help with variance problems, but it might also increase bias. Reducing the number  \\nof features slightly (say going from 1,000 features to 900) is unlikely to have a huge effect  \\non bias. Reducing it significantly (say going from 1,000 features to 100—a 10x reduction)  \\nis more likely to have a significant effect, so long as you are not excluding too many useful  \\nfeatures. In modern deep learning, when data is plentiful, there has been a shift away from  \\nfeature selection, and we are now more likely to give all the features we have to the  \\nalgorithm and let the algorithm sort out which ones to use based on the data. But when  \\nyour training set is small, feature selection can be very useful.   \\n•Decrease the model size \\u200b(such as number of neurons/layers): \\u200bUse with caution. \\u200b This \\ntechnique could decrease variance, while possibly increasing bias. However, I don’t  \\nrecommend this technique for addressing variance. Adding regularization usually gives  \\nbetter classification performance. The advantage of reducing the model size is reducing  \\nyour computational cost and thus speeding up how quickly you can train models. If  \\nspeeding up model training is useful, then by all means consider decreasing the model size.  \\nBut if your goal is to reduce variance, and you are not concerned about the computational  \\ncost, consider adding regularization instead.   \\nHere are two additional tactics, repeated from the previous chapter on addressing bias:   \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm to eliminate a  \\nparticular category of errors. These new features could help with both bias and variance. In  \\nPage 53 Machine Learning Yearning-Draft Andrew Ng  \\ntheory, adding more features could increase the variance; but if you find this to be the case,  \\nthen use regularization, which will usually eliminate the increase in variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\n \\n \\n  \\nPage 54 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLearning curves  \\n\\xa0\\n\\xa0 \\xa0\\nPage 55 Machine Learning Yearning-Draft Andrew Ng  \\n28 Diagnosing bias and variance: Learning  \\ncurves  \\n \\nWe’ve seen some ways to estimate how much error can be attributed to avoidable bias vs.  \\nvariance. We did so by estimating the optimal error rate and computing the algorithm’s  \\ntraining set and dev set errors. Let’s discuss a technique that is even more informative:  \\nplotting a learning curve.  \\nA learning curve plots your dev set error against the number of training examples. To plot it,  \\nyou would run your algorithm using different training set sizes. For example, if you have  \\n1,000 examples, you might train separate copies of the algorithm on 100, 200, 300, …, 1000 \\nexamples. Then you could plot how dev set error varies with the training set size. Here is an  \\nexample: \\nAs the training set size increases, the dev set error should decrease.   \\nWe will often have some “desired error rate” that we hope our learning algorithm will  \\neventually achieve. For example:   \\n•If we hope for human-level performance, then the human error rate could be the “desired  \\nerror rate.”  \\n•If our learning algorithm serves some product (such as delivering cat pictures), we might  \\nhave an intuition about what level of performance is needed to give users a great  \\nexperience.  \\nPage 56 Machine Learning Yearning-Draft Andrew Ng \\n \\n•If you have worked on a important application for a long time, then you might have  \\nintuition about how much more progress you can reasonably make in the next  \\nquarter/year.   \\nAdd the desired level of performance to your learning curve:   \\n \\n \\n \\n \\n \\n \\n \\nYou can visually extrapolate the red “dev error” curve  to guess how much closer you could  \\nget to the desired level of performance by adding more data. In the example above, it looks  \\nplausible that doubling the training set size might allow you to reach the desired  \\nperformance.  \\nBut if the dev error curve has “plateaued” (i.e. flattened out), then you can immediately tell  \\nthat adding more data won’t get you to your goal:   \\nLooking at the learning curve might therefore help you avoid spending months collecting  \\ntwice as much training data, only to realize it does not help.  \\nPage 57 Machine Learning Yearning-Draft Andrew Ng \\n \\nOne downside of this process is that if you only look at the dev error curve, it can be hard to  \\nextrapolate and predict exactly where the red curve will go if you had more data. There is one  \\nadditional plot that can help you estimate the impact of adding more data: the training error.  \\nPage 58 Machine Learning Yearning-Draft Andrew Ng  \\n29 Plotting training error \\n \\nYour dev set (and test set) error should decrease as the training set size grows. But your  \\ntraining set error usually \\u200bincreases \\u200b as the training set size grows.   \\nLet’s illustrate this effect with an example. Suppose your training set has only 2 examples:  \\nOne cat image and one non-cat image. Then it is easy for the learning algorithms to  \\n“memorize” both examples in the training set, and get 0% training set error. Even if either or  \\nboth of the training examples were mislabeled, it is still easy for the algorithm to memorize  \\nboth labels.   \\nNow suppose your training set has 100 examples. Perhaps even a few examples are  \\nmislabeled, or ambiguous—some images are very blurry, so even humans cannot tell if there  \\nis a cat. Perhaps the learning algorithm can still “memorize” most or all of the training set,  \\nbut it is now harder to obtain 100% accuracy. By increasing the training set from 2 to 100  \\nexamples, you will find that the training set accuracy will drop slightly.   \\nFinally, suppose your training set has 10,000 examples. In this case, it becomes even harder  \\nfor the algorithm to perfectly fit all 10,000 examples, especially if some are ambiguous or  \\nmislabeled. Thus, your learning algorithm will do even worse on this training set.   \\nLet’s add a plot of training error to our earlier figures:   \\n \\n \\n \\n \\n \\n \\n \\nYou can see that the blue “training error” curve increases with the size of the training set.  \\nFurthermore, your algorithm usually does better on the training set than on the dev set; thus  \\nthe red dev error curve usually lies strictly above the blue training error curve.   \\nLet’s discuss next how to interpret these plots. \\xa0\\nPage 59 Machine Learning Yearning-Draft Andrew Ng \\n \\n30 Interpreting learning curves: High bias  \\n \\nSuppose your dev error curve looks like this:   \\n \\n \\n \\n \\n \\n \\nWe previously said that, if your dev error curve plateaus, you are unlikely to achieve the  \\ndesired performance just by adding data.   \\nBut it is hard to know exactly what an extrapolation of the red dev error curve will look like.  \\nIf the dev set was small, you would be even less certain because the curves could be noisy.  \\nSuppose we add the training error curve to this plot and get the following:   \\n \\n \\n \\n \\n \\n \\n \\nNow, you can be absolutely sure that adding more data will not, by itself, be sufficient. Why  \\nis that? Remember our two observations:  \\nPage 60 Machine Learning Yearning-Draft Andrew Ng \\n \\n•As we add more training data, training error can only get worse. Thus, the blue training  \\nerror curve can only stay the same or go higher, and thus it can only get further away from  \\nthe (green line) level of desired performance.   \\n•The red dev error curve is usually higher than the blue training error. Thus, there’s almost  \\nno way that adding more data would allow the red dev error curve to drop down to the  \\ndesired level of performance when even the training error is higher than the desired level  \\nof performance.  \\nExamining both the dev error curve and the training error curve on the same plot allows us  \\nto more confidently extrapolate the dev error curve.   \\nSuppose, for the sake of discussion, that the desired performance is our estimate of the  \\noptimal error rate. The figure above is then the standard “textbook” example of what a  \\nlearning curve with high avoidable bias looks like: At the largest training set  \\nsize—presumably corresponding to all the training data we have—there is a large gap  \\nbetween the training error and the desired performance, indicating large avoidable bias.  \\nFurthermore, the gap between the training and dev curves is small, indicating small  \\nvariance.   \\nPreviously, we were measuring training and dev set error only at the rightmost point of this  \\nplot, which corresponds to using all the available training data. Plotting the full learning  \\ncurve gives us a more comprehensive picture of the algorithms’ performance on different  \\ntraining set sizes.   \\n \\n  \\nPage 61 Machine Learning Yearning-Draft Andrew Ng  \\n31 Interpreting learning curves: Other cases   \\n \\nConsider this learning curve:  \\n \\nDoes this plot indicate high bias, high variance, or both?  \\nThe blue training error curve is relatively low, and the red dev error curve is much higher  \\nthan the blue training error. Thus, the bias is small, but the variance is large. Adding more  \\ntraining data will probably help close the gap between dev error and training error.  \\nNow, consider this: \\n \\nThis time, the training error is large, as it is much higher than the desired level of  \\nperformance. The dev error is also much larger than the training error. Thus, you have  \\nsignificant bias and significant variance. You will have to find a way to reduce both bias and  \\nvariance in your algorithm.   \\nPage 62 Machine Learning Yearning-Draft Andrew Ng  \\n32 Plotting learning curves  \\n \\nSuppose you have a very small training set of 100 examples. You train your algorithm using a  \\nrandomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing  \\nthe number of examples by intervals of ten. You then use these 10 data points to plot your  \\nlearning curve. You might find that the curve looks slightly noisy (meaning that the values  \\nare higher/lower than expected) at the smaller training set sizes.   \\nWhen training on just 10 randomly chosen examples, you might be unlucky and have a  \\nparticularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or,  \\nyou might get lucky and get a particularly “good” training set. Having a small training set  \\nmeans that the dev and training errors may randomly fluctuate.   \\nIf your machine learning application is heavily skewed toward one class (such as a cat  \\nclassification task where the fraction of negative examples is much larger than positive  \\nexamples), or if it has a huge number of classes (such as recognizing 100 different animal  \\nspecies), then the chance of selecting an especially “unrepresentative” or bad training set is  \\nalso larger. For example, if 80% of your examples are negative examples (y=0), and only  \\n20% are positive examples (y=1), then there is a chance that a training set of 10 examples  \\ncontains only negative examples, thus making it very difficult for the algorithm to learn  \\nsomething meaningful.   \\nIf the noise in the training curve makes it hard to see the true trends, here are two solutions:   \\n•Instead of training just one model on 10 examples, instead select several (say 3-10)  \\ndifferent randomly chosen training sets of 10 examples by sampling with replacement  10\\nfrom your original set of 100. Train a different model on each of these, and compute the  \\ntraining and dev set error of each of the resulting models. Compute and plot the average  \\ntraining error and average dev set error.  \\n•If your training set is skewed towards one class, or if it has many classes, choose a  \\n“balanced” subset instead of 10 training examples at random out of the set of 100. For  \\nexample, you can make sure that 2/10 of the examples are positive examples, and 8/10 are  \\n10 Here’s what sampling \\u200bwith replacement \\u200b means: You would randomly pick 10 different examples out of the 100 to form  \\nyour first training set. Then to form the second training set, you would again pick 10 examples, but without taking into \\naccount what had been chosen in the first training set. Thus, it is possible for one specific exa mple to appear in both the  \\nfirst and second training sets. In contrast, if you were sampling \\u200bwithout replacement \\u200b, the second training set would be  \\nchosen from just the 90 examples that had not been chosen the first time around. In practice, sampling with or without \\nreplacement shouldn’t make a huge difference, but the former is common practice.   \\nPage 63 Machine Learning Yearning-Draft Andrew Ng  \\nnegative. More generally, you can make sure the fraction of examples from each class is as  \\nclose as possible to the overall fraction in the original training set.   \\nI would not bother with either of these techniques unless you have already tried plotting  \\nlearning curves and concluded that the curves are too noisy to see the underlying trends. If  \\nyour training set is large—say over 10,000 examples—and your class distribution is not very  \\nskewed, you probably won’t need these techniques.   \\nFinally, plotting a learning curve may be computationally expensive: For example, you might  \\nhave to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training  \\nmodels with small datasets is much faster than training models with large datasets. Thus,  \\ninstead of evenly spacing out the training set sizes on a linear scale as above, you might train  \\nmodels with 1,000, 2,000, 4,000, 6,000, and 10,000 examples. This should still give you a  \\nclear sense of the trends in the learning curves. Of course, this technique is relevant only if  \\nthe computational cost of training all the additional models is significant.   \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 64 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nComparing to \\nhuman-level \\nperformance \\n\\xa0\\n\\xa0\\n  \\nPage 65 Machine Learning Yearning-Draft Andrew Ng  \\n33 Why we compare to human-level  \\nperformance  \\n \\nMany machine learning systems aim to automate things that humans do well. Examples  \\ninclude image recognition, speech recognition, and email spam classification. Learning  \\nalgorithms have also improved so much that we are now surpassing human-level  \\nperformance on more and more of these tasks.   \\nFurther, there are several reasons building an ML system is easier if you are trying to do a  \\ntask that people can do well:   \\n1. Ease of obtaining data from human labelers. \\u200b For example, since people recognize  \\ncat images well, it is straightforward for people to provide high accuracy labels for your  \\nlearning algorithm.  \\n2. Error analysis can draw on human intuition. \\u200b Suppose a speech recognition  \\nalgorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\naudio clip as “This recipe calls for a \\u200bpear\\u200b of apples,” mistaking “pair” for “pear.” You can  \\ndraw on human intuition and try to understand what information a person uses to get the  \\ncorrect transcription, and use this knowledge to modify the learning algorithm.   \\n3. Use human-level performance to estimate the optimal error rate and also set  \\na “desired error rate.” \\u200b Suppose your algorithm achieves 10% error on a task, but a person  \\nachieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\navoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\nEven though item #3 might not sound important, I find that having a reasonable and  \\nachievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\nhigh avoidable bias is incredibly valuable and opens up a menu of options to try.   \\nThere are some tasks that even humans aren’t good at. For example, picking a book to  \\nrecommend to you; or picking an ad to show a user on a website; or predicting the stock  \\nmarket. Computers already surpass the performance of most people on these tasks. With  \\nthese applications, we run into the following problems:   \\n•It is harder to obtain labels. \\u200b For example, it’s hard for human labelers to annotate a  \\ndatabase of users with the “optimal” book recommendation. If you operate a website or  \\napp that sells books, you can obtain data by showing books to users and seeing what they  \\nbuy. If you do not operate such a site, you need to find more creative ways to get data.  \\nPage 66 Machine Learning Yearning-Draft Andrew Ng  \\n•Human intuition is harder to count on. \\u200b For example, pretty much no one can  \\npredict the stock market. So if our stock prediction algorithm does no better than random  \\nguessing, it is hard to figure out how to improve it.   \\n•It is hard to know what the optimal error rate and reasonable desired error  \\nrate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\n'),\n",
       " Document(page_content=\"rate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\nprevious chapter for comparing to human-level performance apply:   \\n•Ease of obtaining labeled data from human labelers. \\u200b You can get a team of doctors  \\nto provide labels to you with a 2% error rate.  \\n•Error analysis can draw on human intuition. \\u200bBy discussing images with a team of  \\ndoctors, you can draw on their intuitions.  \\n•Use human-level performance to estimate the optimal error rate and also set  \\nachievable “desired error rate.” \\u200b It is reasonable to use 2% error as our estimate of the  \\noptimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\nhigher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\nreasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\nestimates are necessarily too high.  \\nWhen it comes to obtaining labeled data, you might not want to discuss every image with an  \\nentire team of doctors since their time is expensive. Perhaps you can have a single junior  \\ndoctor label the vast majority of cases and bring only the harder cases to more experienced  \\ndoctors or to the team of doctors.   \\nIf your system is currently at 40% error, then it doesn’t matter much whether you use a  \\njunior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\nintuitions. But if your system is already at 10% error, then defining the human-level  \\nreference as 2% gives you better tools to keep improving your system.   \\n \\xa0\\nPage 68 Machine Learning Yearning-Draft Andrew Ng  \\n35 Surpassing human-level performance   \\n \\nYou are working on speech recognition and have a dataset of audio clips. Suppose your  \\ndataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\nsystem already achieves 8% error. Can you use any of the three techniques described in  \\nChapter 33 to continue making rapid progress?  \\nIf you can identify a subset of data in which humans significantly surpass your system, then  \\nyou can still use those techniques to drive rapid progress. For example, suppose your system  \\nis much better than people at recognizing speech in noisy audio, but humans are still better  \\nat transcribing very rapidly spoken speech.  \\nFor the subset of data with rapidly spoken speech:  \\n1.You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\noutput.  \\n2.You can draw on human intuition to understand why they correctly heard a rapidly  \\nspoken utterance when your system didn’t.  \\n3.You can use human-level performance on rapidly spoken speech as a desired performance  \\ntarget. \\nMore generally, so long as there are dev set examples where humans are right and your  \\nalgorithm is wrong, then many of the techniques described earlier will apply. This is true  \\neven if, averaged over the entire dev/test set, your performance is already surpassing  \\nhuman-level performance.  \\nThere are many important machine learning applications where machines surpass human  \\nlevel performance. For example, machines are better at predicting movie ratings, how long it  \\ntakes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\nsubset of techniques apply once humans have a hard time identifying examples that the  \\nalgorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\nwhere machines already surpass human-level performance, while progress is faster when  \\nmachines are still trying to catch up to humans.   \\n \\n  \\nPage 69 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTraining and  \\ntesting on different  \\ndistributions  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 70 Machine Learning Yearning-Draft Andrew Ng  \\n36 When you should train and test on  \\ndifferent distributions  \\n \\nUsers of your cat pictures app have uploaded 10,000 images, which you have manually  \\nlabeled as containing cats or not. You also have a larger set of 200,000 images that you  \\ndownloaded off the internet. How should you define train/dev/test sets?   \\nSince the 10,000 user images closely reflect the actual probability distribution of data you  \\nwant to do well on, you might use that for your dev and test sets. If you are training a  \\ndata-hungry deep learning algorithm, you might give it the additional 200,000 internet  \\nimages for training. Thus, your training and dev/test sets come from different probability  \\ndistributions. How does this affect your work?   \\nInstead of partitioning our data into train/dev/test sets, we could take all 210,000 images we  \\nhave, and randomly shuffle them into train/dev/test sets. In this case, all the data comes  \\nfrom the same distribution. But I recommend against this method, because about  \\n205,000/210,000 ≈ 97.6% of your dev/test data would come from internet images, which  \\ndoes not reflect the actual distribution you want to do well on. Remember our  \\nrecommendation on choosing dev/test sets:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nMost of the academic literature on machine learning assumes that the training set, dev set  \\nand test set all come from the same distribution.  In the early days of machine learning, data  11\\nwas scarce. We usually only had one dataset drawn from some probability distribution. So  \\nwe would randomly split that data into train/dev/test sets, and the assumption that all the  \\ndata was coming from the same source was usually satisfied.   \\n11 There is some academic research on training and testing on differ ent distributions.  Examples  \\ninclude “domain adaptation, ” “tr ansfer lear ning” and “multitask  lear ning. ” But ther e is still a huge \\ngap between theory and practice.  If you tr ain on dataset A and test on some v ery differ ent type of data \\nB, luck could hav e a huge effect on how well  your  algor ithm performs.  (Her e, “luck ” includes the \\nresearcher’ s hand-designed features for  the particular task , as well as other factors that we just don’ t \\nunder stand yet. ) This mak es the academic study of training and testing on differ ent distributions  \\ndifficult to carr y out in a systematic way.   \\nPage 71 Machine Learning Yearning-Draft Andrew Ng  \\nBut in the era of big data, we now have access to huge training sets, such as cat internet  \\nimages. Even if the training set comes from a different distribution than the dev/test set, we  \\nstill want to use it for learning since it can provide a lot of information.  \\nFor the cat detector example, instead of putting all 10,000 user-uploaded images into the  \\ndev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining  \\n5,000 user-uploaded examples into the training set. This way, your training set of 205,000  \\nexamples contains some data that comes from your dev/test distribution along with the  \\n200,000 internet images. We will discuss in a later chapter why this method is helpful.   \\nLet’s consider a second example. Suppose you are building a speech recognition system to  \\ntranscribe street addresses for a voice-controlled mobile map/navigation app. You have  \\n20,000 examples of users speaking street addresses. But you also have 500,000 examples of  \\nother audio clips with users speaking about other topics. You might take 10,000 examples of  \\nstreet addresses for the dev/test sets, and use the remaining 10,000, plus the additional  \\n500,000 examples, for training.   \\nWe will continue to assume that your dev data and your test data come from the same  \\ndistribution. But it is important to understand that different training and dev/test  \\ndistributions offer some special challenges.   \\n  \\nPage 72 Machine Learning Yearning-Draft Andrew Ng  \\n37 How to decide whether to use all your data  \\n \\nSuppose your cat detector’s training set includes 10,000 user-uploaded images. This data  \\ncomes from the same distribution as a separate dev/test set, and represents the distribution  \\nyou care about doing well on. You also have an additional 20,000 images downloaded from  \\nthe internet. Should you provide all 20,000+10,000=30,000 images to your learning \\nalgorithm as its training set, or discard the 20,000 internet images for fear of it biasing your  \\nlearning algorithm?   \\nWhen using earlier generations of learning algorithms (such as hand-designed computer  \\nvision features, followed by a simple linear classifier) there was a real risk that merging both  \\ntypes of data would cause you to perform worse. Thus, some engineers will warn you against  \\nincluding the 20,000 internet images. \\nBut in the modern era of powerful, flexible learning algorithms—such as large neural  \\nnetworks—this risk has greatly diminished. If you can afford to build a neural network with a  \\nlarge enough number of hidden units/layers, you can safely add the 20,000 images to your  \\ntraining set. Adding the images is more likely to increase your performance.   \\nThis observation relies on the fact that there is some x —> y mapping that works well for  \\nboth types of data. In other words, there exists some system that inputs either an internet  \\nimage or a mobile app image and reliably predicts the label, even without knowing the  \\nsource of the image.   \\nAdding the additional 20,000 images has the following effects:  \\n1.It gives your neural network more examples of what cats do/do not look like. This is  \\nhelpful, since internet images and user-uploaded mobile app images do share some  \\nsimilarities. Your neural network can apply some of the knowledge acquired from internet  \\nimages to mobile app images.   \\n2.It forces the neural network to expend some of its capacity to learn about properties that  \\nare specific to internet images (such as higher resolution, different distributions of how  \\nthe images are framed, etc.) If these properties differ greatly from mobile app images, it  \\nwill “use up” some of the representational capacity of the neural network. Thus there is  \\nless capacity for recognizing data drawn from the distribution of mobile app images,  \\nwhich is what you really care about. Theoretically, this could hurt your algorithms’  \\nperformance.  \\nPage 73 Machine Learning Yearning-Draft Andrew Ng  \\nTo describe the second effect in different terms, we can turn to the fictional character  \\nSherlock Holmes, who says that your brain is like an attic; it only has a finite amount of  \\nspace. He says that “for every addition of knowledge, you forget something that you knew  \\nbefore. It is of the highest importance, therefore, not to have useless facts elbowing out the  \\nuseful ones.”   12\\nFortunately, if you have the computational capacity needed to build a big enough neural  \\nnetwork—i.e., a big enough attic—then this is not a serious concern. You have enough  \\ncapacity to learn from both internet and from mobile app images, without the two types of  \\ndata competing for capacity. Your algorithm’s “brain” is big enough that you don’t have to  \\nworry about running out of attic space.   \\nBut if you do not have a big enough neural network (or another highly flexible learning  \\nalgorithm), then you should pay more attention to your training data matching your dev/test  \\nset distribution.  \\nIf you think you have data that has no benefit,you should just leave out that data for  \\ncomputational reasons. For example, suppose your dev/test sets contain mainly casual  \\npictures of people, places, landmarks, animals. Suppose you also have a large collection of  \\nscanned historical documents:   \\n \\n \\n \\n \\n \\n \\n \\nThese documents don’t contain anything resembling a cat. They also look completely unlike  \\nyour dev/test distribution. There is no point including this data as negative examples,  \\nbecause the benefit from the first effect above is negligible—there is almost nothing your  \\nneural network can learn from this data that it can apply to your dev/test set distribution.  \\nIncluding them would waste computation resources and representation capacity of the  \\nneural network. \\xa0\\n12 \\u200bA Study  in Sc arlet\\u200b \\u200bby Arthur Conan Doyle \\nPage 74 Machine Learning Yearning-Draft Andrew Ng \\n \\n38 How to decide whether to include  \\ninconsistent data  \\n \\nSuppose you want to learn to predict housing prices in New York City. Given the size of a  \\nhouse (input feature x), you want to predict the price (target label y).   \\nHousing prices in New York City are very high. Suppose you have a second dataset of  \\nhousing prices in Detroit, Michigan, where housing prices are much lower. Should you  \\ninclude this data in your training set?   \\nGiven the same size x, the price of a house y is very different depending on whether it is in  \\nNew York City or in Detroit. If you only care about predicting New York City housing prices,  \\nputting the two datasets together will hurt your performance.  In this case, it would be better  \\nto leave out the inconsistent Detroit data.  13\\nHow is this New York City vs. Detroit example different from the  mobile app vs. internet cat  \\nimages example?  \\nThe cat image example is different because, given an input picture x, one can reliably predict  \\nthe label y indicating whether there is a cat, even without knowing if the image is an internet  \\nimage or a mobile app image. I.e., there is a function f(x) that reliably maps from the input x  \\nto the target output y, even without knowing the origin of x. Thus, the task of recognition  \\nfrom internet images is “consistent” with the task of recognition from mobile app images.  \\nThis means there was little downside (other than computational cost) to including all the  \\ndata, and some possible significant upside. In contrast, New York City and Detroit, Michigan  \\ndata are not consistent. Given the same x (size of house), the price is very different  \\ndepending on where the house is.   \\n \\n \\n  \\n13 There is one way to address the problem of Detr oit data being inconsistent with New Yor k City  \\ndata,  which is to add an extra featur e to each training example indicating the city.  Given an input \\nx—which now specifies the city—the target v alue of y is now unambiguous.  Howev er, in practice I  do \\nnot see this done fr equently.   \\nPage 75 Machine Learning Yearning-Draft Andrew Ng  \\n39 Weighting data   \\n \\nSuppose you have 200,000 images from the internet and 5,000 images from your mobile  \\napp users. There is a 40:1 ratio between the size of these datasets. In theory, so long as you  \\nbuild a huge neural network and train it long enough on all 205,000 images, there is no  \\nharm in trying to make the algorithm do well on both internet images and mobile images.   \\nBut in practice, having 40x as many internet images as mobile app images might mean you  \\nneed to spend 40x (or more) as much computational resources to model both, compared to if  \\nyou trained on only the 5,000 images.   \\nIf you don’t have huge computational resources, you could  give the internet images a much  \\nlower weight as a compromise.   \\nFor example, suppose your optimization objective is squared error (This is not a good choice  \\nfor a classification task, but it will simplify our explanation.) Thus, our learning algorithm  \\ntries to optimize:  \\n  \\nThe first sum above is over the 5,000 mobile images, and the second sum is over the  \\n200,000 internet images. You can instead optimize with an additional parameter \\u200b𝛽\\u200b:  \\n \\n If you set \\u200b𝛽\\u200b=1/40, the algorithm would give equal weight to th e 5,000 mobile images and the  \\n200,000 internet images. You can also set the parameter \\u200b𝛽\\u200b to other values, perhaps by  \\ntuning to the dev set.   \\nBy weighting the additional Internet images less, you don’t have to build as massive a neural  \\nnetwork to make sure the algorithm does well on both types of tasks. This type of  \\nre-weighting is needed only when you suspect the additional data (Internet Images) has a  \\nvery different distribution than the dev/test set, or if the additional data is much larger than  \\nthe data that came from the same distribution as the dev/test set (mobile images).   \\nPage 76 Machine Learning Yearning-Draft Andrew Ng  \\n40 Generalizing from the training set to the  \\ndev set  \\n \\nSuppose you are applying ML in a setting where the training and the dev/test distributions  \\nare different. Say, the training set contains Internet images + Mobile images, and the  \\ndev/test sets contain only Mobile images. However, the algorithm is not working well: It has  \\na much higher dev/test set error than you would like. Here are some possibilities of what  \\nmight be wrong:  \\n1.It does not do well on the training set. This is the problem of high (avoidable) bias on the  \\ntraining set distribution.   \\n2.It does well on the training set, but does not generalize well to previously unseen data  \\ndrawn from the same distribution as the training set \\u200b. This is high variance.   \\n3.It generalizes well to new data drawn from the same distribution as the training set, but  \\nnot to data drawn from the dev/test set distribution. We call this problem \\u200bdata  \\nmismatch \\u200b, since it is because the training set data is a poor match for the dev/test set  \\ndata.   \\nFor example, suppose that humans achieve near perfect performance on the cat recognition  \\ntask. Your algorithm achieves this:  \\n•1% error on the training set  \\n•1.5% error on data drawn from the same distribution as the training set that the algorithm  \\nhas not seen  \\n•10% error on the dev set   \\nIn this case, you clearly have a data mismatch problem. To address this, you might try to  \\nmake the training data more similar to the dev/test data. We discuss some techniques for  \\nthis later.  \\nIn order to diagnose to what extent an algorithm suffers from each of the problems 1-3  \\nabove, it will be useful to have another dataset. Specifically, rather than giving the algorithm  \\nall the available training data, you can split it into two subsets: The actual training set which  \\nthe algorithm will train on, and a separate set, which we will call the “Training dev” set, that  \\nwe will not train on.   \\nYou now have four subsets of data:  \\nPage 77 Machine Learning Yearning-Draft Andrew Ng  \\n•Training set. This is the data that the algorithm will learn from (e.g., Internet images +  \\nMobile images). This does not have to be drawn from the same distribution as what we  \\nreally care about (the dev/test set distribution).  \\n•Training dev set: This data is drawn from the same distribution as the training set (e.g.,  \\nInternet images + Mobile images). This is usually smaller than the training set; it only  \\nneeds to be large enough to evaluate and track the progress of our learning algorithm.   \\n•Dev set: This is drawn from the same distribution as the test set, and it reflects the  \\ndistribution of data that we ultimately care about doing well on. (E.g., mobile images.)   \\n•Test set: This is drawn from the same distribution as the dev set. (E.g., mobile images.)  \\nArmed with these four separate datasets, you can now evaluate:  \\n•Training error, by evaluating on the training set.   \\n•The algorithm’s ability to generalize to new data drawn from the training set distribution,  \\nby evaluating on the training dev set.  \\n•The algorithm’s performance on the task you care about, by evaluating on the dev and/or  \\ntest sets.   \\nMost of the guidelines in Chapters 5-7 for picking the size of the dev set also apply to the  \\ntraining dev set.  \\n \\n \\n \\n \\n \\n \\n  \\nPage 78 Machine Learning Yearning-Draft Andrew Ng  \\n41 Identifying Bias, Variance, and Data  \\nMismatch Errors  \\n \\nSuppose humans achieve almost perfect performance (≈0% error) on the cat detection task,  \\nand thus the optimal error rate is about 0%. Suppose you have:  \\n•1% error on the training set. \\n•5% error on training dev set.   \\n•5% error on the dev set.   \\nWhat does this tell you? Here, you know that you have high variance. The variance reduction  \\ntechniques described earlier should allow you to make progress.   \\nNow, suppose your algorithm achieves:   \\n•10% error on the training set.  \\n•11% error on training dev set.  \\n•12% error on the dev set.   \\nThis tells you that you have high avoidable bias on the training set. I.e., the algorithm is  \\ndoing poorly on the training set. Bias reduction techniques should help.  \\nIn the two examples above, the algorithm suffered from only high avoidable bias or high  \\nvariance. It is possible for an algorithm to suffer from any subset of high avoidable bias, high  \\nvariance, and data mismatch. For example:   \\n•10% error on the training set.   \\n•11% error on training dev set.  \\n•20% error on the dev set.   \\nThis algorithm suffers from high avoidable bias and from data mismatch. It does not,  \\nhowever, suffer from high variance on the training set distribution.   \\nIt might be easier to understand how the different types of errors relate to each other by  \\ndrawing them as entries in a table:   \\nPage 79 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\nContinuing with the example of th \\u200be cat image detector, you can see that there are two  \\ndifferent distributions of data on the x-axis. On the y-axis, we ha \\u200bve three types of error:  \\nhuman level error, error on examples the algorithm has trained on, and error on examples  \\nthe algorithm has not trained on. We can fill in the boxes with the different types of errors we  \\nidentified in the previous chapter.   \\nIf you wish, you can also fill in the remaining two boxes in this table: You can fill in the  \\nupper-right box (Human level performance on Mobile Images) by asking some humans to  \\nlabel your mobile cat images data and measure their error. You can fill in the next box by  \\ntaking the mobile cat images (Distribution B) and putting a small fraction of into the training  \\nset so that the neural network learns on it too. Then you measure the learned model’s error  \\non that subset of data. Filling in these two additional entries may sometimes give additional  \\ninsight about what the algorithm is doing on the two different distributions (Distribution A  \\nand B) of data.  \\nBy understanding which types of error the algorithm suffers from the most, you will be better  \\npositioned to decide whether to focus on reducing bias, reducing variance, or reducing data  \\nmismatch.  \\n  \\nPage 80 Machine Learning Yearning-Draft Andrew Ng  \\n42 Addressing data mismatch  \\n \\nSuppose you have developed a speech recognition system that does very well on the training  \\nset and on the training dev set. However, it does poorly on your dev set: You have a data  \\nmismatch problem. What can you do?  \\nI recommend that you: (i) Try to understand what properties of the data differ between the  \\ntraining and the dev set distributions. (ii) Try to find more training data that better matches  \\nthe dev set examples that your algorithm has trouble with.  14\\nFor example, suppose you carry out an error analysis on the speech recognition dev set: You  \\nmanually go through 100 examples, and try to understand where the algorithm is making  \\nmistakes. You find that your system does poorly because most of the audio clips in the dev  \\nset are taken within a car, whereas most of the training examples were recorded against a  \\nquiet background. The engine and road noise dramatically worsen the performance of your  \\nspeech system. In this case, you might try to acquire more training data comprising audio  \\nclips that were taken in a car. The purpose of the error analysis is to understand the  \\nsignificant differences between the training and the dev set, which is what leads to the data  \\nmismatch.  \\nIf your training and training dev sets include audio recorded within a car, you should also  \\ndouble-check your system’s performance on this subset of data. If it is doing well on the car  \\ndata in the training set but not on car data in the training dev set, then this further validates  \\nthe hypothesis that getting more car data would help. This is why we discussed the  \\npossibility of including in your training set some data drawn from the same distribution as  \\nyour dev/test set in the previous chapter. Doing so allows you to compare your performance  \\non the car data in the training set vs. the dev/test set.   \\nUnfortunately, there are no guarantees in this process. For example, if you don't have any  \\nway to get more training data that better match the dev set data, you might not have a clear  \\npath towards improving performance.  \\n \\n \\xa0\\n14There is also some research on “domain adaptation”—how to tr ain an algorithm on one distribution  \\nand hav e it gener alize to a differ ent distribution.  These methods ar e typically applicable only in  \\nspecial types of problems and are much less widely used than the ideas descr ibed in this chapter .  \\nPage 81 Machine Learning Yearning-Draft Andrew Ng  \\n43 Artificial data synthesis  \\n \\nYour speech system needs more data that sounds as if it were taken from within a car. Rather  \\nthan collecting a lot of data while driving around, there might be an easier way to get this  \\ndata: By artificially synthesizing it.  \\nSuppose you obtain a large quantity of car/road noise audio clips. You can download this  \\ndata from several websites. Suppose you also have a large training set of people speaking in a  \\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip  \\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in  \\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it  \\nwere collected inside a car.   \\nMore generally, there are several circumstances where artificial data synthesis allows you to  \\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as  \\na second example. You notice that dev set images have much more motion blur because they  \\ntend to come from cellphone users who are moving their phone slightly while taking the  \\npicture. You can take non-blurry images from the training set of internet images, and add  \\nsimulated motion blur to them, thus making them more similar to the dev set.   \\nKeep in mind that artificial data synthesis has its challenges: it is sometimes easier to create  \\nsynthetic data that appears realistic to a person than it is to create data that appears realistic  \\nto a computer. For example, suppose you have 1,000 hours of speech training data, but only  \\n1 hour of car noise. If you repeatedly use the same 1 hour of car noise with different portions  \\nfrom the original 1,000 hours of training data, you will end up with a synthetic dataset where  \\nthe same car noise is repeated over and over. While a person listening to this audio probably  \\nwould not be able to tell—all car noise sounds the same to most of us—it is possible that a  \\nlearning algorithm would “overfit” to the 1 hour of car noise. Thus, it could generalize poorly  \\nto a new audio clip where the car noise happens to sound different.   \\nAlternatively, suppose you have 1,000 unique hours of car noise, but all of it was taken from  \\njust 10 different cars. In this case, it is possible for an algorithm to “overfit” to these 10 cars  \\nand perform poorly if tested on audio from a different car. Unfortunately, these problems  \\ncan be hard to spot.   \\n \\n \\n \\nPage 82 Machine Learning Yearning-Draft Andrew Ng  \\n \\nTo take one more example, suppose you are building a computer vision system to recognize  \\ncars. Suppose you partner with a video gaming company, which has computer graphics  \\nmodels of several cars. To train your algorithm, you use the models to generate synthetic  \\nimages of cars. Even if the synthesized images look very realistic, this approach (which has  \\nbeen independently proposed by many people) will probably not work well. The video game  \\nmight have ~20 car designs in the entire video game. It is very expensive to build a 3D car  \\nmodel of a car; if you were playing the game, you probably wouldn’t notice that you’re seeing  \\nthe same cars over and over, perhaps only painted differently. I.e., this data looks very  \\nrealistic to you. But compared to the set of all cars out on roads—and therefore what you’re  \\nlikely to see in the dev/test sets—this set of 20 synthesized cars captures only a minuscule  \\nfraction of the world’s distribution of cars. Thus if your 100,000 training examples all come  \\nfrom these 20 cars, your system will “overfit” to these 20 specific car designs, and it will fail  \\nto generalize well to dev/test sets that include other car designs.   \\nWhen synthesizing data, put some thought into whether you’re really synthesizing a  \\nrepresentative set of examples. Try to avoid giving the synthesized data properties that  \\nmakes it possible for a learning algorithm to distinguish synthesized from non-synthesized  \\nexamples—such as if all the synthesized data comes from one of 20 car designs, or all the  \\nsynthesized audio comes from only 1 hour of car noise. This advice can be hard to follow.   \\nWhen working on data synthesis, my teams have sometimes taken weeks before we produced  \\ndata with details that are close enough to the actual distribution for the synthesized data to  \\nhave a significant effect. But if you are able to get the details right, you can suddenly access a  \\nfar larger training set than before.   \\n \\n \\nPage 83 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\nDebugging  \\ninference  \\nalgorithms \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 84 Machine Learning Yearning-Draft Andrew Ng  \\n44 The Optimization Verification test  \\n \\nSuppose you are building a speech recognition system. Your system works by inputting an  \\naudio clip \\u200bA\\u200b, and computing some Score \\u200bA\\u200b(\\u200bS\\u200b) for each possible  output sentence \\u200bS\\u200b. For  \\nexample, you might try to estimate Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b), the pro bability that the correct  \\noutput transcription is the sentence \\u200bS\\u200b,  given that the input audio was \\u200bA.   \\nGiven a way to compute Score \\u200bA\\u200b(\\u200bS\\u200b), you still have to find the En glish sentence \\u200bS\\u200b that \\nmaximizes it:  \\n \\nHow do you compute the “arg max” above? If the English language has 50,000 words, then  \\nthere are (50,000)\\u200bN \\u200bpossi ble sentences of length \\u200bN\\u200b—far too many to exhaustively enumerate.  \\nSo, you need to apply an approximate search algorithm, to try to find the value of \\u200bS\\u200b that  \\noptimizes (maximizes) Score \\u200bA\\u200b(\\u200bS\\u200b). One example search algorith m is “beam search,” which  \\nkeeps only \\u200bK\\u200b top candidates during the search process. (For the purposes of this chapter, you  \\ndon’t need to understand the details of beam search.) Algorithms like this are not guaranteed  \\nto find the value of \\u200bS \\u200bthat maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nSuppose that an audio clip \\u200bA\\u200b records someone saying “I love machine learning.” But instead  \\nof outputting the correct transcription, your system outputs the incorrect “I love robots.”  \\nThere are now two possibilities for what went wrong:   \\n1.Search algorithm problem \\u200b. The approximate search algorithm (beam search) failed  \\nto find the value of \\u200bS\\u200b that maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\n2.Objective (scoring function) problem. \\u200b Our estimates for Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b) were \\ninaccurate. In particular, our choice of Score \\u200bA\\u200b(\\u200bS\\u200b) failed to r ecognize that “I love machine  \\nlearning” is the correct transcription.   \\nDepending on which of these was the cause of the failure, you should prioritize your efforts  \\nvery differently. If #1 was the problem, you should work on improving the search algorithm.  \\nIf #2 was the problem, you should work on the learning algorithm that estimates Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nFacing this situation, some researchers will randomly decide to work on the search  \\nalgorithm; others will randomly work on a better way to learn values for Score \\u200bA\\u200b(S). But \\nunless you know which of these is the underlying cause of the error, your efforts could be  \\nwasted. How can you decide more systematically what to work on?   \\nPage 85 Machine Learning Yearning-Draft Andrew Ng  \\nLet S\\u200bout\\u200b be the output tran scription (“I love robots”). Let S* be the correct transcription (“I  \\nlove machine learning”). In order to understand whether #1 or #2 above is the problem, you  \\ncan perform the \\u200bOptimization Verification test \\u200b: First, compute Score \\u200bA\\u200b(\\u200bS\\u200b*) and \\nScore\\u200bA\\u200b(\\u200bS\\u200bout\\u200b). Then check w hether Score \\u200bA\\u200b(\\u200bS\\u200b*) > Score \\u200bA\\u200b(\\u200bS\\u200bout\\u200b). There are two possibilities:   \\nCase 1: Score\\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, your learning algorithm has correctly given S* a higher score than S \\u200bout\\u200b. \\nNevertheless, our approximate search algorithm chose S \\u200bout \\u200brather than S*. This tells you that  \\nyour approximate search algorithm is failing to choose the value of S that maximizes  \\nScore\\u200bA\\u200b(\\u200bS\\u200b). In this case, the Optimization Verification test tells  you that you have a search  \\nalgorithm problem and should focus on that. For example, you could try increasing the beam  \\nwidth of beam search.   \\nCase 2: Score\\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, you know that the way you’re computing Score \\u200bA\\u200b(.) is at fault: It is failing to give a  \\nstrictly higher score to the correct output \\u200bS\\u200b* than the incorrect \\u200bS\\u200bout\\u200b. The Optimization \\nVerification test tells you that you have an objective (scoring) function problem. Thus, you  \\nshould focus on improving how you learn or approximate Score \\u200bA\\u200b(\\u200bS\\u200b) for different sentences \\u200bS\\u200b.  \\nOur discussion has focused on a single example. To apply the Optimization Verification test  \\nin practice, you should examine the errors in your dev set. For each error, you would test  \\nwhether Score \\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b). Each dev example for which this inequality holds will get  \\nmarked as an error caused by the optimization algorithm. Each example for which this does  \\nnot hold (Score \\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)) gets counted as a mistake due to the way you’re  \\ncomputing Score \\u200bA\\u200b(.).  \\nFor example, suppose you find that 95% of the errors were due to the scoring function  \\nScore\\u200bA\\u200b(.), and only 5% due to the optimization algorithm. Now  you know that no matter how  \\nmuch you improve your optimization procedure, you would realistically eliminate only ~5%  \\nof our errors. Thus, you should instead focus on improving how you estimate Score \\u200bA\\u200b(.).  \\n \\n \\n \\n \\xa0\\nPage 86 Machine Learning Yearning-Draft Andrew Ng  \\n45 General form of Optimization Verification  \\ntest \\n \\nYou can apply the Optimization Verification test when, given some input \\u200bx\\u200b, you know how  to  \\ncompute Score \\u200bx\\u200b(\\u200by\\u200b) that indicates how good a response \\u200by\\u200b is to a n input \\u200bx\\u200b. Furthermore, you \\nare using an approximate algorithm to try to find arg max \\u200by\\u200b Score\\u200bx\\u200b(\\u200by\\u200b), but suspect that the  \\nsearch algorithm is sometimes failing to find the maximum. In our previous speech  \\nrecognition example, \\u200bx=A\\u200b was an audio clip, and \\u200by=S\\u200b was the output transcript.   \\nSuppose y* is the “correct” output but the algorithm instead outputs y \\u200bout\\u200b. Then the key test is  \\nto measure whether Score \\u200bx\\u200b(y*) > Score \\u200bx\\u200b(y\\u200bout\\u200b). If this inequality holds, then we blame the  \\noptimization algorithm for the mistake. Refer to the previous chapter to make sure you  \\nunderstand the logic behind this. Otherwise, we blame the computation of Score \\u200bx\\u200b(y).  \\nLet’s look at one more example. Suppose you are building a Chinese-to-English machine  \\ntranslation system. Your system works by inputting a Chinese sentence \\u200bC\\u200b, and computing \\nsome Score \\u200bC\\u200b(\\u200bE\\u200b) for each p ossible translation \\u200bE\\u200b. For example, you might use Score \\u200bC\\u200b(\\u200bE\\u200b) = \\nP(\\u200bE\\u200b|\\u200bC\\u200b), the probability of the translation being E given that the input sentence was \\u200bC\\u200b.  \\nYour algorithm translates sentences by trying to compute:   \\n \\nHowever, the set of all possible English sentences \\u200bE \\u200bis too large, so you rely on a heuristic  \\nsearch algorithm.  \\nSuppose your algorithm outputs an incorrect translation \\u200bE\\u200bout\\u200b rather than some correct  \\ntranslation \\u200bE \\u200b*. Then the Optimization Verification test would ask you to compute whether  \\nScore\\u200bC\\u200b(\\u200bE*\\u200b) > Score \\u200bC\\u200b(\\u200bE\\u200bout\\u200b). If this inequality holds, then the Score \\u200bC\\u200b(.) correctly recognized E*  \\nas a superior output to \\u200bE\\u200bout\\u200b; thus, you would attribute this error to the approximate search  \\nalgorithm. Otherwise, you attribute this error to the computation of Score \\u200bC\\u200b(.).  \\nIt is a very common “design pattern” in AI to first learn an approximate scoring function  \\nScore\\u200bx\\u200b(.), then use an approximate maximization algorithm. If you are able to spot this  \\npattern, you will be able to use the Optimization Verification test to understand your source  \\nof errors.  \\n  \\nPage 87 Machine Learning Yearning-Draft Andrew Ng  \\n46 Reinforcement learning example  \\n \\nSuppose you are using machine learning to teach a helicopter to fly complex maneuvers.  \\nHere is a time-lapse photo of a computer-controller helicopter executing a landing with the  \\nengine turned off.  \\nThis is called an “autorotation” maneuver. It allows helicopters to land even if their engine  \\nunexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal  \\nis to use a learning algorithm to fly the helicopter through a trajectory \\u200bT \\u200bthat ends in a safe  \\nlanding.   \\nTo apply reinforcement learning, you have to develop a “Reward function” \\u200bR\\u200b(.) that gives a  \\nscore measuring how good each possible trajectory \\u200bT\\u200b is. For example, if \\u200bT \\u200bresults in the  \\nhelicopter crashing, then perhaps the reward is \\u200bR(T)\\u200b = -1,000—a huge negative reward. A \\ntrajectory \\u200bT\\u200b resulting in a safe landing might result in a positive \\u200bR(T) \\u200bwith the exact value  \\ndepending on how smooth the landing was. The reward function \\u200bR\\u200b(.) is typically chosen by  \\nhand to quantify how desirable different trajectories \\u200bT\\u200b are. It has to trade off how bumpy the  \\nlanding was, whether the helicopter landed in exactly the desired spot, how rough the ride  \\ndown was for passengers, and so on. It is not easy to design good reward functions.   \\nPage 88 Machine Learning Yearning-Draft Andrew Ng \\n \\nGiven a reward function \\u200bR(T), \\u200bthe job of the reinforcement learning algorithm is to control  \\nthe helicopter so that it  achieves max \\u200bT\\u200b R(T). \\u200bHowever, reinforcement learning algorithms  \\nmake many approximations and may not succeed in achieving this maximization.   \\nSuppose you have picked some reward \\u200bR(.)\\u200b and have run your learning algorithm. However, \\nits performance appears far worse than your human pilot—the landings are bumpier and  \\nseem less safe than what a human pilot achieves. How can you tell if the fault is with the  \\nreinforcement learning algorithm—which is trying to carry out a trajectory that achieves  \\nmax\\u200bT\\u200b \\u200bR(T)\\u200b—or if the fault  is with the reward function—which is trying to measure as well as  \\nspecify the ideal tradeoff between ride bumpiness and accuracy of landing spot?   \\nTo apply the Optimization Verification test, let \\u200bT\\u200bhum an\\u200b be the trajectory achieved by the  \\nhuman pilot, and let \\u200bT\\u200bout \\u200bbe the trajectory achieved by the algorithm. According to our  \\ndescription above, \\u200bT\\u200bhum an \\u200bis a superior trajectory to \\u200bT\\u200bout\\u200b. Thus, the key test is the following:  \\nDoes it hold true that \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) > \\u200bR\\u200b(\\u200bT\\u200bout\\u200b)?  \\nCase 1: If this inequality holds, then the reward function \\u200bR\\u200b(.) is correctly rating \\u200bT\\u200bhum an \\u200bas \\nsuperior to \\u200bT\\u200bout\\u200b. But our reinforcement learning algorithm is finding the inferior \\u200bT\\u200bout. \\u200bThis  \\nsuggests that working on improving our reinforcement learning algorithm is worthwhile.   \\nCase 2: The inequality does not hold: \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) ≤ \\u200bR\\u200b(\\u200bT\\u200bout\\u200b). This means \\u200bR \\u200b(.) assigns a worse  \\nscore to \\u200bT\\u200bhum an \\u200beven though it is the superior trajectory. You sh ould work on improving \\u200bR \\u200b(.) to \\nbetter capture the tradeoffs that correspond to a good landing.   \\nMany machine learning applications have this “pattern” of optimizing an approximate  \\nscoring function Score \\u200bx\\u200b(.) using an approximate search algorit hm. Sometimes, there is no  \\nspecified input \\u200bx\\u200b, so this reduces to just Score(.). In our example above, the scoring function  \\nwas the reward function Score( \\u200bT\\u200b)=R(\\u200bT\\u200b), and the optimization algorithm was the  \\nreinforcement learning algorithm trying to execute a good trajectory \\u200bT\\u200b.  \\nOne difference between this and earlier examples is that, rather than comparing to an  \\n“optimal” output, you were instead comparing to human-level performance \\u200bT\\u200bhum an\\u200b.We \\nassumed \\u200bT\\u200bhum an\\u200b is pretty good, even if not optimal. In general, so long as you have some y* (in  \\nthis example, \\u200bT\\u200bhum an\\u200b) that is a superior output to the performance of your current learning  \\nalgorithm—even if it is not the “optimal” output—then the Optimization Verification test can  \\nindicate whether it is more promising to improve the optimization algorithm or  the scoring  \\nfunction.  \\n \\n\\xa0\\nPage 89 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nEnd-to-end  \\ndeep learning  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 90 Machine Learning Yearning-Draft Andrew Ng  \\n47 The rise of end-to-end learning  \\n \\nSuppose you want to build a system to examine online product reviews and automatically tell  \\nyou if the writer liked or disliked that product. For example, you hope to recognize the  \\nfollowing review as highly positive:   \\nThis is a great mop!   \\nand the following as highly negative:  \\nThis mop is low quality--I regret buying it.   \\nThe problem of recognizing positive vs. negative opinions is called “sentiment classification.”  \\nTo build this system, you might build a “pipeline” of two components:  \\n1.Parser: A system that annotates the text with information identifying the most  \\nimportant words.  For example, you might use the parser to label all the adjectives  15\\nand nouns. You would therefore get the following annotated text:   \\nThis is a great \\u200bAdjectiv e\\u200b mop\\u200bNoun\\u200b! \\n2.Sentiment classifier: A learning algorithm that takes as input the annotated text and  \\npredicts the overall sentiment. The parser’s annotation could help this learning  \\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to  \\nquickly hone in on the important words such as “great,” and ignore less important  \\nwords such as “this.”   \\nWe can visualize your “pipeline” of two components as follows:   \\n \\n \\n  \\nThere has been a recent trend toward replacing pipeline systems with a single learning  \\nalgorithm. An \\u200bend-to-end learning algorithm \\u200b for this task would simply take as input  \\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:   \\n \\n15 A parser giv es a much r icher annotation of the text than this,  but this simplified descr iption will  \\nsuffice for explaining end-to-end deep lear ning.   \\nPage 91 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\n \\n \\n \\nNeural networks are commonly used in end-to-end learning systems. The term “end-to-end”  \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to  \\nthe desired output. I.e., the learning algorithm directly connects the “input end” of the  \\nsystem to the “output end.”  \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same\"),\n",
       " Document(page_content=' abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c”  \\nsound in “cake.” This system tries to recognize the phonemes in the audio clip.  \\n3.Final recognizer: Take the sequence of recognized phonemes, and try to string them  \\ntogether into an output transcript.   \\nIn contrast, an end-to-end system might input an audio clip, and try to directly output the  \\ntranscript:   \\n \\n \\n \\nSo far, we have only described machine learning “pipelines” that are completely linear: the  \\noutput is sequentially passed from one staged to the next. Pipelines can be more complex.  \\nFor example, here is a simple architecture for an autonomous car:   \\n \\n \\n \\n \\nPage 93 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nIt has three components: One detects other cars using the camera images; one detects  \\npedestrians; then a final component plans a path for our own car that avoids the cars and  \\npedestrians.   \\nNot every component in a pipeline has to be learned. For example, the literature on “robot  \\nmotion planning” has numerous algorithms for the final path planning step for the car. Many  \\nof these algorithms do not involve learning.   \\nIn contrast, and end-to-end approach might try to take in the sensor inputs and directly  \\noutput the steering direction:   \\n \\n \\n \\nEven though end-to-end learning has seen many successes, it is not always the best  \\napproach. For example, end-to-end speech recognition works well. But I’m skeptical about  \\nend-to-end learning for autonomous driving. The next few chapters explain why.   \\n  \\nPage 94 Machine Learning Yearning-Draft Andrew Ng \\n \\n49 Pros and cons of end-to-end learning   \\n \\nConsider the same speech pipeline from our earlier example:  \\nMany parts of this pipeline were “hand-engineered”:  \\n•MFCCs are a set of hand-designed audio features. Although they provide a reasonable  \\nsummary of the audio input, they also simplify the input signal by throwing some  \\ninformation away.  \\n•Phonemes are an invention of linguists. They are an imperfect representation of speech  \\nsounds. To the extent that phonemes are a poor approximation of reality, forcing an  \\nalgorithm to use a phoneme representation will limit the speech system’s performance.  \\nThese hand-engineered components limit the potential performance of the speech system.  \\nHowever, allowing hand-engineered components also has some advantages:  \\n•The MFCC features are robust to some properties of speech that do not affect the content,  \\nsuch as speaker pitch. Thus, they help simplify the problem for the learning algorithm.  \\n•To the extent that phonemes are a reasonable representation of speech, they can also help  \\nthe learning algorithm understand basic sound components and therefore improve its  \\nperformance. \\nHaving more hand-engineered components generally allows a speech system to learn with  \\nless data. The hand-engineered knowledge captured by MFCCs and phonemes  \\n“supplements” the knowledge our algorithm acquires from data. When we don’t have much  \\ndata, this knowledge is useful.   \\nNow, consider the end-to-end system:   \\n \\n \\n \\nPage 95 Machine Learning Yearning-Draft Andrew Ng \\n \\nThis system lacks the hand-engineered knowledge. Thus, when the training set is small, it  \\nmight do worse than the hand-engineered pipeline.   \\nHowever, when the training set is large, then it is not hampered by the limitations of an  \\nMFCC or phoneme-based representation. If the learning algorithm is a large-enough neural  \\nnetwork and if it is trained with enough training data, it has the potential to do very well, and  \\nperhaps even approach the optimal error rate.   \\nEnd-to-end learning systems tend to do well when there is a lot of labeled data for “both  \\nends”—the input end and the output end. In this example, we require a large dataset of  \\n(audio, transcript) pairs. When this type of data is not available, approach end-to-end  \\nlearning with great caution.   \\nIf you are working on a machine learning problem where the training set is very small, most  \\nof your algorithm’s knowledge will have to come from your human insight. I.e., from your  \\n“hand engineering” components.   \\nIf you choose not to use an end-to-end system, you will have to decide what are the steps in  \\nyour pipeline, and how they should plug together. In the next few chapters, we’ll give some  \\nsuggestions for designing such pipelines.   \\n \\n \\n \\n  \\nPage 96 Machine Learning Yearning-Draft Andrew Ng  \\n50 Choosing pipeline components: Data  \\navailability \\n \\nWhen building a non-end-to-end pipeline system, what are good candidates for the  \\ncomponents of the pipeline? How you design the pipeline will greatly impact the overall  \\nsystem’s performance. One important factor is whether you can easily collect data to train  \\neach of the components.   \\n \\nFor example, consider this autonomous driving architecture:  \\n \\n \\n \\n \\n \\nYou can use machine learning to detect cars and pedestrians. Further, it is not hard to obtain  \\ndata for these: There are numerous computer vision datasets with large numbers of labeled  \\ncars and pedestrians. You can also use crowdsourcing (such as Amazon Mechanical Turk) to  \\nobtain even larger datasets. It is thus relatively easy to obtain training data to build a car  \\ndetector and a pedestrian detector.   \\nIn contrast, consider a pure end-to-end approach:   \\n \\n \\n \\nTo train this system, we would need a large dataset of (Image, Steering Direction) pairs. It is  \\nvery time-consuming and expensive to have people drive cars around and record their  \\nsteering direction to collect such data. You need a fleet of specially-instrumented cars, and a  \\nhuge amount of driving to cover a wide range of possible scenarios. This makes an  \\nend-to-end system difficult to train. It is much easier to obtain a large dataset of labeled car  \\nor pedestrian images.   \\nMore generally, if there is a lot of data available for training “intermediate modules” of a  \\npipeline (such as a car detector or a pedestrian detector), then you might consider using a  \\nPage 97 Machine Learning Yearning-Draft Andrew Ng \\n \\npipeline with multiple stages. This structure could be superior because you could use all that  \\navailable data to train the intermediate modules.   \\nUntil more end-to-end data becomes available, I believe the non-end-to-end approach is  \\nsignificantly more promising for autonomous driving: Its architecture better matches the  \\navailability of data.  \\n \\n \\n \\n  \\nPage 98 Machine Learning Yearning-Draft Andrew Ng  \\n51 Choosing pipeline components: Task  \\nsimplicity   \\n \\nOther than data availability, you should also consider a second factor when picking  \\ncomponents of a pipeline: How simple are the tasks solved by the individual components?  \\nYou should try to choose pipeline components that are individually easy to build or learn.  \\nBut what does it mean for a component to be “easy” to learn?   \\n \\nConsider these machine learning tasks, listed in order of increasing difficulty:   \\n1.Classifying whether an image is overexposed (like the example above)   \\n2.Classifying whether an image was taken indoor or outdoor  \\n3.Classifying whether an image contains a cat  \\n4.Classifying whether an image contains a cat with both black and white fur  \\n5.Classifying whether an image contains a Siamese cat (a particular breed of cat)  \\n \\nEach of these is a binary image classification task: You have to input an image, and output  \\neither 0 or 1. But the tasks earlier in the list seem much “easier” for a neural network to  \\nlearn. You will be able to learn the easier tasks with fewer training examples.   \\nMachine learning does not yet have a good formal definition of what makes a task easy or  \\nhard.  With the rise of deep learning and multi-layered neural networks, we sometimes say a  16\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a  \\nshallow neural network), and “hard” if it requires more computation steps (requiring a  \\ndeeper neural network). But these are informal definitions.   \\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function  \\nis the length of the shortest computer program that can produce that function. However, this theoretical concept has found  \\nfew practical applications in AI. See also: https://en.wikipedia.org/wiki/Kolmogorov_complexity  \\nPage 99 Machine Learning Yearning-Draft Andrew Ng  \\nIf you are able to take a complex task, and break it down into simpler sub-tasks, then by  \\ncoding in the steps of the sub-tasks explicitly, you are giving the algorithm prior knowledge  \\nthat can help it learn a task more efficiently.   \\n \\nSuppose you are building a Siamese cat detector. This is the pure end-to-end architecture:  \\n \\nIn contrast, you can alternatively use a pipeline with two steps:   \\n \\nThe first step (cat detector) detects all the cats in the image.   \\nPage 100 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThe second step then passes cropped images of each of the detected cats (one at a time) to a  \\ncat species classifier, and finally outputs 1 if any of the cats detected is a Siamese cat.  \\n \\nCompared to training a purely end-to-end classifier using just labels 0/1, each of the two  \\ncomponents in the pipeline--the cat detector and the cat breed classifier--seem much easier  \\nto learn and will require significantly less data.  17\\n \\n17 If you are familiar with practical object detection algorithms, you will recognize that they do not learn just with 0/1 \\nimage labels, but are instead trained with bounding boxes provided as part of the training data. A discussion of them is  \\nbeyond the scope of this chapter. See the Deep Learning specialization on Coursera (\\u200bhttp://deeplearning.ai\\u200b) if you would \\nlike to learn more about such algorithms.   \\nPage 101 Machine Learning Yearning-Draft Andrew Ng  \\nAs one final example, let’s revisit the autonomous driving pipeline.   \\n \\n \\n \\n \\n \\n \\n \\nBy using this pipeline, you are telling the algorithm that there are 3 key steps to driving: (1)  \\nDetect other cars, (2) Detect pedestrians, and (3) Plan a path for your car. Further, each of  \\nthese is a relatively simpler function--and can thus be learned with less data--than the  \\npurely end-to-end approach.  \\n \\nIn summary, when deciding what should be the components of a pipeline, try to build a  \\npipeline where each component is a relatively “simple” function that can therefore be learned  \\nfrom only a modest amount of data.   \\n \\n \\n \\n \\n  \\nPage 102 Machine Learning Yearning-Draft Andrew Ng \\n \\n52 Directly learning rich outputs  \\n \\nAn image classification algorithm will input an image \\u200bx\\u200b, and output an integer indicating the  \\nobject category. Can an algorithm instead output an entire sentence describing the image?   \\nFor example:  \\n \\n \\nx\\u200b =   \\n \\ny\\u200b = “A yellow bus driving down a road with  \\ngreen trees and green grass in the  \\nbackground.” \\nTraditional applications of supervised learning learned a function \\u200bh\\u200b:\\u200bX\\u200b→\\u200bY\\u200b, where the output  \\ny\\u200b was usually an integer or a real number. For example:   \\nProblem \\xa0 X\\xa0 Y\\xa0\\nSpam classification \\xa0 Email\\xa0\\xa0 Spam/Not spam (0/1) \\xa0\\nImage recognition \\xa0 Image\\xa0 Integer label \\xa0\\nHousing price prediction \\xa0Features of house\\xa0 Price in dollars \\xa0\\nProduct recommendation \\xa0Product & user features\\xa0 Chance of purchase \\xa0\\n \\nOne of the most exciting developments in end-to-end deep learning is that it is letting us  \\ndirectly learn \\u200by\\u200b that are much more complex than a number. In the image-captioning  \\nexample above, you can have a neural network input an image ( \\u200bx\\u200b) and directly output a  \\ncaption ( \\u200by\\u200b).  \\n \\n \\n \\n \\n \\nPage 103 Machine Learning Yearning-Draft Andrew Ng  \\nHere are more examples:  \\nProblem \\xa0 X\\xa0 Y\\xa0 Example Citation \\xa0\\nImage captioning \\xa0 Image\\xa0 Text\\xa0 Mao et al., 2014\\xa0\\nMachine translation \\xa0 English text \\xa0 French text\\xa0 Suskever et al., 2014\\xa0\\nQuestion answering \\xa0 (Text,Question) pair \\xa0\\xa0 Answer text \\xa0 Bordes et al., 2015 \\xa0\\nSpeech recognition \\xa0 Audio\\xa0 Transcription \\xa0 Hannun et al., 2015 \\xa0\\nTTS\\xa0 Text features \\xa0 Audio\\xa0 van der Oord et al., 2016\\xa0\\n \\nThis is an accelerating trend in deep learning: When you have the right (input,output)  \\nlabeled pairs, you can sometimes learn end-to-end even when the output is a sentence, an  \\nimage, audio, or other outputs that are richer than a single number.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 104 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n\\xa0\\nError analysis \\nby parts \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 105 Machine Learning Yearning-Draft Andrew Ng  \\n53 Error analysis by parts   \\n \\nSuppose your system is built using a complex machine learning pipeline, and you would like  \\nto improve the system’s performance. Which part of the pipeline should you work on  \\nimproving? By attributing errors to specific parts of the pipeline, you can decide how to  \\nprioritize your work.   \\n \\nLet’s use our Siamese cat classifier example:   \\n \\n \\nThe first part, the cat detector, detects cats and crops them out of the image. The second  \\npart, the cat breed classifier, decides if it is a Siamese cat. It is possible to spend years  \\nworking on improving either of these two pipeline components. How do you decide which  \\ncomponent(s) to focus on?   \\nBy carrying out \\u200berror analysis by parts \\u200b, you can try to attribute each mistake the  \\nalgorithm makes to one (or sometimes both) of the two parts of the pipeline. For example,  \\nthe algorithm misclassifies this image as not containing a Siamese cat (y=0) even though the  \\ncorrect label is y=1.   \\n \\nLet’s manually examine what the two steps of the algorithm did. Suppose the Siamese cat  \\ndetector had detected a cat as follows:   \\nPage 106 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThis means that the cat breed classifier is given the following image:   \\n \\n The cat breed classifier then correctly classifies this image as not containing a Siamese cat.  \\nThus, the cat breed classifier is blameless: It was given of a pile of rocks and outputted a very  \\nreasonable label y=0. Indeed, a human classifying the cropped image above would also have  \\npredicted y=0. Thus, you can clearly attribute this error to the cat detector.   \\nIf, on the other hand, the cat detector had outputted the following bounding box:  \\n \\nthen you would conclude that the cat detector had done its job, and that it was the cat breed  \\nclassifier that is at fault.   \\n \\nSay you go through 100 misclassified dev set images and find that 90 of the errors are  \\nattributable to the cat detector, and only 10 errors are attributable to the cat breed classifier.  \\nYou can safely conclude that you should focus more attention on improving the cat detector.   \\nPage 107 Machine Learning Yearning-Draft Andrew Ng  \\n \\nFurther, you have now also conveniently found 90 examples where the cat detector  \\noutputted incorrect bounding boxes. You can use these 90 examples to carry out a deeper  \\nlevel of error analysis on the cat detector to see how to improve that.   \\n \\nOur description of how you attribute error to one part of the pipeline has been informal so  \\nfar: you look at the output of each of the parts and see if you can decide which one made a  \\nmistake. This informal method could be all you need. But in the next chapter, you’ll also see  \\na more formal way of attributing error.   \\n \\n \\n  \\nPage 108 Machine Learning Yearning-Draft Andrew Ng  \\n54 Attributing error to one part   \\n \\nLet’s continue to use this example:  \\n  \\nSuppose the cat detector outputted this bounding box:   \\n \\n \\n \\nThe cat breed classifier is thus given this cropped image, whereupon it incorrectly outputs  \\ny=0, or that there is no cat in the picture.   \\n \\nThe cat detector did its job poorly. However, a highly skilled human could arguably still  \\nrecognize the Siamese cat from the poorly cropped image. So do we attribute this error to the  \\ncat detector, or the cat breed classifier, or both? It is ambiguous.  \\n \\nIf the number of ambiguous cases like these is small, you can make whatever decision you  \\nwant and get a similar result. But here is a more formal test that lets you more definitively  \\nattribute the error to exactly one part:  \\n \\n1.Replace the cat detector output with a hand-labeled bounding box.  \\nPage 109 Machine Learning Yearning-Draft Andrew Ng  \\n \\n2.Run the corresponding cropped image through the cat breed classifier. If the cat breed  \\nclassifier still misclassifies it, attribute the error to the cat breed classifier. Otherwise,  \\nattribute the error to the cat detector.   \\n \\nIn other words, run an experiment in which you give the cat breed classifier a “perfect” input.  \\nThere are two cases:  \\n \\n●Case 1: Even given a “perfect” bounding box, the cat breed classifier still incorrectly  \\noutputs y=0. In this case, clearly the cat breed classifier is at fault.  \\n●Case 2: Given a “perfect” bounding box, the breed classifier now correctly outputs  \\ny=1. This shows that if only the cat detector had given a more perfect bounding box,  \\nthen the overall system’s output would have been correct. Thus, attribute the error to  \\nthe cat detector.   \\n \\nBy carrying out this analysis on the misclassified dev set images, you can now  \\nunambiguously attribute each error to one component. This allows you to estimate the  \\nfraction of errors due to each component of the pipeline, and therefore decide where to focus  \\nyour attention.  \\n \\n \\n \\n \\n  \\nPage 110 Machine Learning Yearning-Draft Andrew Ng  \\n55 General case of error attribution   \\n \\nHere are the general steps for error attribution. Suppose the pipeline has three steps A, B  \\nand C, where A feeds directly into B, and B feeds directly into C.   \\n \\n \\n \\nFor each mistake the system makes on the dev set:  \\n \\n1.Try manually modifying A’s output to be a “perfect” output (e.g.,  the “perfect”  \\nbounding box for the cat), and run the rest of the pipeline B, C on this output. If the  \\nalgorithm now gives a correct output, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been correct; thus, you can  \\nattribute this error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B’s output to be the “perfect” output for B. If the algorithm  \\nnow gives a correct output, then attribute the error to component B. Otherwise, go on  \\nto Step 3. \\n3.Attribute the error to component C.   \\n \\nLet’s look at a more complex example:  \\nYour self-driving car uses this pipeline. How do you use error analysis by parts to decide  \\nwhich component(s) to focus on?   \\n \\nYou can map the three components to A, B, C as follows:  \\nA: Detect cars  \\nB: Detect pedestrians  \\nC: Plan path for car  \\n \\n \\nPage 111 Machine Learning Yearning-Draft Andrew Ng \\n \\nFollowing the procedure described above, suppose you test out your car on a closed track  \\nand find a case where the car chooses a more jarring steering direction than a skilled driver  \\nwould. In the self-driving world, such a case is usually called a \\u200bscenario \\u200b. You would then:  \\n  \\n1.Try manually modifying A (detecting cars)’s output to be a “perfect” output (e.g.,  \\nmanually go in and tell it where the other cars are). Run the rest of the pipeline B, C as  \\nbefore, but allow C (plan path) to use A’s now perfect output. If the algorithm now  \\nplans a much better path for the car, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been better; Thus, you can attribute  \\nthis error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B (detect pedestrian)’s output to be the “perfect” output for  \\nB. If the algorithm now gives a correct output, then attribute the error to component  \\nB. Otherwise, go on to Step 3.   \\n3.Attribute the error to component C.   \\n \\nThe components of an ML pipeline should be ordered according to a Directed Acyclic Graph  \\n(DAG), meaning that you should be able to compute them in some fixed left-to-right order,  \\nand later components should depend only on earlier components’ outputs. So long as the  \\nmapping of the components to the A->B->C order follows the DAG ordering, then the error  \\nanalysis will be fine. You might get slightly different results if you swap A and B:  \\n \\nA: Detect pedestrians (was previously \\u200bDetect cars \\u200b)  \\nB: Detect cars (was previously \\u200bDetect pedestrians \\u200b) \\nC: Plan path for car  \\n \\nBut the results of this analysis would still be valid and give good guidance for where to focus  \\nyour attention.  \\n \\n \\n  \\nPage 112 Machine Learning Yearning-Draft Andrew Ng  \\n56 Error analysis by parts and comparison to  \\nhuman-level performance   \\n \\nCarrying out error analysis on a learning algorithm is like using data science to analyze an  \\nML system’s mistakes in order to derive insights about what to do next. At its most basic,  \\nerror analysis by parts tells us what component(s) performance is (are) worth the greatest  \\neffort to improve.  \\n \\nSay you have a dataset about customers buying things on a website. A data scientist may  \\nhave many different ways of analyzing the data. She may draw many different conclusions  \\nabout whether the website should raise prices, about the lifetime value of customers acquired  \\nthrough different marketing campaigns, and so on. There is no one “right” way to analyze a  \\ndataset, and there are many possible useful insights one could draw. Similarly, there is no  \\none “right” way to carry out error analysis. Through these chapters you have learned many of  \\nthe most common design patterns for drawing useful insights about your ML system, but you  \\nshould feel free to experiment with other ways of analyzing errors as well.  \\n \\nLet’s return to the self-driving application, where a car detection algorithm outputs the  \\nlocation (and perhaps velocity) of the nearby cars, a pedestrian detection algorithm outputs  \\nthe location of the nearby pedestrians, and these two outputs are finally used to plan a path  \\nfor the car.  \\n \\nTo debug this pipeline, rather than rigorously following the procedure you saw in the  \\nprevious chapter, you could more informally ask:  \\n \\n1.How far is the Detect cars component from human-level performance at detecting  \\ncars?  \\n2.How far is the Detect pedestrians component from human-level performance?  \\nPage 113 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.How far is the overall system’s performance from human-level performance? Here,  \\nhuman-level performance assumes the human has to plan a path for the car given  \\nonly the outputs from the previous two pipeline components (rather than access to  \\nthe camera images). In other words, how does the Plan path component’s  \\nperformance compare to that of a human’s, when the human is given only the same  \\ninput?   \\n \\nIf you find that one of the components is far from human-level performance, you now have a  \\ngood case to focus on improving the performance of that component.   \\n \\nMany error analysis processes work best when we are trying to automate something humans  \\ncan do and can thus benchmark against human-level performance. Most of our preceding  \\nexamples had this implicit assumption. If you are building an ML system where the final  \\noutput or some of the intermediate components are doing things that even humans cannot  \\ndo well, then some of these procedures will not apply.  \\n \\nThis is another advantage of working on problems that humans can solve--you have more  \\npowerful error analysis tools, and thus you can prioritize your team’s work more efficiently.   \\n \\n \\n \\n  \\nPage 114 Machine Learning Yearning-Draft Andrew Ng  \\n57 Spotting a flawed ML pipeline   \\n \\nWhat if each individual component of your ML pipeline is performing at human-level  \\nperformance or near-human-level performance, but the overall pipeline falls far short of  \\nhuman-level? This usually means that the pipeline is flawed and needs to be redesigned.  \\nError analysis can also help you understand if you need to redesign your pipeline.   \\n \\n \\nIn the previous chapter, we posed the question of whether each of the three components’  \\nperformance is at human level. Suppose the answer to all three questions is yes. That is:  \\n \\n1.The Detect cars component is at (roughly) human-level performance for detecting  \\ncars from the camera images.  \\n2.The Detect pedestrians component is at (roughly) human-level performance for  \\ndetecting cars from the camera images.   \\n3.Compared to a human that has to plan a path for the car given only the outputs  \\nfrom the previous two pipeline components (rather than access to the camera  \\nimages), \\u200b the Plan path component’s performance is at a similar level.   \\n \\nHowever, your overall self-driving car is performing significantly below human-level  \\nperformance. I.e., humans given access to the camera images can plan significantly better  \\npaths for the car. What conclusion can you draw?   \\n \\nThe only possible conclusion is that the ML pipeline is flawed. In this case, the Plan path  \\ncomponent is doing as well as it can \\u200bgiven its inputs \\u200b, but the inputs do not contain enough  \\ninformation. You should ask yourself what other information, other than the outputs from  \\nthe two earlier pipeline components, is needed to plan paths very well for a car to drive. In  \\nother words, what other information does a skilled human driver need?   \\n \\nPage 115 Machine Learning Yearning-Draft Andrew Ng  \\nFor example, suppose you realize that a human driver also needs to know the location of the  \\nlane markings. This suggests that you should redesign the pipeline as follows : 18\\n \\n \\n \\n \\nUltimately, if you don’t think your pipeline as a whole will achieve human-level performance,  \\neven if every individual component has human-level performance (remember that you are  \\ncomparing to a human who is given the same input as the component), then the pipeline is  \\nflawed and should be redesigned.  \\n  \\n18 In the self-driv ing example abov e, in theory one could solv e this problem by also feeding the raw camera  \\nimage into the planning component.  Howev er, this would v iolate the design principle of “Task  simplicity”  \\ndescribed in Chapter 51,  because the path planning module now needs to input a raw image and has a v ery \\ncomplex task  to solv e. That’ s why adding a Detect lane markings component  is a better choice--it helps get the \\nimportant and prev iously missing information about lane markings to the path planning module,  but you av oid \\nmak ing any particular module ov erly complex to build/train.   \\n \\nPage 116 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xa0\\nConclusion \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 117 Machine Learning Yearning-Draft Andrew Ng  \\n58 Building a superhero team - Get your  \\nteammates to read this  \\n \\nCongratulations on finishing this book!   \\nIn Chapter 2, we talked about how this book can help you become the superhero of your  \\nteam.  \\n \\nThe only thing better than being a superhero is being part of a superhero team. I hope you’ll  \\ngive copies of this book to your friends and teammates and help create other superheroes!  \\n \\n  \\nPage 118 Machine Learning Yearning-Draft Andrew Ng ')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' \\n \\n \\n \\n \\n \\n \\nMachine L earning Year ning is a   \\ndeeplear ning. ai pr oject. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n© 2018 Andrew Ng.  All Rights R eserv ed. \\n \\n  \\nPage 2 Machine Learning Yearning-Draft Andrew Ng Deeplearning.AI \\nTable of Contents \\n \\n1 Why Machine Learning Strategy  \\n2 How to use this book to help your team  \\n3 Prerequisites and Notation  \\n4 Scale drives machine learning progress  \\n5 Your development and test sets  \\n6 Your dev and test sets should come from the same distribution  \\n7 How large do the dev/test sets need to be?  \\n8 Establish a single-number evaluation metric for your team to optimize  \\n9 Optimizing and satisficing metrics  \\n10 Having a dev set and metric speeds up iterations  \\n11 When to change dev/test sets and metrics  \\n12 Takeaways: Setting up development and test sets  \\n13 Build your first system quickly, then iterate  \\n14 Error analysis: Look at dev set examples to evaluate ideas  \\n15 Evaluating multiple ideas in parallel during error analysis  \\n16 Cleaning up mislabeled dev and test set examples  \\n17 If you have a large dev set, split it into two subsets, only one of which you look at  \\n18 How big should the Eyeball and Blackbox dev sets be?  \\n19 Takeaways: Basic error analysis  \\n20 Bias and Variance: The two big sources of error  \\n21 Examples of Bias and Variance  \\n22 Comparing to the optimal error rate  \\n23 Addressing Bias and Variance  \\n24 Bias vs. Variance tradeoff  \\n25 Techniques for reducing avoidable bias  \\nPage 3 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n27 Techniques for reducing variance  \\n28 Diagnosing bias and variance: Learning curves  \\n29 Plotting training error  \\n30 Interpreting learning curves: High bias  \\n31 Interpreting learning curves: Other cases  \\n32 Plotting learning curves  \\n33 Why we compare to human-level performance  \\n34 How to define human-level performance  \\n35 Surpassing human-level performance  \\n36 When you should train and test on different distributions  \\n37 How to decide whether to use all your data  \\n38 How to decide whether to include inconsistent data  \\n39 Weighting data  \\n40 Generalizing from the training set to the dev set  \\n41 Identifying Bias, Variance, and Data Mismatch Errors  \\n42 Addressing data mismatch  \\n43 Artificial data synthesis  \\n44 The Optimization Verification test  \\n45 General form of Optimization Verification test  \\n46 Reinforcement learning example  \\n47 The rise of end-to-end learning  \\n48 More end-to-end learning examples  \\n49 Pros and cons of end-to-end learning  \\n50 Choosing pipeline components: Data availability  \\n51 Choosing pipeline components: Task simplicity  \\nPage 4 Machine Learning Yearning-Draft Andrew Ng  \\n52 Directly learning rich outputs  \\n53 Error analysis by parts  \\n54 Attributing error to one part  \\n55 General case of error attribution  \\n56 Error analysis by parts and comparison to human-level performance  \\n57 Spotting a flawed ML pipeline  \\n58 Building a superhero team - Get your teammates to read this  \\n \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 5 Machine Learning Yearning-Draft Andrew Ng  \\n1 Why Machine Learning Strategy  \\n \\nMachine learning is the foundation of countless important applications, including web  \\nsearch, email anti-spam, speech recognition, product recommendations, and more. I assume  \\nthat you or your team is working on a machine learning application, and that you want to  \\nmake rapid progress. This book will help you do so.   \\nExample:  Building a cat picture startup  \\nSay you’re building a startup that will provide an endless stream of cat pictures to cat lovers.  \\nYou use a neural network to build a computer vision system for detecting cats in pictures.  \\nBut tragically, your learning algorithm’s accuracy is not yet good enough. You are under  \\ntremendous pressure to improve your cat detector. What do you do?   \\nYour team has a lot of ideas, such as:  \\n•Get more data: Collect more pictures of cats.   \\n•Collect a more diverse training set. For example, pictures of cats in unusual positions; cats  \\nwith unusual coloration; pictures shot with a variety of camera settings; ….   \\n•Train the algorithm longer, by running more gradient descent iterations.  \\n•Try a bigger neural network, with more layers/hidden units/parameters.   \\nPage 6 Machine Learning Yearning-Draft Andrew Ng \\n \\n•Try a smaller neural network. \\n•Try adding regularization (such as L2 regularization).  \\n•Change the neural network architecture (activation function, number of hidden units, etc.)  \\n•…  \\nIf you choose well among these possible directions, you’ll build the leading cat picture  \\nplatform, and lead your company to success. If you choose poorly, you might waste months.  \\nHow do you proceed?   \\nThis book will tell you'),\n",
       " Document(page_content=' 6 Machine Learning Yearning-Draft Andrew Ng \\n \\n•Try a smaller neural network. \\n•Try adding regularization (such as L2 regularization).  \\n•Change the neural network architecture (activation function, number of hidden units, etc.)  \\n•…  \\nIf you choose well among these possible directions, you’ll build the leading cat picture  \\nplatform, and lead your company to success. If you choose poorly, you might waste months.  \\nHow do you proceed?   \\nThis book will tell you how. Most machine learning problems leave clues that tell you what’s  \\nuseful to try, and what’s not useful to try. Learning to read those clues will save you months  \\nor years of development time.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 7 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n2 How to use this book to help your team  \\n \\nAfter finishing this book, you will have a deep understanding of how to set technical  \\ndirection for a machine learning project.   \\nBut your teammates might not understand why you’re recommending a particular direction.  \\nPerhaps you want your team to define a single-number evaluation metric, but they aren’t  \\nconvinced. How do you persuade them?   \\nThat’s why I made the chapters short: So that you can print them out and get your  \\nteammates to read just the 1-2 pages you need them to know.   \\nA few changes in prioritization can have a huge effect on your team’s productivity. By helping  \\nyour team with a few such changes, I hope that you can become the superhero of your team!   \\n \\n \\n \\xa0\\nPage 8 Machine Learning Yearning-Draft Andrew Ng \\n \\n3 Prerequisites and Notation  \\n \\nIf you have taken a Machine Learning course such as my machine learning MOOC on  \\nCoursera, or if you have experience applying supervised learning, you will be able to  \\nunderstand this text.   \\nI assume you are familiar with \\u200bsupervised learning \\u200b: learning a function that maps from x  \\nto y, using labeled training examples (x,y). Supervised learning algorithms include linear  \\nregression, logistic regression, and neural networks. There are many forms of machine  \\nlearning, but the majority of Machine Learning’s practical value today comes from  \\nsupervised learning.   \\nI will frequently refer to neural networks (also known as “deep learning”). You’ll only need a  \\nbasic understanding of what they are to follow this text.   \\nIf you are not familiar with the concepts mentioned here, watch the first three weeks of  \\nvideos in the Machine Learning course on Coursera at \\u200bhttp://ml-class.org  \\n  \\nPage 9 Machine Learning Yearning-Draft Andrew Ng  \\n4 Scale drives machine learning progress  \\n \\nMany of the ideas of deep learning (neural networks) have been around for decades. Why are  \\nthese ideas taking off now?   \\nTwo of the biggest drivers of recent progress have been:  \\n•Data availability. \\u200b People are now spending more time on digital devices (laptops, mobile  \\ndevices). Their digital activities generate huge amounts of data that we can feed to our  \\nlearning algorithms.  \\n•Computational scale. \\u200bWe started just a few years ago to be able to train neural  \\nnetworks that are big enough to take advantage of the huge datasets we now have.   \\nIn detail, even as you accumulate more data, usually the performance of older learning  \\nalgorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens  \\nout,” and the algorithm stops improving even as you give it more data:   \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIt was as if the older algorithms didn’t know what to do with all the data we now have.   \\nIf you train a small neutral network (NN) on the same supervised learning task, you might  \\nget slightly better performance:   \\n \\n \\n \\nPage 10 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nHere, by “Small NN” we mean a neural network with only a small number of hidden  \\nunits/layers/parameters. Finally, if you train larger and larger neural networks, you can  \\nobtain even better performance:  1\\n \\nThus, you obtain the best performance when you (i) Train a very large neural network, so  \\nthat you are on the green curve above; (ii) Have a huge amount of data.   \\nMany other details such as neural network architecture are also important, and there has  \\nbeen much innovation here. But one of the more reliable ways to improve an algorithm’s  \\nperformance today is still to (i) train a bigger network and (ii) get more data.   \\n1 This diagram shows NNs doing better in the r egime of small datasets.  This effect is less consistent \\nthan the effect of NNs doing well in the r egime of huge datasets.  In the small data regime,  depending  \\non how the features ar e'),\n",
       " Document(page_content=' and there has  \\nbeen much innovation here. But one of the more reliable ways to improve an algorithm’s  \\nperformance today is still to (i) train a bigger network and (ii) get more data.   \\n1 This diagram shows NNs doing better in the r egime of small datasets.  This effect is less consistent \\nthan the effect of NNs doing well in the r egime of huge datasets.  In the small data regime,  depending  \\non how the features ar e hand-engineer ed, traditional algor ithms may or  may not do better.  For \\nexample,  if you hav e 20 training examples,  it might not matter  much whether you use logist ic \\nregr ession or  a neur al network ; the hand-engineering of featur es will hav e a bigger effect than the  \\nchoice of algorithm.  But if you hav e 1 million examples,  I would fav or the neur al network .  \\nPage 11 Machine Learning Yearning-Draft Andrew Ng  \\nThe process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss  \\nthe details at length. We will start with general strategies that are useful for both traditional  \\nlearning algorithms and neural networks, and build up to the most modern strategies for  \\nbuilding deep learning systems.   \\nPage 12 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSetting up \\ndevelopment and \\ntest sets  \\nPage 13 Machine Learning Yearning-Draft Andrew Ng  \\n5 Your development and test sets  \\n \\nLet’s return to our earlier cat pictures example: You run a mobile app, and users are  \\nuploading pictures of many different things to your app. You want to automatically find the  \\ncat pictures.   \\nYour team gets a large training set by downloading pictures of cats (positive examples) and  \\nnon-cats (negative examples) off of different websites. They split the dataset 70%/30% into  \\ntraining and test sets. Using this data, they build a cat detector that works well on the  \\ntraining and test sets.   \\nBut when you deploy this classifier into the mobile app, you find that the performance is  \\nreally poor!   \\n         \\nWhat happened?  \\nYou figure out that the pictures users are uploading have a different look than the website  \\nimages that make up your training set: Users are uploading pictures taken with mobile  \\nphones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test  \\nsets were made of website images, your algorithm did not generalize well to the actual  \\ndistribution you care about: mobile phone pictures.   \\nBefore the modern era of big data, it was a common rule in machine learning to use a  \\nrandom 70%/30% split to form your training and test sets. This practice can work, but it’s a  \\nbad idea in more and more applications where the training distribution (website images in  \\nPage 14 Machine Learning Yearning-Draft Andrew Ng  \\nour example above) is different from the distribution you ultimately care about (mobile  \\nphone images).  \\nWe usually define:  \\n•Training set \\u200b — Which you run your learning algorithm on.  \\n•Dev (development) set \\u200b — Which you use to tune parameters, select features, and  \\nmake other decisions regarding the learning algorithm. Sometimes also called the  \\nhold-out cross validation set \\u200b.  \\n•Test set\\u200b — which you use to evaluate the performance of the algorithm, but not to make  \\nany decisions regarding what learning algorithm or parameters to use.   \\nOnce you define a dev set (development set) and test set, your team will try a lot of ideas,  \\nsuch as different learning algorithm parameters, to see what works best. The dev and test  \\nsets allow your team to quickly see how well your algorithm is doing.   \\nIn other words, \\u200bthe purpose of the dev and test sets are to direct your team toward  \\nthe most important changes to make to the machine learning system \\u200b.  \\nSo, you should do the following:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nIn other words, your test set should not simply be 30% of the available data, especially if you  \\nexpect your future data (mobile phone images) to be different in nature from your training  \\nset (website images).   \\nIf you have not yet launched your mobile app, you might not have any users yet, and thus  \\nmight not be able to get data that accurately reflects what you have to do well on in the  \\nfuture. But you might still try to approximate this. For example, ask your friends to take  \\nmobile phone pictures of cats and send them to you. Once your app is launched, you can  \\nupdate your dev/test sets using actual user data.   \\nIf you really don’t have any way'),\n",
       " Document(page_content='If you have not yet launched your mobile app, you might not have any users yet, and thus  \\nmight not be able to get data that accurately reflects what you have to do well on in the  \\nfuture. But you might still try to approximate this. For example, ask your friends to take  \\nmobile phone pictures of cats and send them to you. Once your app is launched, you can  \\nupdate your dev/test sets using actual user data.   \\nIf you really don’t have any way of getting data that approximates what you expect to get in  \\nthe future, perhaps you can start by using website images. But you should be aware of the  \\nrisk of this leading to a system that doesn’t generalize well.   \\nIt requires judgment to decide how much to invest in developing great dev and test sets. But  \\ndon’t assume your training distribution is the same as your test distribution. Try to pick test  \\nPage 15 Machine Learning Yearning-Draft Andrew Ng  \\nexamples that reflect what you ultimately want to perform well on, rather than whatever data  \\nyou happen to have for training.   \\n \\n  \\nPage 16 Machine Learning Yearning-Draft Andrew Ng  \\n6 Your dev and test sets should come from the  \\nsame distribution  \\nYou have your cat app image data segmented into four regions, based on your largest  \\nmarkets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a dev set and a test  \\nset, say we put US and India in the dev set; China and Other in the test set. In other words,  \\nwe can randomly assign two of these segments to the dev set, and the other two to the test  \\nset, right?  \\nOnce you define the dev and test sets, your team will be focused on improving dev set  \\nperformance. Thus, the dev set should reflect the task you want to improve on the most: To  \\ndo well on all four geographies, and not only two.   \\nThere is a second problem with having different dev and test set distributions: There is a  \\nchance that your team will build something that works well on the dev set, only to find that it  \\ndoes poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid  \\nletting this happen to you.   \\nAs an example, suppose your team develops a system that works well on the dev set but not  \\nthe test set. If your dev and test sets had come from the same distribution, then you would  \\nhave a very clear diagnosis of what went wrong: You have overfit the dev set. The obvious  \\ncure is to get more dev set data.   \\nBut if the dev and test sets come from different distributions, then your options are less  \\nclear. Several things could have gone wrong:   \\n1.You had overfit to the dev set.   \\n2.The test set is harder than the dev set. So your algorithm might be doing as well as could  \\nbe expected, and no further significant improvement is possible.   \\nPage 17 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.The test set is not necessarily harder, but just different, from the dev set. So what works  \\nwell on the dev set just does not work well on the test set. In this case, a lot of your work  \\nto improve dev set performance might be wasted effort.   \\nWorking on machine learning applications is hard enough. Having mismatched dev and test  \\nsets introduces additional uncertainty about whether improving on the dev set distribution  \\nalso improves test set performance. Having mismatched dev and test sets makes it harder to  \\nfigure out what is and isn’t working, and thus makes it harder to prioritize what to work on.   \\nIf you are working on a 3rd party benchmark problem, their creator might have specified dev  \\nand test sets that come from different distributions. Luck, rather than skill, will have a  \\ngreater impact on your performance on such benchmarks compared to if the dev and test  \\nsets come from the same distribution. It is an important research problem to develop  \\nlearning algorithms that are trained on one distribution and generalize well to another. But if  \\nyour goal is to make progress on a specific machine learning application rather than make  \\nresearch progress, I  recommend trying to choose dev and test sets that are drawn from the  \\nsame distribution. This will make your team more efficient.   \\n \\n \\n \\n \\xa0\\nPage 18 Machine Learning Yearning-Draft Andrew Ng  \\n7 How large do the dev/test sets need to be?  \\n \\nThe dev set should be large enough to detect differences between algorithms that you are  \\ntrying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an  \\naccuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1%  \\ndifference. Compared to other machine learning'),\n",
       " Document(page_content=' 18 Machine Learning Yearning-Draft Andrew Ng  \\n7 How large do the dev/test sets need to be?  \\n \\nThe dev set should be large enough to detect differences between algorithms that you are  \\ntrying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an  \\naccuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1%  \\ndifference. Compared to other machine learning problems I’ve seen, a 100 example dev set is  \\nsmall. Dev sets with sizes from 1,000 to 10,000 examples are common. With 10,000  \\nexamples, you will have a good chance of detecting an improvement of 0.1%.  2\\nFor mature and important applications—for example, advertising, web search, and product  \\nrecommendations—I have also seen teams that are highly motivated to eke out even a 0.01%  \\nimprovement, since it has a direct impact on the company’s profits. In this case, the dev set  \\ncould be much larger than 10,000, in order to detect even smaller improvements.   \\nHow about the size of the test set? It should be large enough to give high confidence in the  \\noverall performance of your system. One popular heuristic had been to use 30% of your data  \\nfor your test set. This works well when you have a modest number of examples—say 100 to  \\n10,000 examples. But in the era of big data where we now have machine learning problems  \\nwith sometimes more than a billion examples, the fraction of data allocated to dev/test sets  \\nhas been shrinking, even as the absolute number of examples in the dev/test sets has been  \\ngrowing. There is no need to have excessively large dev/test sets beyond what is needed to  \\nevaluate the performance of your algorithms.   \\n2 In theory,  one could also test if a change to an algorithm mak es a statistically significant difference \\non the dev  set. In pr actice,  most teams don’t bother  with this (unless they are publishing academic  \\nresearch papers),  and I usually do not find statistical significance tests useful for measuring inter im \\nprogr ess.  \\nPage 19 Machine Learning Yearning-Draft Andrew Ng  \\n8 Establish a single-number evaluation metric  \\nfor your team to optimize   \\n \\nClassification accuracy is an example of a \\u200bsingle-number evaluation metric \\u200b: You run \\nyour classifier on the dev set (or test set), and get back a single number about what fraction  \\nof examples it classified correctly. According to this metric, if classifier A obtains 97%  \\naccuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.  \\nIn contrast, Precision and Recall  is not a single-number evaluation metric: It gives two  3\\nnumbers for assessing your classifier. Having multiple-number evaluation metrics makes it  \\nharder to compare algorithms. Suppose your algorithms perform as follows:   \\n \\nClassifier \\xa0 Precision \\xa0 Recall\\xa0\\nA\\xa0 95% 90% \\nB\\xa0 98% 85% \\n \\nHere, neither classifier is obviously superior, so it doesn’t immediately guide you toward  \\npicking one. \\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\n \\nDuring development, your team will try a lot of ideas about algorithm architecture, model  \\nparameters, choice of features, etc. Having a \\u200bsingle-number evaluation metric \\u200b such as  \\naccuracy allows you to sort all your models according to their performance on this metric,  \\nand quickly decide what is working best.   \\nIf you really care about both Precision and Recall, I recommend using one of the standard  \\nways to combine them into a single number. For example, one could take the average of  \\nprecision and recall, to end up with a single number.  Alternatively, you can compute the “F1  \\n3 The Precision of a cat classifier  is the fraction of images in the dev  (or test) set it labeled as cats that \\nreally are cats.  Its Recall is the percentage of all cat images in the dev  (or test) set that it cor rectly  \\nlabeled as a cat.  There is often a tradeoff between hav ing high pr ecision and high recall.   \\nPage 20 Machine Learning Yearning-Draft Andrew Ng  \\nscore,” which is a modified way of computing their average, and works better than simply  \\ntaking the mean. 4\\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\nB\\xa0 98% 85% 91.0%  \\n \\nHaving a single-number evaluation metric speeds up your ability to make a decision when  \\nyou are selecting among a'),\n",
       " Document(page_content=' 20 Machine Learning Yearning-Draft Andrew Ng  \\nscore,” which is a modified way of computing their average, and works better than simply  \\ntaking the mean. 4\\n \\nClassifier \\xa0Precision \\xa0Recall\\xa0 F1 score \\xa0\\nA\\xa0 95% 90% 92.4%  \\nB\\xa0 98% 85% 91.0%  \\n \\nHaving a single-number evaluation metric speeds up your ability to make a decision when  \\nyou are selecting among a large number of classifiers. It gives a clear preference ranking  \\namong all of them, and therefore a clear direction for progress.   \\nAs a final example, suppose you are separately tracking the accuracy of your cat classifier in  \\nfour key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By  \\ntaking an average or weighted average of these four numbers, you end up with a single  \\nnumber metric. Taking an average or weighted average is one of the most common ways to  \\ncombine multiple metrics into one.   \\n \\n \\n \\xa0\\n4 If you want to learn more about the F1  scor e, see \\u200bhttps: //en. wikipedia. org/wik i/F1_score\\u200b.  It is the  \\n“harmonic mean” between Precision and R ecall,  and is calculated as 2 /((1/Pr ecision)+(1/R ecall)).   \\nPage 21 Machine Learning Yearning-Draft Andrew Ng  \\n9 Optimizing and satisficing metrics   \\n \\nHere’s another way to combine multiple evaluation metrics.   \\nSuppose you care about both the accuracy and the running time of a learning algorithm. You  \\nneed to choose from these three classifiers:   \\nClassifier \\xa0 Accuracy \\xa0 Running time\\xa0\\nA\\xa0 90%\\xa0 80ms\\xa0\\nB\\xa0 92% 95ms\\xa0\\nC\\xa0 95% 1,500ms \\xa0\\n \\nIt seems unnatural to derive a single metric by putting accuracy and running time into a  \\nsingle formula, such as:  \\nAccuracy - 0.5*RunningTime \\nHere’s what you can do instead: First, define what is an “acceptable” running time. Lets say  \\nanything that runs in 100ms is acceptable. Then, maximize accuracy, subject to your  \\nclassifier meeting the running time criteria. Here, running time is a “satisficing  \\nmetric”—your classifier just has to be “good enough” on this metric, in the sense that it  \\nshould take at most 100ms. Accuracy is the “optimizing metric.”  \\nIf you are trading off N different criteria, such as binary file size of the model (which is  \\nimportant for mobile apps, since users don’t want to download large apps), running time,  \\nand accuracy, you might consider setting N-1 of the criteria as “satisficing” metrics. I.e., you  \\nsimply require that they meet a certain value. Then define the final one as the “optimizing”  \\nmetric. For example, set a threshold for what is acceptable for binary file size and running  \\ntime, and try to optimize accuracy given those constraints.  \\nAs a final example, suppose you are building a hardware device that uses a microphone to  \\nlisten for the user saying a particular “wakeword,” that then causes the system to wake up.  \\nExamples include Amazon Echo listening for “Alexa”; Apple Siri listening for “Hey Siri”;  \\nAndroid listening for “Okay Google”; and Baidu apps listening for “Hello Baidu.” You care  \\nabout both the false positive rate—the frequency with which the system wakes up even when  \\nno one said the wakeword—as well as the false negative rate—how often it fails to wake up  \\nwhen someone says the wakeword. One reasonable goal for the performance of this system is  \\nPage 22 Machine Learning Yearning-Draft Andrew Ng  \\nto minimize the false negative rate (optimizing metric), subject to there being no more than  \\none false positive every 24 hours of operation (satisficing metric).   \\nOnce your team is aligned on the evaluation metric to optimize, they will be able to make  \\nfaster progress.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 23 Machine Learning Yearning-Draft Andrew Ng  \\n10 Having a dev set and metric speeds up  \\niterations \\n \\nIt is very difficult to know in advance what approach will work best for a new problem. Even  \\nexperienced machine learning researchers will usually try out many dozens of ideas before  \\nthey discover something satisfactory. When building a machine learning system, I will often:   \\n1.Start off with some \\u200bidea\\u200b on how to build the system.   \\n2.Implement the idea in \\u200bcode\\u200b.  \\n3.Carry out an \\u200bexperiment \\u200b which tells me how well the idea worked. (Usually my first few  \\nideas don’t work!) Based on these learnings,'),\n",
       " Document(page_content=' for a new problem. Even  \\nexperienced machine learning researchers will usually try out many dozens of ideas before  \\nthey discover something satisfactory. When building a machine learning system, I will often:   \\n1.Start off with some \\u200bidea\\u200b on how to build the system.   \\n2.Implement the idea in \\u200bcode\\u200b.  \\n3.Carry out an \\u200bexperiment \\u200b which tells me how well the idea worked. (Usually my first few  \\nideas don’t work!) Based on these learnings, go back to generate more ideas, and keep on  \\niterating.  \\nThis is an iterative process. The faster you can go round this loop, the faster you will make  \\nprogress. This is why having dev/test sets and a metric are important: Each time you try an  \\nidea, measuring your idea’s performance on the dev set lets you quickly decide if you’re  \\nheading in the right direction.   \\nIn contrast, suppose you don’t have a specific dev set and metric. So each time your team  \\ndevelops a new cat classifier, you have to incorporate it into your app, and play with the app  \\nfor a few hours to get a sense of whether the new classifier is an improvement. This would be  \\nincredibly slow! Also, if your team improves the classifier’s accuracy from 95.0% to 95.1%,  \\nyou might not be able to detect that 0.1% improvement from playing with the app. Yet a lot  \\nof progress in your system will be made by gradually accumulating dozens of these 0.1%  \\nimprovements. Having a dev set and metric allows you to very quickly detect which ideas are  \\nsuccessfully giving you small (or large) improvements, and therefore lets you quickly decide  \\nwhat ideas to keep refining, and which ones to discard.   \\n \\xa0\\nPage 24 Machine Learning Yearning-Draft Andrew Ng \\n \\n11 When to change dev/test sets and metrics   \\n \\nWhen starting out on a new project, I try to quickly choose dev/test sets, since this gives the  \\nteam a well-defined target to aim for.   \\nI typically ask my teams to come up with an initial dev/test set and an initial metric in less  \\nthan one week—rarely longer. It is better to come up with something imperfect and get going  \\nquickly, rather than overthink this. But this one week timeline does not apply to mature  \\napplications. For example, anti-spam is a mature deep learning application. I have seen  \\nteams working on already-mature systems spend months to acquire even better dev/test  \\nsets.  \\nIf you later realize that your initial dev/test set or metric missed the mark, by all means  \\nchange them quickly. For example, if your dev set + metric ranks classifier A above classifier  \\nB, but your team thinks that classifier B is actually superior for your product, then this might  \\nbe a sign that you need to change your dev/test sets or your evaluation metric.   \\nThere are three main possible causes of the dev set/metric incorrectly rating classifier A  \\nhigher:  \\n1. The actual distribution you need to do well on is different from the dev/test sets.  \\nSuppose your initial dev/test set had mainly pictures of adult cats. You ship your cat app,  \\nand find that users are uploading a lot more kitten images than expected. So, the dev/test set  \\ndistribution is not representative of the actual distribution you need to do well on. In this  \\ncase, update your dev/test sets to be more representative.   \\n \\nPage 25 Machine Learning Yearning-Draft Andrew Ng \\n \\n2. You have overfit to the dev set.  \\nThe process of repeatedly evaluating ideas on the dev set causes your algorithm to gradually  \\n“overfit” to the dev set. When you are done developing, you will evaluate your system on the  \\ntest set. If you find that your dev set performance is much better than your test set  \\nperformance, it is a sign that you have overfit to the dev set. In this case, get a fresh dev set.   \\nIf you need to track your team’s progress, you can also evaluate your system regularly—say  \\nonce per week or once per month—on the test set. But do not use the test set to make any  \\ndecisions regarding the algorithm, including whether to roll back to the previous week’s  \\nsystem. If you do so, you will start to overfit to the test set, and can no longer count on it to  \\ngive a completely unbiased estimate of your system’s performance (which you would need if  \\nyou’re publishing research papers, or perhaps using this metric to make important business  \\ndecisions).  \\n3. The metric is measuring something other than what the project needs to optimize.  \\nSuppose that for your cat application, your metric is classification accuracy. This metric  \\ncurrently ranks classifier A as superior to classifier B. But suppose you try'),\n",
       " Document(page_content='fit to the test set, and can no longer count on it to  \\ngive a completely unbiased estimate of your system’s performance (which you would need if  \\nyou’re publishing research papers, or perhaps using this metric to make important business  \\ndecisions).  \\n3. The metric is measuring something other than what the project needs to optimize.  \\nSuppose that for your cat application, your metric is classification accuracy. This metric  \\ncurrently ranks classifier A as superior to classifier B. But suppose you try out both  \\nalgorithms, and find classifier A is allowing occasional pornographic images to slip through.  \\nEven though classifier A is more accurate, the bad impression left by the occasional  \\npornographic image means its performance is unacceptable. What do you do?   \\nHere, the metric is failing to identify the fact that Algorithm B is in fact better than  \\nAlgorithm A for your product. So, you can no longer trust the metric to pick the best  \\nalgorithm. It is time to change evaluation metrics. For example, you can change the metric to  \\nheavily penalize letting through pornographic images.  I would strongly recommend picking  \\na new metric and using the new metric to explicitly define a new goal for the team, rather  \\nthan proceeding for too long without a trusted metric and reverting to manually choosing  \\namong classifiers.  \\n \\nIt is quite common to change dev/test sets or evaluation metrics during a project. Having an  \\ninitial dev/test set and metric helps you iterate quickly. If you ever find that the dev/test sets  \\nor metric are no longer pointing your team in the right direction, it’s not a big deal! Just  \\nchange them and make sure your team knows about the new direction.   \\n \\n \\xa0\\nPage 26 Machine Learning Yearning-Draft Andrew Ng  \\n12 Takeaways: Setting up development and  \\ntest sets  \\n \\n•Choose dev and test sets from a distribution that reflects what data you expect to get in  \\nthe future and want to do well on. This may not be the same as your training data’s  \\ndistribution.   \\n•Choose dev and test sets from the same distribution if possible.   \\n•Choose a single-number evaluation metric for your team to optimize. If there are multiple  \\ngoals that you care about, consider combining them into a single formula (such as  \\naveraging multiple error metrics) or defining satisficing and optimizing metrics.   \\n•Machine learning is a highly iterative process: You may try many dozens of ideas before  \\nfinding one that you’re satisfied with.   \\n•Having dev/test sets and a single-number evaluation metric helps you quickly evaluate  \\nalgorithms, and therefore iterate faster.   \\n•When starting out on a brand new application, try to establish dev/test sets and a metric  \\nquickly, say in less than a week. It might be okay to take longer on mature applications.   \\n•The old heuristic of a 70%/30% train/test split does not apply for problems where you  \\nhave a lot of data; the dev and test sets can be much less than 30% of the data.   \\n•Your dev set should be large enough to detect meaningful changes in the accuracy of your  \\nalgorithm, but not necessarily much larger. Your test set should be big enough to give you  \\na confident estimate of the final performance of your system.   \\n•If your dev set and metric are no longer pointing your team in the right direction, quickly  \\nchange them: (i) If you had overfit the dev set, get more dev set data. (ii) If the actual  \\ndistribution you care about is different from the dev/test set distribution, get new  \\ndev/test set data. (iii) If your metric is no longer measuring what is most important to  \\nyou, change the metric.   \\n \\n \\n \\nPage 27 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\nBasic Error \\nAnalysis \\n\\xa0\\n\\xa0 \\xa0\\nPage 28 Machine Learning Yearning-Draft Andrew Ng  \\n13 Build your first system quickly, then iterate  \\n \\nYou want to build a new email anti-spam system. Your team has several ideas:   \\n•Collect a huge training set of spam email. For example, set up a “honeypot”: deliberately  \\nsend fake email addresses to known spammers, so that you can automatically harvest the  \\nspam messages they send to those addresses.  \\n•Develop features for understanding the text content of the email.   \\n•Develop features for understanding the email envelope/header features to show what set  \\nof internet servers the message went through.   \\n•and more.  \\nEven though I have worked extensively on anti-spam, I would still have a hard time picking  \\none of these directions. It is even harder if you are not an expert in the application area.   \\nSo don’t start off trying to design and build the perfect system. Instead, build and train a  \\nbasic system quickly—perhaps in just a few days.  Even if the basic'),\n",
       " Document(page_content=' envelope/header features to show what set  \\nof internet servers the message went through.   \\n•and more.  \\nEven though I have worked extensively on anti-spam, I would still have a hard time picking  \\none of these directions. It is even harder if you are not an expert in the application area.   \\nSo don’t start off trying to design and build the perfect system. Instead, build and train a  \\nbasic system quickly—perhaps in just a few days.  Even if the basic system is far from the  5\\n“best” system you can build, it is valuable to examine how the basic system functions: you  \\nwill  quickly find clues that show you the most promising directions in which to invest your  \\ntime. These next few chapters will show you how to read these clues.  \\n \\n\\xa0\\n\\xa0 \\xa0\\n5 This adv ice is meant for readers wanting to build AI applications,  rather  than those whose goal is to  \\npublish academic papers.  I will later  return to the topic of doing research.   \\nPage 29 Machine Learning Yearning-Draft Andrew Ng  \\n14 Error analysis: Look at dev set examples to  \\nevaluate ideas  \\nWhen you play with your cat app, you notice several examples where it mistakes dogs for  \\ncats. Some dogs do look like cats!   \\nA team member proposes incorporating 3rd party software that will make the system do  \\nbetter on dog images. These changes will take a month, and the team member is  \\nenthusiastic. Should you ask them to go ahead?   \\nBefore investing a month on this task, I recommend that you first estimate how much it will  \\nactually improve the system’s accuracy. Then you can more rationally decide if this is worth  \\nthe month of development time, or if you’re better off using that time on other tasks.   \\nIn detail, here’s what you can do:   \\n1.Gather a sample of 100 dev set examples that your system \\u200bmisclassified \\u200b. I.e., examples  \\nthat your system made an error on.   \\n2.Look at these examples manually, and count what fraction of them are dog images.   \\nThe process of looking at misclassified examples is called \\u200berror analysis \\u200b. In this example, if \\nyou find that only 5% of the misclassified images are dogs, then no matter how much you  \\nimprove your algorithm’s performance on dog images, you won’t get rid of more than 5% of  \\nyour errors. In other words, 5% is a “ceiling” (meaning maximum possible amount) for how  \\nmuch the proposed project could help. Thus, if your overall system is currently 90% accurate  \\n(10% error), this improvement is likely to result in at best 90.5% accuracy (or 9.5% error,  \\nwhich is 5% less error than the original 10% error).   \\nPage 30 Machine Learning Yearning-Draft Andrew Ng \\n \\nIn contrast, if you find that 50% of the mistakes are dogs, then you can be more confident  \\nthat the proposed project will have a big impact. It could boost accuracy from 90% to 95% (a  \\n50% relative reduction in error, from 10% down to 5%).   \\nThis simple counting procedure of error analysis gives you a quick way to estimate the  \\npossible value of incorporating the 3rd party software for dog images. It provides a  \\nquantitative basis on which to decide whether to make this investment.   \\nError analysis can often help you figure out how promising different directions are. I’ve seen  \\nmany engineers reluctant to carry out error analysis. It often feels more exciting to just jump  \\nin and implement some idea, rather than question if the idea is worth the time investment.  \\nThis is a common mistake: It might result in your team spending a month only to realize  \\nafterward that it resulted in little benefit.   \\nManually examining 100 examples does not take long. Even if you take one minute per  \\nimage, you’d be done in under two hours. These two hours could save you a month of wasted  \\neffort.  \\nError Analysis \\u200b refers to the process of examining dev set examples that your algorithm  \\nmisclassified, so that you can understand the underlying causes of the errors. This can help  \\nyou prioritize projects—as in this example—and inspire new directions, which we will discuss  \\nnext. The next few chapters will also present best practices for carrying out error analyses.   \\n \\n \\n  \\nPage 31 Machine Learning Yearning-Draft Andrew Ng  \\n15 Evaluating multiple ideas in parallel during  \\nerror analysis \\n \\nYour team has several ideas for improving the cat detector:  \\n•Fix the problem of your algorithm recognizing \\u200bdogs\\u200b as cats. \\n•Fix the problem of your algorithm recognizing \\u200bgreat cats \\u200b (lions, panthers, etc.) as house  \\ncats (pets).   \\n•Improve'),\n",
       " Document(page_content=' next few chapters will also present best practices for carrying out error analyses.   \\n \\n \\n  \\nPage 31 Machine Learning Yearning-Draft Andrew Ng  \\n15 Evaluating multiple ideas in parallel during  \\nerror analysis \\n \\nYour team has several ideas for improving the cat detector:  \\n•Fix the problem of your algorithm recognizing \\u200bdogs\\u200b as cats. \\n•Fix the problem of your algorithm recognizing \\u200bgreat cats \\u200b (lions, panthers, etc.) as house  \\ncats (pets).   \\n•Improve the system’s performance on \\u200bblurry\\u200b images.  \\n•… \\nYou can efficiently evaluate all of these ideas in parallel. I usually create a spreadsheet and  \\nfill it out while looking through ~100 misclassified dev set images. I also jot down comments  \\nthat might help me remember specific examples. To illustrate this process, let’s look at a  \\nspreadsheet you might produce with a small dev set of four examples:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments \\xa0\\n1 ✔ \\xa0   Unusual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken at \\xa0\\nzoo on rainy day\\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n% of total \\xa0 25%\\xa0 50%\\xa0 50%\\xa0  \\n \\nImage #3 above has both the Great Cat and the Blurry columns checked. Furthermore,  \\nbecause it is possible for one example to be associated with multiple categories, the  \\npercentages at the bottom may not add up to 100%.   \\nAlthough you may first formulate the categories (Dog, Great cat, Blurry) then categorize the  \\nexamples by hand, in practice, once you start looking through examples, you will probably be  \\ninspired to propose new error categories. For example, say you go through a dozen images  \\nand realize a lot of mistakes occur with Instagram-filtered pictures. You can go back and add  \\na new “Instagram” column to the spreadsheet. Manually looking at examples that the  \\nalgorithm misclassified and asking how/whether you as a human could have labeled the  \\nPage 32 Machine Learning Yearning-Draft Andrew Ng  \\npicture correctly will often inspire you to come up with new categories of errors and  \\nsolutions.  \\nThe most helpful error categories will be ones that you have an idea for improving. For  \\nexample, the Instagram category will be most helpful to add if you have an idea to “undo”  \\nInstagram filters and recover the original image. But you don’t have to restrict yourself only  \\nto error categories you know how to improve; the goal of this process is to build your  \\nintuition about the most promising areas to focus on.   \\nError analysis is an iterative process. Don’t worry if you start off with no categories in mind.  \\nAfter looking at a couple of images, you might come up with a few ideas for error categories.  \\nAfter manually categorizing some images, you might think of  new categories and re-examine  \\nthe images in light of the new categories, and so on.   \\nSuppose you finish carrying out error analysis on 100 misclassified dev set examples and get  \\nthe following:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Comments\\xa0\\n1 ✔ \\xa0   Usual pitbull color \\xa0\\n2   ✔ \\xa0  \\n3  ✔ \\xa0 ✔ \\xa0 Lion; picture taken \\xa0\\nat zoo on rainy day \\xa0\\n4  ✔ \\xa0  Panther behind tree\\xa0\\n…\\xa0 …\\xa0 …\\xa0 …\\xa0 ...\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0  \\n \\nYou now know that working on a project to address the Dog mistakes can eliminate 8% of  \\nthe errors at most. Working on Great Cat or Blurry image errors could help eliminate more  \\nerrors. Therefore, you might pick one of the two latter categories to focus on. If your team  \\nhas enough people to pursue multiple directions in parallel, you can also ask some engineers  \\nto work on Great Cats and others to work on Blurry images.   \\nError analysis does not produce a rigid mathematical formula that tells you what the highest  \\npriority task should be. You also have to take into account how much progress you expect to  \\nmake on different categories and the amount of work needed to tackle each one.   \\n\\xa0\\n\\xa0\\nPage 33 Machine Learning Yearning-Draft Andrew Ng  \\n16 Cleaning up mislabeled dev and test set  \\nexamples  \\n \\nDuring error analysis, you might notice that some examples in your dev set are mislabeled.  \\nWhen I say “mislabeled” here, I mean that the pictures were already mislabeled by a human  \\nlabeler even before the algorithm encountered it. I.e., the class label in an example \\u200b(x,y)\\u200b has \\nan incorrect value for \\u200by\\u200b. For example, perhaps some pictures that are not cats are mislabeled  \\nas containing a cat, and vice versa.'),\n",
       " Document(page_content=' analysis, you might notice that some examples in your dev set are mislabeled.  \\nWhen I say “mislabeled” here, I mean that the pictures were already mislabeled by a human  \\nlabeler even before the algorithm encountered it. I.e., the class label in an example \\u200b(x,y)\\u200b has \\nan incorrect value for \\u200by\\u200b. For example, perhaps some pictures that are not cats are mislabeled  \\nas containing a cat, and vice versa. If you suspect the fraction of mislabeled images is  \\nsignificant, add a category to keep track of the fraction of examples mislabeled:   \\nImage\\xa0 Dog\\xa0 Great cat\\xa0 Blurry\\xa0 Mislabeled \\xa0 Comments \\xa0\\n…\\xa0     \\n98    ✔ \\xa0 Labeler missed cat \\xa0\\nin background \\xa0\\n99  ✔ \\xa0    \\n100    ✔ \\xa0 Drawing of a cat;\\xa0\\nnot a real cat. \\xa0\\xa0\\n% of total \\xa0 8%\\xa0 43%\\xa0 61%\\xa0 6%\\xa0  \\n \\nShould you correct the labels in your dev set? Remember that the goal of the dev set is to  \\nhelp you quickly evaluate algorithms so that you can tell if Algorithm A or B is better. If the  \\nfraction of the dev set that is mislabeled impedes your ability to make these judgments, then  \\nit is worth spending time to fix the mislabeled dev set labels.   \\nFor example, suppose your classifier’s performance is:  \\n•Overall accuracy on dev set.………………. 90% (10% overall error.)  \\n•Errors due to mislabeled examples……. 0.6% (6% of dev set errors.)   \\n•Errors due to other causes………………… 9.4% (94% of dev set errors)  \\nHere, the 0.6% inaccuracy due to mislabeling might not be significant enough relative to the  \\n9.4% of errors you could be improving. There is no harm in manually fixing the mislabeled  \\nimages in the dev set, but it is not crucial to do so: It might be fine not knowing whether your  \\nsystem has 10% or 9.4% overall error. \\nSuppose you keep improving the cat classifier and reach the following performance:   \\nPage 34 Machine Learning Yearning-Draft Andrew Ng  \\n•Overall accuracy on dev set.………………. 98.0% (2.0% overall error.) \\n•Errors due to mislabeled examples……. 0.6%. (30% of dev set errors.)   \\n•Errors due to other causes………………… 1.4% (70% of dev set errors)  \\n30% of your errors are due to the mislabeled dev set images, adding significant error to your  \\nestimates of accuracy. It is now worthwhile to improve the quality of the labels in the dev set.  \\nTackling the mislabeled examples will help you figure out if a classifier’s error is closer to  \\n1.4% or 2%—a significant relative difference.   \\nIt is not uncommon to start off tolerating some mislabeled dev/test set examples, only later  \\nto change your mind as your system improves so that the fraction of mislabeled examples  \\ngrows relative to the total set of errors.   \\nThe last chapter explained how you can improve error categories such as Dog, Great Cat and  \\nBlurry through algorithmic improvements. You have learned in this chapter that you can  \\nwork on the Mislabeled category as well—through improving the data’s labels.   \\nWhatever process you apply to fixing dev set labels, remember to apply it to the test set  \\nlabels too so that your dev and test sets continue to be drawn from the same distribution.  \\nFixing your dev and test sets together would prevent the problem we discussed in Chapter 6,  \\nwhere your team optimizes for dev set performance only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\n'),\n",
       " Document(page_content=' only to realize later that they are being  \\njudged on a different criterion based on a different test set.   \\nIf you decide to improve the label quality, consider double-checking both the labels of  \\nexamples that your system misclassified as well as labels of examples it correctly classified. It  \\nis possible that both the original label and your learning algorithm were wrong on an  \\nexample. If you fix only the labels of examples that your system had misclassified, you might  \\nintroduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier  \\nhas 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine  \\nall 980 examples classified correctly. Because it is easier in practice to check only the  \\nmisclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are  \\ninterested only in developing a product or application, but it would be a problem if you plan  \\nto use the result in an academic research paper or need a completely unbiased measure of  \\ntest set accuracy.  \\xa0\\n\\xa0\\n  \\nPage 35 Machine Learning Yearning-Draft Andrew Ng  \\n17 If you have a large dev set, split it into two  \\nsubsets, only one of which you look at   \\n \\nSuppose you have a large dev set of 5,000 examples in which you have a 20% error rate.  \\nThus, your algorithm is misclassifying ~1,000 dev images. It takes a long time to manually  \\nexamine 1,000 images, so we might decide not to use all of them in the error analysis.   \\nIn this case, I would explicitly split the dev set into two subsets, one of which you look at, and  \\none of which you don’t. You will more rapidly overfit the portion that you are manually  \\nlooking at. You can use the portion you are not manually looking at to tune parameters.   \\nLet\\u200b’\\u200bs continue our example above, in which the algorithm is misclassifying 1,000 out of  \\n5,000 dev set examples. Suppose we want to manually examine about 100 errors for error  \\nanalysis (10% of the errors). You should randomly select 10% of the dev set and place that  \\ninto what we’ll call an \\u200bEyeball dev set \\u200b to remind ourselves that we are looking at it with our  \\neyes. (For a project on speech recognition, in which you would be listening to audio clips,  \\nperhaps you would call this set an Ear dev set instead). The Eyeball dev set therefore has 500  \\nexamples, of which we would expect our algorithm to misclassify about 100.   \\nThe second subset of the dev set, called the \\u200bBlackbox dev set \\u200b, will have the remaining  \\n4500 examples. You can use the Blackbox dev set to evaluate classifiers automatically by  \\nmeasuring their error rates. You can also use it to select among algorithms or tune  \\nhyperparameters. However, you should avoid looking at it with your eyes. We use the term  \\n“Blackbox” because we will only use this subset of the data to obtain “Blackbox” evaluations  \\nof classifiers.   \\nPage 36 Machine Learning Yearning-Draft Andrew Ng \\n \\nWhy do we explicitly separate the dev set into Eyeball and Blackbox dev sets? Since you will  \\ngain intuition about the examples in the Eyeball dev set, you will start to overfit the Eyeball  \\ndev set faster. If you see the performance on the Eyeball dev set improving much more  \\nrapidly than the performance on the Blackbox dev set, you have overfit the Eyeball dev set.  \\nIn this case, you might need to discard it and find a new Eyeball dev set by moving more  \\nexamples from the Blackbox dev set into the Eyeball dev set or by acquiring new labeled  \\ndata.   \\nExplicitly splitting your dev set into Eyeball and Blackbox dev sets allows you to tell when  \\nyour manual error analysis process is causing you to overfit the Eyeball portion of your data.   \\n \\n  \\nPage 37 Machine Learning Yearning-Draft Andrew Ng \\n \\n18 How big should the Eyeball and Blackbox  \\ndev sets be?  \\nYour Eyeball dev set should be large enough to give you a sense of your algorithm’s major  \\nerror categories. If you are working on a task that humans do well (such as recognizing cats  \\nin images), here are some rough guidelines:   \\n•An eyeball dev set in which your classifier makes 10 mistakes would be considered very  \\nsmall. With just 10 errors, it’s hard to accurately estimate the impact of different error  \\ncategories. But if you have very little data and cannot afford to put more into the Eyeball  \\ndev set, it \\u200b’\\u200bs better than nothing and'),\n",
       " Document(page_content='. If you are working on a task that humans do well (such as recognizing cats  \\nin images), here are some rough guidelines:   \\n•An eyeball dev set in which your classifier makes 10 mistakes would be considered very  \\nsmall. With just 10 errors, it’s hard to accurately estimate the impact of different error  \\ncategories. But if you have very little data and cannot afford to put more into the Eyeball  \\ndev set, it \\u200b’\\u200bs better than nothing and will help with project prioritization.   \\n•If your classifier makes ~20 mistakes on eyeball dev examples, you would start to get a  \\nrough sense of the major error sources.   \\n•With ~50 mistakes, you would get a good sense of the major error sources.  \\n•With ~100 mistakes, you would get a very good sense of the major sources of errors. I’ve  \\nseen people manually analyze even more errors—sometimes as many as 500. There is no  \\nharm in this as long as you have enough data.   \\nSay your classifier has a 5% error rate. To make sure you have ~100 misclassified examples  \\nin the Eyeball dev set, the Eyeball dev set would have to have about 2,000 examples (since  \\n0.05*2,000 = 100). The lower your classifier’s error rate, the larger your Eyeball dev set  \\nneeds to be in order to get a large enough set of errors to analyze.   \\nIf you are working on a task that even humans cannot do well, then the exercise of examining  \\nan Eyeball dev set will not be as helpful because it is harder to figure out why the algorithm  \\ndidn’t classify an example correctly. In this case, you might omit having an Eyeball dev set.  \\nWe discuss guidelines for such problems in a later chapter.   \\nPage 38 Machine Learning Yearning-Draft Andrew Ng \\n \\nHow about the Blackbox dev set? We previously said that dev sets of around 1,000-10,000  \\nexamples are common. To refine that statement, a Blackbox dev set of 1,000-10,000  \\nexamples will often give you enough data to tune hyperparameters and select among models,  \\nthough there is little harm in having even more data. A Blackbox dev set of 100 would be  \\nsmall but still useful.   \\nIf you have a small dev set, then you might not have enough data to split into Eyeball and  \\nBlackbox dev sets that are both large enough to serve their purposes. Instead, your entire dev  \\nset might have to be used as the Eyeball dev set—i.e., you would manually examine all the  \\ndev set data.   \\nBetween the Eyeball and Blackbox dev sets, I consider the Eyeball dev set more important  \\n(assuming that you are working on a problem that humans can solve well and that examining  \\nthe examples helps you gain insight). If you only have an Eyeball dev set, you can perform  \\nerror analyses, model selection and hyperparameter tuning all on that set. The downside of  \\nhaving only an Eyeball dev set is that the risk of overfitting the dev set is greater.   \\nIf you have plentiful access to data, then the size of the Eyeball dev set would be determined  \\nmainly by how many examples you have time to manually analyze. For example, I’ve rarely  \\nseen anyone manually analyze more than 1,000 errors.  \\n \\n \\n \\xa0\\nPage 39 Machine Learning Yearning-Draft Andrew Ng \\n \\n19 Takeaways: Basic error analysis  \\n \\n•When you start a new project, especially if it is in an area in which you are not an expert,  \\nit is hard to correctly guess the most promising directions.   \\n•So don’t start off trying to design and build the perfect system. Instead build and train a  \\nbasic system as quickly as possible—perhaps in a few days. Then use error analysis to  \\nhelp you identify the most promising directions and iteratively improve your algorithm  \\nfrom there.  \\n•Carry out error analysis by manually examining ~100 dev set examples the algorithm  \\nmisclassifies and counting the major categories of errors. Use this information to  \\nprioritize what types of errors to work on fixing.   \\n•Consider splitting the dev set into an Eyeball dev set, which you will manually examine,  \\nand a Blackbox dev set, which you will not manually examine. If performance on the  \\nEyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev  \\nset and should consider acquiring more data for it.   \\n•The Eyeball dev set should be big enough so that your algorithm misclassifies enough  \\nexamples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is'),\n",
       " Document(page_content=' examine,  \\nand a Blackbox dev set, which you will not manually examine. If performance on the  \\nEyeball dev set is much better than the Blackbox dev set, you have overfit the Eyeball dev  \\nset and should consider acquiring more data for it.   \\n•The Eyeball dev set should be big enough so that your algorithm misclassifies enough  \\nexamples for you to analyze. A Blackbox dev set of 1,000-10,000 examples is sufficient  \\nfor many applications.  \\n•If your dev set is not big enough to split this way, just use the entire dev set as an Eyeball  \\ndev set for manual error analysis, model selection, and hyperparameter tuning.   \\n \\n \\n \\n  \\nPage 40 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBias and Variance  \\n \\n \\n \\n \\n \\n \\xa0\\nPage 41 Machine Learning Yearning-Draft Andrew Ng  \\n20 Bias and Variance: The two big sources of  \\nerror  \\n \\nSuppose your training, dev and test sets all come from the same distribution. Then you  \\nshould always try to get more training data, since that can only improve performance, right?  \\nEven though having more data can’t hurt, unfortunately it doesn’t always help as much as  \\nyou might hope. It could be a waste of time to work on getting more data. So, how do you  \\ndecide when to add data, and when not to bother?  \\nThere are two major sources of error in machine learning: bias and variance. Understanding  \\nthem will help you decide whether adding data, as well as other tactics to improve  \\nperformance, are a good use of time.   \\nSuppose you hope to build a cat recognizer that has 5% error. Right now, your training set  \\nhas an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding  \\ntraining data probably won’t help much. You should focus on other changes. Indeed, adding  \\nmore examples to your training set only makes it harder for your algorithm to do well on the  \\ntraining set. (We explain why in a later chapter.)   \\nIf your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error  \\n(95% accuracy), then the first problem to solve is to improve your algorithm \\u200b’\\u200bs performance  \\non your training set. Your dev/test set performance is usually worse than your training set  \\nperformance. So if you are getting 85% accuracy on the examples your algorithm has seen,  \\nthere’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen.   \\nSuppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break  \\nthe 16% error into two components:   \\n•First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of  \\nthis informally as the algorithm’s \\u200bbias\\u200b.  \\n•Second, how much worse the algorithm does on the dev (or test) set than the training set.  \\nIn this example, it does 1% worse on the dev set than the training set. We think of this  \\ninformally as the algorithm’s \\u200bvariance \\u200b. 6\\n6 The field of statistics has mor e for mal definitions of bias and v ariance that we won’ t worr y about.  \\nRoughly,  the bias is the err or rate of your algor ithm on your  training set when you hav e a v ery large  \\ntraining set.  The variance is how much worse you do on the test set compar ed to the tr aining set in  \\nPage 42 Machine Learning Yearning-Draft Andrew Ng  \\nSome changes to a learning algorithm can address the first component of error— \\u200bbias\\u200b—and \\nimprove its performance on the training set. Some changes address the second  \\ncomponent— \\u200bvariance \\u200b—and help it generalize better from the training set to the dev/test  \\nsets.  To select the most promising changes, it is incredibly useful to understand which of  7\\nthese two components of error is more pressing to address.  \\nDeveloping good intuition about Bias and Variance will help you choose effective changes for  \\nyour algorithm.  \\n\\xa0\\n\\xa0  \\nthis setting.  When your err or metr ic is mean sq uared er ror, you can write down formulas specifying \\nthese two q uantities,  and pr ove that T otal Err or = Bias + V ariance.  But for  our purposes of deciding  \\nhow to mak e progress on an ML  problem,  the more informal definition of bias and v ariance giv en \\nhere will suffice.   \\n'),\n",
       " Document(page_content=' algorithm.  \\n\\xa0\\n\\xa0  \\nthis setting.  When your err or metr ic is mean sq uared er ror, you can write down formulas specifying \\nthese two q uantities,  and pr ove that T otal Err or = Bias + V ariance.  But for  our purposes of deciding  \\nhow to mak e progress on an ML  problem,  the more informal definition of bias and v ariance giv en \\nhere will suffice.   \\n7 There are also some methods that can simultaneously red uce bias and v ariance,  by mak ing major  \\nchanges to the system architecture.  But these tend to be harder to identify and implement.   \\nPage 43 Machine Learning Yearning-Draft Andrew Ng  \\n21 Examples of Bias and Variance \\n \\nConsider our cat classification task. An “ideal” classifier (such as a human) might achieve  \\nnearly perfect performance in this task.   \\nSuppose your algorithm performs as follows:  \\n•Training error = 1%  \\n•Dev error = 11%  \\nWhat problem does it have? Applying the definitions from the previous chapter, we estimate  \\nthe bias as 1%, and the variance as 10% (=11%-1%). Thus, it has \\u200bhigh variance \\u200b. The \\nclassifier has very low training error, but it is failing to generalize to the dev set. This is also  \\ncalled \\u200boverfitting \\u200b. \\nNow consider this:  \\n•Training error = 15%  \\n•Dev error = 16% \\nWe estimate the bias as 15%, and variance as 1%. This classifier is fitting the training set  \\npoorly with 15% error, but its error on the dev set is barely higher than the training error.  \\nThis classifier therefore has \\u200bhigh bias \\u200b, but low variance. We say that this algorithm is  \\nunderfitting \\u200b. \\nNow, consider this:  \\n•Training error = 15%  \\n•Dev error = 30% \\nWe estimate the bias as 15%, and variance as 15%. This classifier has \\u200bhigh bias and high  \\nvariance \\u200b: It is doing poorly on the training set, and therefore has high bias, and its  \\nperformance on the dev set is even worse, so it also has high variance. The  \\noverfitting/underfitting terminology is hard to apply here since the classifier is  \\nsimultaneously overfitting and underfitting.   \\n \\n \\nPage 44 Machine Learning Yearning-Draft Andrew Ng  \\nFinally, consider this:  \\n•Training error = 0.5%  \\n•Dev error = 1% \\nThis classifier is doing well, as it has low bias and low variance. Congratulations on achieving  \\nthis great performance!  \\xa0\\n  \\nPage 45 Machine Learning Yearning-Draft Andrew Ng  \\n22 Comparing to the optimal error rate  \\n \\nIn our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal”  \\nclassifier—is nearly 0%. A human looking at a picture would be able to recognize if it  \\ncontains a cat almost all the time; thus, we can hope for a machine that would do just as well.   \\nOther problems are harder. For example, suppose that you are building a speech recognition  \\nsystem, and find that 14% of the audio clips have so much background noise or are so  \\nunintelligible that even a human cannot recognize what was said. In this case, even the most  \\n“optimal” speech recognition system might have error around 14%.   \\nSuppose that on this speech recognition problem, your algorithm achieves:   \\n•Training error = 15%  \\n•Dev error = 30% \\nThe training set performance is already close to the optimal error rate of 14%. Thus, there is  \\nnot much room for improvement in terms of bias or in terms of training set performance.  \\nHowever, this algorithm is not generalizing well to the dev set; thus there is ample room for  \\nimprovement in the errors due to variance.   \\nThis example is similar to the third example from the previous chapter, which also had a  \\ntraining error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training  \\nerror of 15% leaves much room for improvement. This suggests bias-reducing changes might  \\nbe fruitful. But if the optimal error rate is 14%, then the same training set performance tells  \\nus that there’s little room for improvement in the classifier’s bias.   \\nFor problems where the optimal error rate is far from zero, here \\u200b’\\u200bs a more detailed  \\nbreakdown of an algorithm \\u200b’\\u200bs error. Continuing with our speech recognition example above,  \\nthe total dev set error of 30% can be broken down as follows (a similar analysis can be  \\napplied to the test'),\n",
       " Document(page_content=' if the optimal error rate is 14%, then the same training set performance tells  \\nus that there’s little room for improvement in the classifier’s bias.   \\nFor problems where the optimal error rate is far from zero, here \\u200b’\\u200bs a more detailed  \\nbreakdown of an algorithm \\u200b’\\u200bs error. Continuing with our speech recognition example above,  \\nthe total dev set error of 30% can be broken down as follows (a similar analysis can be  \\napplied to the test set error):   \\n•Optimal error rate (“unavoidable bias”) \\u200b: 14%. Suppose we decide that, even with the  \\nbest possible speech system in the world, we would still suffer 14% error. We can think of  \\nthis as the “unavoidable” part of a learning algorithm \\u200b’\\u200bs bias.   \\nPage 46 Machine Learning Yearning-Draft Andrew Ng  \\n•Avoidable bias \\u200b: 1%. This is calculated as the difference between the training error and  \\nthe optimal error rate.  8\\n•Variance \\u200b: 15%. The difference between the dev error and the training error.   \\nTo relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:   9\\nBias = Optimal error rate (“unavoidable bias”) + Avoidable bias  \\nThe “avoidable bias” reflects how much worse your algorithm performs on the training set  \\nthan the “optimal classifier.”   \\nThe concept of variance remains the same as before. In theory, we can always reduce  \\nvariance to nearly zero by training on a massive training set. Thus, all variance is “avoidable”  \\nwith a sufficiently large dataset, so there is no such thing as “unavoidable variance.”   \\nConsider one more example, where the optimal error rate is 14%, and we have:   \\n•Training error = 15%  \\n•Dev error = 16% \\nWhereas in the previous chapter we called this a high bias classifier, now we would say that  \\nerror from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm  \\nis already doing well, with little room for improvement. It is only 2% worse than the optimal  \\nerror rate.   \\nWe see from these examples that knowing the optimal error rate is helpful for guiding our  \\nnext steps. In statistics, the optimal error rate is also called \\u200bBayes error rate \\u200b, or Bayes \\nrate.  \\nHow do we know what the optimal error rate is? For tasks that humans are reasonably good  \\nat, such as recognizing pictures or transcribing audio clips, you can ask a human to provide  \\nlabels then measure the accuracy of the human labels relative to your training set. This  \\nwould give an estimate of the optimal error rate. If you are working on a problem that even  \\n8 If this number  is negativ e, you are doing better  on the tr aining set than the optimal er ror rate.  This  \\nmeans you are ov erfitting on the training set,  and the algor ithm has ov er-memorized the tr aining set.  \\nYou should focus on v ariance r eduction methods rather than on fur ther  bias reduction methods.  \\n9 These definitions ar e chosen  to conv ey insight on how to impr ove your  lear ning algorithm.  These \\ndefinitions ar e different than how statisticians define Bias and V ariance.  Technically,  what I define \\nhere as “Bias” should be called “Error  we attribute to bias”;  and “Av oidable bias” should be “err or we \\nattribute to the lear ning algorithm’ s bias that is ov er the optimal er ror rate. ”  \\nPage 47 Machine Learning Yearning-Draft Andrew Ng  \\nhumans have a hard time solving (e.g., predicting what movie to recommend, or what ad to  \\nshow to a user) it can be hard to estimate the optimal error rate.   \\nIn the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss  \\nin more detail the process of comparing a learning algorithm’s performance to human-level  \\nperformance.  \\nIn the last few chapters, you learned how to estimate avoidable/unavoidable bias and  \\nvariance by looking at training and dev set error rates. The next chapter will discuss how you  \\ncan use insights from such an analysis to prioritize techniques that reduce bias vs.  \\ntechniques that reduce variance. There are very different techniques that you should apply  \\ndepending on whether your project’s current problem is high (avoidable) bias or high  \\nvariance. Read on!   \\n  \\nPage 48 Machine Learning Yearning-Draft Andrew Ng  \\n23 Addressing Bias and Variance \\n \\nHere is the simplest formula for addressing bias and variance issues:  \\n•'),\n",
       " Document(page_content=' The next chapter will discuss how you  \\ncan use insights from such an analysis to prioritize techniques that reduce bias vs.  \\ntechniques that reduce variance. There are very different techniques that you should apply  \\ndepending on whether your project’s current problem is high (avoidable) bias or high  \\nvariance. Read on!   \\n  \\nPage 48 Machine Learning Yearning-Draft Andrew Ng  \\n23 Addressing Bias and Variance \\n \\nHere is the simplest formula for addressing bias and variance issues:  \\n•If you have high avoidable bias, increase the size of your model (for example, increase the  \\nsize of your neural network by adding layers/neurons).  \\n•If you have high variance, add data to your training set.  \\nIf you are able to increase the neural network size and increase training data without limit, it  \\nis possible to do very well on many learning problems.   \\nIn practice, increasing the size of your model will eventually cause you to run into  \\ncomputational problems because training very large models is slow. You might also exhaust  \\nyour ability to acquire more training data. (Even on the internet, there is only a finite  \\nnumber of cat pictures!)   \\nDifferent model architectures—for example, different neural network architectures—will  \\nhave different amounts of bias/variance for your problem. A lot of recent deep learning  \\nresearch has developed many innovative model architectures. So if you are using neural  \\nnetworks, the academic literature can be a great source of inspiration. There are also many  \\ngreat open-source implementations on github. But the results of trying new architectures are  \\nless predictable than the simple formula of increasing the model size and adding data.   \\nIncreasing the model size generally reduces bias, but it might also increase variance and the  \\nrisk of overfitting. However, this overfitting problem usually arises only when you are not  \\nusing regularization. If you include a well-designed regularization method, then you can  \\nusually safely increase the size of the model without increasing overfitting.   \\nSuppose you are applying deep learning, with L2 regularization or dropout, with the  \\nregularization parameter that performs best on the dev set. If you increase the model size,  \\nusually your performance will stay the same or improve; it is unlikely to worsen significantly.  \\nThe only reason to avoid using a bigger model is the increased computational cost.   \\n \\n\\xa0\\n\\xa0 \\xa0\\nPage 49 Machine Learning Yearning-Draft Andrew Ng  \\n24 Bias vs. Variance tradeoff \\n \\nYou might have heard of the “Bias vs. Variance tradeoff.” Of the changes you could make to  \\nmost learning algorithms, there are some that reduce bias errors but at the cost of increasing  \\nvariance, and vice versa. This creates a “trade off” between bias and variance.   \\nFor example, increasing the size of your model—adding neurons/layers in a neural network,  \\nor adding input features—generally reduces bias but could increase variance. Alternatively,  \\nadding regularization generally increases bias but reduces variance.   \\nIn the modern era, we often have access to plentiful data and can use very large neural  \\nnetworks (deep learning). Therefore, there is less of a tradeoff, and there are now more  \\noptions for reducing bias without hurting variance, and vice versa.   \\nFor example, you can usually increase a neural network size and tune the regularization  \\nmethod to reduce bias without noticeably increasing variance. By adding training data, you  \\ncan also usually reduce variance without affecting bias.   \\nIf you select a model architecture that is well suited for your task, you might also reduce bias  \\nand variance simultaneously. Selecting such an architecture can be difficult.   \\nIn the next few chapters, we discuss additional specific techniques for addressing bias and  \\nvariance.   \\n \\n  \\nPage 50 Machine Learning Yearning-Draft Andrew Ng  \\n25 Techniques for reducing avoidable bias  \\n \\nIf your learning algorithm suffers from high avoidable bias, you might try the following  \\ntechniques:  \\n•Increase the model size \\u200b(such as number of neurons/layers): This technique reduces  \\nbias, since it should allow you to fit the training set better. If you find that this increases  \\nvariance, then use regularization, which will usually eliminate the increase in variance.  \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm eliminate a  \\nparticular category of errors. (We discuss this further in the next chapter.) These new  \\nfeatures could help with both bias and variance. In theory, adding more features could  \\nincrease the variance; but if you find this to be the case, then use regularization, which will  \\nusually eliminate the increase in variance.   \\n•Reduce or eliminate regularization \\u200b (L2 regularization, L1 regularization, dropout):  \\nThis will reduce avoidable bias, but increase variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it'),\n",
       " Document(page_content=' further in the next chapter.) These new  \\nfeatures could help with both bias and variance. In theory, adding more features could  \\nincrease the variance; but if you find this to be the case, then use regularization, which will  \\nusually eliminate the increase in variance.   \\n•Reduce or eliminate regularization \\u200b (L2 regularization, L1 regularization, dropout):  \\nThis will reduce avoidable bias, but increase variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\nOne method that is not helpful:   \\n•Add more training data \\u200b: This technique helps with variance problems, but it usually  \\nhas no significant effect on bias.   \\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 51 Machine Learning Yearning-Draft Andrew Ng  \\n26 Error analysis on the training set  \\n \\nYour algorithm must perform well on the training set before you can expect it to perform  \\nwell on the dev/test sets.   \\nIn addition to the techniques described earlier to address high bias, I sometimes also carry  \\nout an error analysis on the \\u200btraining data \\u200b, following a protocol similar to error analysis on  \\nthe Eyeball dev set. This can be useful if your algorithm has high bias—i.e., if it is not fitting  \\nthe training set well.   \\nFor example, suppose you are building a speech recognition system for an app and have  \\ncollected a training set of audio clips from volunteers. If your system is not doing well on the  \\ntraining set, you might consider listening to a set of ~100 examples that the algorithm is  \\ndoing poorly on to understand the major categories of training set errors. Similar to the dev  \\nset error analysis, you can count the errors in different categories:   \\nAudio clip \\xa0 Loud background \\xa0\\nnoise\\xa0User spoke\\xa0\\nquickly\\xa0Far from \\xa0\\nmicrophone \\xa0Comments\\xa0\\n1 ✔ \\xa0   Car noise \\xa0\\n2 ✔ \\xa0  ✔ \\xa0 Restaurant noise \\xa0\\n3  ✔ \\xa0 ✔ \\xa0 User shouting\\xa0\\nacross living room?\\xa0\\n4 ✔ \\xa0 \\xa0\\xa0  Coffeeshop \\xa0\\n% of total \\xa0 75%\\xa0 25%\\xa0 50%\\xa0  \\n \\nIn this example, you might realize that your algorithm is having a particularly hard time with  \\ntraining examples that have a lot of background noise. Thus, you might focus on techniques  \\nthat allow it to better fit training examples with background noise.   \\nYou might also double-check whether it is possible for a person to transcribe these audio  \\nclips, given the same input audio as your learning algorithm. If there is so much background  \\nnoise that it is simply impossible for anyone to make out what was said, then it might be  \\nunreasonable to expect any algorithm to correctly recognize such utterances. We will discuss  \\nthe benefits of comparing your algorithm to human-level performance in a later section.  \\n\\xa0\\n\\xa0\\nPage 52 Machine Learning Yearning-Draft Andrew Ng  \\n27 Techniques for reducing variance  \\n \\nIf your learning algorithm suffers from high variance, you might try the following  \\ntechniques:  \\n•Add more training data \\u200b: This is the simplest and most reliable way to address variance,  \\nso long as you have access to significantly more data and enough computational power to  \\nprocess the data.   \\n•Add regularization \\u200b (L2 regularization, L1 regularization, dropout): This technique \\nreduces variance but increases bias.   \\n•Add early stopping \\u200b (i.e., stop gradient descent early, based on dev set error): This  \\ntechnique reduces variance but increases bias. Early stopping behaves a lot like  \\nregularization methods, and some authors call it a regularization technique.   \\n•Feature selection to decrease number/type of input features: \\u200b This technique  \\nmight help with variance problems, but it might also increase bias. Reducing the number  \\nof features slightly (say going from 1,000 features to 900) is unlikely to have a huge effect  \\non bias. Reducing it significantly (say going from 1,000 features to 100—a 10x reduction)  \\nis more likely to have a significant effect, so long as you are not excluding too many useful  \\nfeatures. In modern deep learning, when data is plentiful, there has been a shift away from  \\nfeature selection, and we are now more likely to give all the features we have to the  \\nalgorithm and let the algorithm sort out which ones to use based on the data. But when  \\nyour training set is small, feature selection can be very useful.   \\n•Decrease the model size \\u200b(such as number of neurons/layers): \\u200bUse with caution. \\u200b This \\ntechnique could decrease variance, while possibly increasing bias. However, I don’t  \\nrecommend this technique for addressing variance. Adding regularization usually gives  \\nbetter classification performance. The advantage of reducing the model size is reducing  \\nyour computational cost and thus speeding up how quickly'),\n",
       " Document(page_content=' to use based on the data. But when  \\nyour training set is small, feature selection can be very useful.   \\n•Decrease the model size \\u200b(such as number of neurons/layers): \\u200bUse with caution. \\u200b This \\ntechnique could decrease variance, while possibly increasing bias. However, I don’t  \\nrecommend this technique for addressing variance. Adding regularization usually gives  \\nbetter classification performance. The advantage of reducing the model size is reducing  \\nyour computational cost and thus speeding up how quickly you can train models. If  \\nspeeding up model training is useful, then by all means consider decreasing the model size.  \\nBut if your goal is to reduce variance, and you are not concerned about the computational  \\ncost, consider adding regularization instead.   \\nHere are two additional tactics, repeated from the previous chapter on addressing bias:   \\n•Modify input features based on insights from error analysis \\u200b: Say your error \\nanalysis inspires you to create additional features that help the algorithm to eliminate a  \\nparticular category of errors. These new features could help with both bias and variance. In  \\nPage 53 Machine Learning Yearning-Draft Andrew Ng  \\ntheory, adding more features could increase the variance; but if you find this to be the case,  \\nthen use regularization, which will usually eliminate the increase in variance.   \\n•Modify model architecture \\u200b (such as neural network architecture) so that it is more  \\nsuitable for your problem: This technique can affect both bias and variance.   \\n \\n \\n  \\nPage 54 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLearning curves  \\n\\xa0\\n\\xa0 \\xa0\\nPage 55 Machine Learning Yearning-Draft Andrew Ng  \\n28 Diagnosing bias and variance: Learning  \\ncurves  \\n \\nWe’ve seen some ways to estimate how much error can be attributed to avoidable bias vs.  \\nvariance. We did so by estimating the optimal error rate and computing the algorithm’s  \\ntraining set and dev set errors. Let’s discuss a technique that is even more informative:  \\nplotting a learning curve.  \\nA learning curve plots your dev set error against the number of training examples. To plot it,  \\nyou would run your algorithm using different training set sizes. For example, if you have  \\n1,000 examples, you might train separate copies of the algorithm on 100, 200, 300, …, 1000 \\nexamples. Then you could plot how dev set error varies with the training set size. Here is an  \\nexample: \\nAs the training set size increases, the dev set error should decrease.   \\nWe will often have some “desired error rate” that we hope our learning algorithm will  \\neventually achieve. For example:   \\n•If we hope for human-level performance, then the human error rate could be the “desired  \\nerror rate.”  \\n•If our learning algorithm serves some product (such as delivering cat pictures), we might  \\nhave an intuition about what level of performance is needed to give users a great  \\nexperience.  \\nPage 56 Machine Learning Yearning-Draft Andrew Ng \\n \\n•If you have worked on a important application for a long time, then you might have  \\nintuition about how much more progress you can reasonably make in the next  \\nquarter/year.   \\nAdd the desired level of performance to your learning curve:   \\n \\n \\n \\n \\n \\n \\n \\nYou can visually extrapolate the red “dev error” curve  to guess how much closer you could  \\nget to the desired level of performance by adding more data. In the example above, it looks  \\nplausible that doubling the training set size might allow you to reach the desired  \\nperformance.  \\nBut if the dev error curve has “plateaued” (i.e. flattened out), then you can immediately tell  \\nthat adding more data won’t get you to your goal:   \\nLooking at the learning curve might therefore help you avoid spending months collecting  \\ntwice as much training data, only to realize it does not help.  \\nPage 57 Machine Learning Yearning-Draft Andrew Ng \\n \\nOne downside of this process is that if you only look at the dev error curve, it can be hard to  \\nextrapolate and predict exactly where the red curve will go if you had more data. There is one  \\nadditional plot that can help you estimate the impact of adding more data: the training error.  \\nPage 58 Machine Learning Yearning-Draft Andrew Ng  \\n29 Plotting training error \\n \\nYour dev set (and test set) error should decrease as the training set size grows. But your  \\ntraining set error usually \\u200bincreases \\u200b as the training set size grows.   \\nLet’s illustrate this effect with an example. Suppose your training set has only 2 examples:  \\nOne cat image and one non-cat image. Then it is easy for the learning algorithms to  \\n“memorize” both examples in the training set, and get 0% training set error'),\n",
       " Document(page_content='ting training error \\n \\nYour dev set (and test set) error should decrease as the training set size grows. But your  \\ntraining set error usually \\u200bincreases \\u200b as the training set size grows.   \\nLet’s illustrate this effect with an example. Suppose your training set has only 2 examples:  \\nOne cat image and one non-cat image. Then it is easy for the learning algorithms to  \\n“memorize” both examples in the training set, and get 0% training set error. Even if either or  \\nboth of the training examples were mislabeled, it is still easy for the algorithm to memorize  \\nboth labels.   \\nNow suppose your training set has 100 examples. Perhaps even a few examples are  \\nmislabeled, or ambiguous—some images are very blurry, so even humans cannot tell if there  \\nis a cat. Perhaps the learning algorithm can still “memorize” most or all of the training set,  \\nbut it is now harder to obtain 100% accuracy. By increasing the training set from 2 to 100  \\nexamples, you will find that the training set accuracy will drop slightly.   \\nFinally, suppose your training set has 10,000 examples. In this case, it becomes even harder  \\nfor the algorithm to perfectly fit all 10,000 examples, especially if some are ambiguous or  \\nmislabeled. Thus, your learning algorithm will do even worse on this training set.   \\nLet’s add a plot of training error to our earlier figures:   \\n \\n \\n \\n \\n \\n \\n \\nYou can see that the blue “training error” curve increases with the size of the training set.  \\nFurthermore, your algorithm usually does better on the training set than on the dev set; thus  \\nthe red dev error curve usually lies strictly above the blue training error curve.   \\nLet’s discuss next how to interpret these plots. \\xa0\\nPage 59 Machine Learning Yearning-Draft Andrew Ng \\n \\n30 Interpreting learning curves: High bias  \\n \\nSuppose your dev error curve looks like this:   \\n \\n \\n \\n \\n \\n \\nWe previously said that, if your dev error curve plateaus, you are unlikely to achieve the  \\ndesired performance just by adding data.   \\nBut it is hard to know exactly what an extrapolation of the red dev error curve will look like.  \\nIf the dev set was small, you would be even less certain because the curves could be noisy.  \\nSuppose we add the training error curve to this plot and get the following:   \\n \\n \\n \\n \\n \\n \\n \\nNow, you can be absolutely sure that adding more data will not, by itself, be sufficient. Why  \\nis that? Remember our two observations:  \\nPage 60 Machine Learning Yearning-Draft Andrew Ng \\n \\n•As we add more training data, training error can only get worse. Thus, the blue training  \\nerror curve can only stay the same or go higher, and thus it can only get further away from  \\nthe (green line) level of desired performance.   \\n•The red dev error curve is usually higher than the blue training error. Thus, there’s almost  \\nno way that adding more data would allow the red dev error curve to drop down to the  \\ndesired level of performance when even the training error is higher than the desired level  \\nof performance.  \\nExamining both the dev error curve and the training error curve on the same plot allows us  \\nto more confidently extrapolate the dev error curve.   \\nSuppose, for the sake of discussion, that the desired performance is our estimate of the  \\noptimal error rate. The figure above is then the standard “textbook” example of what a  \\nlearning curve with high avoidable bias looks like: At the largest training set  \\nsize—presumably corresponding to all the training data we have—there is a large gap  \\nbetween the training error and the desired performance, indicating large avoidable bias.  \\nFurthermore, the gap between the training and dev curves is small, indicating small  \\nvariance.   \\nPreviously, we were measuring training and dev set error only at the rightmost point of this  \\nplot, which corresponds to using all the available training data. Plotting the full learning  \\ncurve gives us a more comprehensive picture of the algorithms’ performance on different  \\ntraining set sizes.   \\n \\n  \\nPage 61 Machine Learning Yearning-Draft Andrew Ng  \\n31 Interpreting learning curves: Other cases   \\n \\nConsider this learning curve:  \\n \\nDoes this plot indicate high bias, high variance, or both?  \\nThe blue training error curve is relatively low, and the red dev error curve is much higher  \\nthan the blue training error. Thus, the bias is small, but the variance is large. Adding more  \\ntraining data will probably help close the gap between dev error and training error.  \\nNow, consider this: \\n \\nThis time, the training error is large, as it is much higher than the desired level of  \\nperformance. The dev error is also much larger than'),\n",
       " Document(page_content=' high variance, or both?  \\nThe blue training error curve is relatively low, and the red dev error curve is much higher  \\nthan the blue training error. Thus, the bias is small, but the variance is large. Adding more  \\ntraining data will probably help close the gap between dev error and training error.  \\nNow, consider this: \\n \\nThis time, the training error is large, as it is much higher than the desired level of  \\nperformance. The dev error is also much larger than the training error. Thus, you have  \\nsignificant bias and significant variance. You will have to find a way to reduce both bias and  \\nvariance in your algorithm.   \\nPage 62 Machine Learning Yearning-Draft Andrew Ng  \\n32 Plotting learning curves  \\n \\nSuppose you have a very small training set of 100 examples. You train your algorithm using a  \\nrandomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing  \\nthe number of examples by intervals of ten. You then use these 10 data points to plot your  \\nlearning curve. You might find that the curve looks slightly noisy (meaning that the values  \\nare higher/lower than expected) at the smaller training set sizes.   \\nWhen training on just 10 randomly chosen examples, you might be unlucky and have a  \\nparticularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or,  \\nyou might get lucky and get a particularly “good” training set. Having a small training set  \\nmeans that the dev and training errors may randomly fluctuate.   \\nIf your machine learning application is heavily skewed toward one class (such as a cat  \\nclassification task where the fraction of negative examples is much larger than positive  \\nexamples), or if it has a huge number of classes (such as recognizing 100 different animal  \\nspecies), then the chance of selecting an especially “unrepresentative” or bad training set is  \\nalso larger. For example, if 80% of your examples are negative examples (y=0), and only  \\n20% are positive examples (y=1), then there is a chance that a training set of 10 examples  \\ncontains only negative examples, thus making it very difficult for the algorithm to learn  \\nsomething meaningful.   \\nIf the noise in the training curve makes it hard to see the true trends, here are two solutions:   \\n•Instead of training just one model on 10 examples, instead select several (say 3-10)  \\ndifferent randomly chosen training sets of 10 examples by sampling with replacement  10\\nfrom your original set of 100. Train a different model on each of these, and compute the  \\ntraining and dev set error of each of the resulting models. Compute and plot the average  \\ntraining error and average dev set error.  \\n•If your training set is skewed towards one class, or if it has many classes, choose a  \\n“balanced” subset instead of 10 training examples at random out of the set of 100. For  \\nexample, you can make sure that 2/10 of the examples are positive examples, and 8/10 are  \\n10 Here’s what sampling \\u200bwith replacement \\u200b means: You would randomly pick 10 different examples out of the 100 to form  \\nyour first training set. Then to form the second training set, you would again pick 10 examples, but without taking into \\naccount what had been chosen in the first training set. Thus, it is possible for one specific exa mple to appear in both the  \\nfirst and second training sets. In contrast, if you were sampling \\u200bwithout replacement \\u200b, the second training set would be  \\nchosen from just the 90 examples that had not been chosen the first time around. In practice, sampling with or without \\nreplacement shouldn’t make a huge difference, but the former is common practice.   \\nPage 63 Machine Learning Yearning-Draft Andrew Ng  \\nnegative. More generally, you can make sure the fraction of examples from each class is as  \\nclose as possible to the overall fraction in the original training set.   \\nI would not bother with either of these techniques unless you have already tried plotting  \\nlearning curves and concluded that the curves are too noisy to see the underlying trends. If  \\nyour training set is large—say over 10,000 examples—and your class distribution is not very  \\nskewed, you probably won’t need these techniques.   \\nFinally, plotting a learning curve may be computationally expensive: For example, you might  \\nhave to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training  \\nmodels with small datasets is much faster than training models with large datasets. Thus,  \\ninstead of evenly spacing out the training set sizes on a linear scale as above, you might train  \\nmodels with 1,000, 2,000,'),\n",
       " Document(page_content=' techniques.   \\nFinally, plotting a learning curve may be computationally expensive: For example, you might  \\nhave to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training  \\nmodels with small datasets is much faster than training models with large datasets. Thus,  \\ninstead of evenly spacing out the training set sizes on a linear scale as above, you might train  \\nmodels with 1,000, 2,000, 4,000, 6,000, and 10,000 examples. This should still give you a  \\nclear sense of the trends in the learning curves. Of course, this technique is relevant only if  \\nthe computational cost of training all the additional models is significant.   \\n \\n \\n \\n \\n \\n \\n \\xa0\\nPage 64 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nComparing to \\nhuman-level \\nperformance \\n\\xa0\\n\\xa0\\n  \\nPage 65 Machine Learning Yearning-Draft Andrew Ng  \\n33 Why we compare to human-level  \\nperformance  \\n \\nMany machine learning systems aim to automate things that humans do well. Examples  \\ninclude image recognition, speech recognition, and email spam classification. Learning  \\nalgorithms have also improved so much that we are now surpassing human-level  \\nperformance on more and more of these tasks.   \\nFurther, there are several reasons building an ML system is easier if you are trying to do a  \\ntask that people can do well:   \\n1. Ease of obtaining data from human labelers. \\u200b For example, since people recognize  \\ncat images well, it is straightforward for people to provide high accuracy labels for your  \\nlearning algorithm.  \\n2. Error analysis can draw on human intuition. \\u200b Suppose a speech recognition  \\nalgorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\naudio clip as “This recipe calls for a \\u200bpear\\u200b of apples,” mistaking “pair” for “pear.” You can  \\ndraw on human intuition and try to understand what information a person uses to get the  \\ncorrect transcription, and use this knowledge to modify the learning algorithm.   \\n3. Use human-level performance to estimate the optimal error rate and also set  \\na “desired error rate.” \\u200b Suppose your algorithm achieves 10% error on a task, but a person  \\nachieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\navoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\nEven though item #3 might not sound important, I find that having a reasonable and  \\nachievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\nhigh avoidable bias is incredibly valuable and opens up a menu of options to try.   \\nThere are some tasks that even humans aren’t good at. For example, picking a book to  \\nrecommend to you; or picking an ad to show a user on a website; or predicting the stock  \\nmarket. Computers already surpass the performance of most people on these tasks. With  \\nthese applications, we run into the following problems:   \\n•It is harder to obtain labels. \\u200b For example, it’s hard for human labelers to annotate a  \\ndatabase of users with the “optimal” book recommendation. If you operate a website or  \\napp that sells books, you can obtain data by showing books to users and seeing what they  \\nbuy. If you do not operate such a site, you need to find more creative ways to get data.  \\nPage 66 Machine Learning Yearning-Draft Andrew Ng  \\n•Human intuition is harder to count on. \\u200b For example, pretty much no one can  \\npredict the stock market. So if our stock prediction algorithm does no better than random  \\nguessing, it is hard to figure out how to improve it.   \\n•It is hard to know what the optimal error rate and reasonable desired error  \\nrate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\n'),\n",
       " Document(page_content='rate is. \\u200bSuppose you already have a book recommendation system that is doing quite  \\nwell. How do you know how much more it can improve without a human baseline?   \\n \\n \\n \\n \\n \\xa0\\nPage 67 Machine Learning Yearning-Draft Andrew Ng  \\n34 How to define human-level performance   \\n \\nSuppose you are working on a medical imaging application that automatically makes  \\ndiagnoses from x-ray images. A typical person with no previous medical background besides  \\nsome basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\nexperienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\neach image achieves 2% error. Which one of these error rates defines “human-level  \\nperformance”?  \\nIn this case, I would use 2% as the human-level performance proxy for our optimal error  \\nrate. You can also set 2% as the desired performance level because all three reasons from the  \\nprevious chapter for comparing to human-level performance apply:   \\n•Ease of obtaining labeled data from human labelers. \\u200b You can get a team of doctors  \\nto provide labels to you with a 2% error rate.  \\n•Error analysis can draw on human intuition. \\u200bBy discussing images with a team of  \\ndoctors, you can draw on their intuitions.  \\n•Use human-level performance to estimate the optimal error rate and also set  \\nachievable “desired error rate.” \\u200b It is reasonable to use 2% error as our estimate of the  \\noptimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\nhigher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\nreasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\nestimates are necessarily too high.  \\nWhen it comes to obtaining labeled data, you might not want to discuss every image with an  \\nentire team of doctors since their time is expensive. Perhaps you can have a single junior  \\ndoctor label the vast majority of cases and bring only the harder cases to more experienced  \\ndoctors or to the team of doctors.   \\nIf your system is currently at 40% error, then it doesn’t matter much whether you use a  \\njunior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\nintuitions. But if your system is already at 10% error, then defining the human-level  \\nreference as 2% gives you better tools to keep improving your system.   \\n \\xa0\\nPage 68 Machine Learning Yearning-Draft Andrew Ng  \\n35 Surpassing human-level performance   \\n \\nYou are working on speech recognition and have a dataset of audio clips. Suppose your  \\ndataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\nsystem already achieves 8% error. Can you use any of the three techniques described in  \\nChapter 33 to continue making rapid progress?  \\nIf you can identify a subset of data in which humans significantly surpass your system, then  \\nyou can still use those techniques to drive rapid progress. For example, suppose your system  \\nis much better than people at recognizing speech in noisy audio, but humans are still better  \\nat transcribing very rapidly spoken speech.  \\nFor the subset of data with rapidly spoken speech:  \\n1.You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\noutput.  \\n2.You can draw on human intuition to understand why they correctly heard a rapidly  \\nspoken utterance when your system didn’t.  \\n3.You can use human-level performance on rapidly spoken speech as a desired performance  \\ntarget. \\nMore generally, so long as there are dev set examples where humans are right and your  \\nalgorithm is wrong, then many of the techniques described earlier will apply. This is true  \\neven if, averaged over the entire dev/test set, your performance is already surpassing  \\nhuman-level performance.  \\nThere are many important machine learning applications where machines surpass human  \\nlevel performance. For example, machines are better at predicting movie ratings, how long it  \\ntakes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\nsubset of techniques apply once humans have a hard time identifying examples that the  \\nalgorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\nwhere machines already surpass human-level performance, while progress is faster when  \\nmachines are still trying to catch up to humans.   \\n \\n  \\nPage 69 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTraining and  \\ntesting on different  \\ndistributions  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 70 Machine Learning Yearning-Draft Andrew Ng  \\n36 When you should train and test on  \\ndifferent distributions  \\n \\nUsers of'),\n",
       " Document(page_content=' progress is usually slower on problems  \\nwhere machines already surpass human-level performance, while progress is faster when  \\nmachines are still trying to catch up to humans.   \\n \\n  \\nPage 69 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTraining and  \\ntesting on different  \\ndistributions  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 70 Machine Learning Yearning-Draft Andrew Ng  \\n36 When you should train and test on  \\ndifferent distributions  \\n \\nUsers of your cat pictures app have uploaded 10,000 images, which you have manually  \\nlabeled as containing cats or not. You also have a larger set of 200,000 images that you  \\ndownloaded off the internet. How should you define train/dev/test sets?   \\nSince the 10,000 user images closely reflect the actual probability distribution of data you  \\nwant to do well on, you might use that for your dev and test sets. If you are training a  \\ndata-hungry deep learning algorithm, you might give it the additional 200,000 internet  \\nimages for training. Thus, your training and dev/test sets come from different probability  \\ndistributions. How does this affect your work?   \\nInstead of partitioning our data into train/dev/test sets, we could take all 210,000 images we  \\nhave, and randomly shuffle them into train/dev/test sets. In this case, all the data comes  \\nfrom the same distribution. But I recommend against this method, because about  \\n205,000/210,000 ≈ 97.6% of your dev/test data would come from internet images, which  \\ndoes not reflect the actual distribution you want to do well on. Remember our  \\nrecommendation on choosing dev/test sets:   \\nChoose dev and test sets to reflect data you expect to get in the future  \\nand want to do well on.   \\nMost of the academic literature on machine learning assumes that the training set, dev set  \\nand test set all come from the same distribution.  In the early days of machine learning, data  11\\nwas scarce. We usually only had one dataset drawn from some probability distribution. So  \\nwe would randomly split that data into train/dev/test sets, and the assumption that all the  \\ndata was coming from the same source was usually satisfied.   \\n11 There is some academic research on training and testing on differ ent distributions.  Examples  \\ninclude “domain adaptation, ” “tr ansfer lear ning” and “multitask  lear ning. ” But ther e is still a huge \\ngap between theory and practice.  If you tr ain on dataset A and test on some v ery differ ent type of data \\nB, luck could hav e a huge effect on how well  your  algor ithm performs.  (Her e, “luck ” includes the \\nresearcher’ s hand-designed features for  the particular task , as well as other factors that we just don’ t \\nunder stand yet. ) This mak es the academic study of training and testing on differ ent distributions  \\ndifficult to carr y out in a systematic way.   \\nPage 71 Machine Learning Yearning-Draft Andrew Ng  \\nBut in the era of big data, we now have access to huge training sets, such as cat internet  \\nimages. Even if the training set comes from a different distribution than the dev/test set, we  \\nstill want to use it for learning since it can provide a lot of information.  \\nFor the cat detector example, instead of putting all 10,000 user-uploaded images into the  \\ndev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining  \\n5,000 user-uploaded examples into the training set. This way, your training set of 205,000  \\nexamples contains some data that comes from your dev/test distribution along with the  \\n200,000 internet images. We will discuss in a later chapter why this method is helpful.   \\nLet’s consider a second example. Suppose you are building a speech recognition system to  \\ntranscribe street addresses for a voice-controlled mobile map/navigation app. You have  \\n20,000 examples of users speaking street addresses. But you also have 500,000 examples of  \\nother audio clips with users speaking about other topics. You might take 10,000 examples of  \\nstreet addresses for the dev/test sets, and use the remaining 10,000, plus the additional  \\n500,000 examples, for training.   \\nWe will continue to assume that your dev data and your test data come from the same  \\ndistribution. But it is important to understand that different training and dev/test  \\ndistributions offer some special challenges.   \\n  \\nPage 72 Machine Learning Yearning-Draft Andrew Ng  \\n37 How to decide whether to use all your data  \\n \\nSuppose your cat detector’s training set includes 10,000 user-uploaded images. This data  \\ncomes'),\n",
       " Document(page_content=' the additional  \\n500,000 examples, for training.   \\nWe will continue to assume that your dev data and your test data come from the same  \\ndistribution. But it is important to understand that different training and dev/test  \\ndistributions offer some special challenges.   \\n  \\nPage 72 Machine Learning Yearning-Draft Andrew Ng  \\n37 How to decide whether to use all your data  \\n \\nSuppose your cat detector’s training set includes 10,000 user-uploaded images. This data  \\ncomes from the same distribution as a separate dev/test set, and represents the distribution  \\nyou care about doing well on. You also have an additional 20,000 images downloaded from  \\nthe internet. Should you provide all 20,000+10,000=30,000 images to your learning \\nalgorithm as its training set, or discard the 20,000 internet images for fear of it biasing your  \\nlearning algorithm?   \\nWhen using earlier generations of learning algorithms (such as hand-designed computer  \\nvision features, followed by a simple linear classifier) there was a real risk that merging both  \\ntypes of data would cause you to perform worse. Thus, some engineers will warn you against  \\nincluding the 20,000 internet images. \\nBut in the modern era of powerful, flexible learning algorithms—such as large neural  \\nnetworks—this risk has greatly diminished. If you can afford to build a neural network with a  \\nlarge enough number of hidden units/layers, you can safely add the 20,000 images to your  \\ntraining set. Adding the images is more likely to increase your performance.   \\nThis observation relies on the fact that there is some x —> y mapping that works well for  \\nboth types of data. In other words, there exists some system that inputs either an internet  \\nimage or a mobile app image and reliably predicts the label, even without knowing the  \\nsource of the image.   \\nAdding the additional 20,000 images has the following effects:  \\n1.It gives your neural network more examples of what cats do/do not look like. This is  \\nhelpful, since internet images and user-uploaded mobile app images do share some  \\nsimilarities. Your neural network can apply some of the knowledge acquired from internet  \\nimages to mobile app images.   \\n2.It forces the neural network to expend some of its capacity to learn about properties that  \\nare specific to internet images (such as higher resolution, different distributions of how  \\nthe images are framed, etc.) If these properties differ greatly from mobile app images, it  \\nwill “use up” some of the representational capacity of the neural network. Thus there is  \\nless capacity for recognizing data drawn from the distribution of mobile app images,  \\nwhich is what you really care about. Theoretically, this could hurt your algorithms’  \\nperformance.  \\nPage 73 Machine Learning Yearning-Draft Andrew Ng  \\nTo describe the second effect in different terms, we can turn to the fictional character  \\nSherlock Holmes, who says that your brain is like an attic; it only has a finite amount of  \\nspace. He says that “for every addition of knowledge, you forget something that you knew  \\nbefore. It is of the highest importance, therefore, not to have useless facts elbowing out the  \\nuseful ones.”   12\\nFortunately, if you have the computational capacity needed to build a big enough neural  \\nnetwork—i.e., a big enough attic—then this is not a serious concern. You have enough  \\ncapacity to learn from both internet and from mobile app images, without the two types of  \\ndata competing for capacity. Your algorithm’s “brain” is big enough that you don’t have to  \\nworry about running out of attic space.   \\nBut if you do not have a big enough neural network (or another highly flexible learning  \\nalgorithm), then you should pay more attention to your training data matching your dev/test  \\nset distribution.  \\nIf you think you have data that has no benefit,you should just leave out that data for  \\ncomputational reasons. For example, suppose your dev/test sets contain mainly casual  \\npictures of people, places, landmarks, animals. Suppose you also have a large collection of  \\nscanned historical documents:   \\n \\n \\n \\n \\n \\n \\n \\nThese documents don’t contain anything resembling a cat. They also look completely unlike  \\nyour dev/test distribution. There is no point including this data as negative examples,  \\nbecause the benefit from the first effect above is negligible—there is almost nothing your  \\nneural network can learn from this data that it can apply to your dev/test set distribution.  \\nIncluding them would waste computation resources and representation capacity of the  \\nneural network. \\xa0\\n12 \\u200bA Study  in Sc arlet\\u200b \\u200bby Arthur Conan Doyle \\nPage 74 Machine Learning Yearning-Draft Andrew Ng \\n \\n38 How to decide whether to include  \\ninconsistent data  \\n \\nSuppose you want to learn to predict housing prices in'),\n",
       " Document(page_content=' above is negligible—there is almost nothing your  \\nneural network can learn from this data that it can apply to your dev/test set distribution.  \\nIncluding them would waste computation resources and representation capacity of the  \\nneural network. \\xa0\\n12 \\u200bA Study  in Sc arlet\\u200b \\u200bby Arthur Conan Doyle \\nPage 74 Machine Learning Yearning-Draft Andrew Ng \\n \\n38 How to decide whether to include  \\ninconsistent data  \\n \\nSuppose you want to learn to predict housing prices in New York City. Given the size of a  \\nhouse (input feature x), you want to predict the price (target label y).   \\nHousing prices in New York City are very high. Suppose you have a second dataset of  \\nhousing prices in Detroit, Michigan, where housing prices are much lower. Should you  \\ninclude this data in your training set?   \\nGiven the same size x, the price of a house y is very different depending on whether it is in  \\nNew York City or in Detroit. If you only care about predicting New York City housing prices,  \\nputting the two datasets together will hurt your performance.  In this case, it would be better  \\nto leave out the inconsistent Detroit data.  13\\nHow is this New York City vs. Detroit example different from the  mobile app vs. internet cat  \\nimages example?  \\nThe cat image example is different because, given an input picture x, one can reliably predict  \\nthe label y indicating whether there is a cat, even without knowing if the image is an internet  \\nimage or a mobile app image. I.e., there is a function f(x) that reliably maps from the input x  \\nto the target output y, even without knowing the origin of x. Thus, the task of recognition  \\nfrom internet images is “consistent” with the task of recognition from mobile app images.  \\nThis means there was little downside (other than computational cost) to including all the  \\ndata, and some possible significant upside. In contrast, New York City and Detroit, Michigan  \\ndata are not consistent. Given the same x (size of house), the price is very different  \\ndepending on where the house is.   \\n \\n \\n  \\n13 There is one way to address the problem of Detr oit data being inconsistent with New Yor k City  \\ndata,  which is to add an extra featur e to each training example indicating the city.  Given an input \\nx—which now specifies the city—the target v alue of y is now unambiguous.  Howev er, in practice I  do \\nnot see this done fr equently.   \\nPage 75 Machine Learning Yearning-Draft Andrew Ng  \\n39 Weighting data   \\n \\nSuppose you have 200,000 images from the internet and 5,000 images from your mobile  \\napp users. There is a 40:1 ratio between the size of these datasets. In theory, so long as you  \\nbuild a huge neural network and train it long enough on all 205,000 images, there is no  \\nharm in trying to make the algorithm do well on both internet images and mobile images.   \\nBut in practice, having 40x as many internet images as mobile app images might mean you  \\nneed to spend 40x (or more) as much computational resources to model both, compared to if  \\nyou trained on only the 5,000 images.   \\nIf you don’t have huge computational resources, you could  give the internet images a much  \\nlower weight as a compromise.   \\nFor example, suppose your optimization objective is squared error (This is not a good choice  \\nfor a classification task, but it will simplify our explanation.) Thus, our learning algorithm  \\ntries to optimize:  \\n  \\nThe first sum above is over the 5,000 mobile images, and the second sum is over the  \\n200,000 internet images. You can instead optimize with an additional parameter \\u200b𝛽\\u200b:  \\n \\n If you set \\u200b𝛽\\u200b=1/40, the algorithm would give equal weight to th e 5,000 mobile images and the  \\n200,000 internet images. You can also set the parameter \\u200b𝛽\\u200b to other values, perhaps by  \\ntuning to the dev set.   \\nBy weighting the additional Internet images less, you don’t have to build as massive a neural  \\nnetwork to make sure the algorithm does well on both types of tasks. This type of  \\nre-weighting is needed only when you suspect the additional data (Internet Images) has a  \\nvery different distribution than the dev/test set, or if the additional data is much larger than  \\nthe data that came from the same distribution as the dev/test set (mobile images).   \\nPage 76 Machine Learning Yearning-Draft Andrew Ng  \\n40 Generalizing from the training set to the  \\ndev set  \\n \\nSuppose you are applying ML in a setting where the training and the dev/test distributions'),\n",
       " Document(page_content='  \\nre-weighting is needed only when you suspect the additional data (Internet Images) has a  \\nvery different distribution than the dev/test set, or if the additional data is much larger than  \\nthe data that came from the same distribution as the dev/test set (mobile images).   \\nPage 76 Machine Learning Yearning-Draft Andrew Ng  \\n40 Generalizing from the training set to the  \\ndev set  \\n \\nSuppose you are applying ML in a setting where the training and the dev/test distributions  \\nare different. Say, the training set contains Internet images + Mobile images, and the  \\ndev/test sets contain only Mobile images. However, the algorithm is not working well: It has  \\na much higher dev/test set error than you would like. Here are some possibilities of what  \\nmight be wrong:  \\n1.It does not do well on the training set. This is the problem of high (avoidable) bias on the  \\ntraining set distribution.   \\n2.It does well on the training set, but does not generalize well to previously unseen data  \\ndrawn from the same distribution as the training set \\u200b. This is high variance.   \\n3.It generalizes well to new data drawn from the same distribution as the training set, but  \\nnot to data drawn from the dev/test set distribution. We call this problem \\u200bdata  \\nmismatch \\u200b, since it is because the training set data is a poor match for the dev/test set  \\ndata.   \\nFor example, suppose that humans achieve near perfect performance on the cat recognition  \\ntask. Your algorithm achieves this:  \\n•1% error on the training set  \\n•1.5% error on data drawn from the same distribution as the training set that the algorithm  \\nhas not seen  \\n•10% error on the dev set   \\nIn this case, you clearly have a data mismatch problem. To address this, you might try to  \\nmake the training data more similar to the dev/test data. We discuss some techniques for  \\nthis later.  \\nIn order to diagnose to what extent an algorithm suffers from each of the problems 1-3  \\nabove, it will be useful to have another dataset. Specifically, rather than giving the algorithm  \\nall the available training data, you can split it into two subsets: The actual training set which  \\nthe algorithm will train on, and a separate set, which we will call the “Training dev” set, that  \\nwe will not train on.   \\nYou now have four subsets of data:  \\nPage 77 Machine Learning Yearning-Draft Andrew Ng  \\n•Training set. This is the data that the algorithm will learn from (e.g., Internet images +  \\nMobile images). This does not have to be drawn from the same distribution as what we  \\nreally care about (the dev/test set distribution).  \\n•Training dev set: This data is drawn from the same distribution as the training set (e.g.,  \\nInternet images + Mobile images). This is usually smaller than the training set; it only  \\nneeds to be large enough to evaluate and track the progress of our learning algorithm.   \\n•Dev set: This is drawn from the same distribution as the test set, and it reflects the  \\ndistribution of data that we ultimately care about doing well on. (E.g., mobile images.)   \\n•Test set: This is drawn from the same distribution as the dev set. (E.g., mobile images.)  \\nArmed with these four separate datasets, you can now evaluate:  \\n•Training error, by evaluating on the training set.   \\n•The algorithm’s ability to generalize to new data drawn from the training set distribution,  \\nby evaluating on the training dev set.  \\n•The algorithm’s performance on the task you care about, by evaluating on the dev and/or  \\ntest sets.   \\nMost of the guidelines in Chapters 5-7 for picking the size of the dev set also apply to the  \\ntraining dev set.  \\n \\n \\n \\n \\n \\n \\n  \\nPage 78 Machine Learning Yearning-Draft Andrew Ng  \\n41 Identifying Bias, Variance, and Data  \\nMismatch Errors  \\n \\nSuppose humans achieve almost perfect performance (≈0% error) on the cat detection task,  \\nand thus the optimal error rate is about 0%. Suppose you have:  \\n•1% error on the training set. \\n•5% error on training dev set.   \\n•5% error on the dev set.   \\nWhat does this tell you? Here, you know that you have high variance. The variance reduction  \\ntechniques described earlier should allow you to make progress.   \\nNow, suppose your algorithm achieves:   \\n•10% error on the training set.  \\n•11% error on training dev set.  \\n•12% error on the dev set.   \\nThis tells you that you have high avoidable bias on the training set. I.e., the algorithm is  \\ndoing poorly on the training set. Bias reduction techniques should help.  \\nIn the two examples above'),\n",
       " Document(page_content=\" you have high variance. The variance reduction  \\ntechniques described earlier should allow you to make progress.   \\nNow, suppose your algorithm achieves:   \\n•10% error on the training set.  \\n•11% error on training dev set.  \\n•12% error on the dev set.   \\nThis tells you that you have high avoidable bias on the training set. I.e., the algorithm is  \\ndoing poorly on the training set. Bias reduction techniques should help.  \\nIn the two examples above, the algorithm suffered from only high avoidable bias or high  \\nvariance. It is possible for an algorithm to suffer from any subset of high avoidable bias, high  \\nvariance, and data mismatch. For example:   \\n•10% error on the training set.   \\n•11% error on training dev set.  \\n•20% error on the dev set.   \\nThis algorithm suffers from high avoidable bias and from data mismatch. It does not,  \\nhowever, suffer from high variance on the training set distribution.   \\nIt might be easier to understand how the different types of errors relate to each other by  \\ndrawing them as entries in a table:   \\nPage 79 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\nContinuing with the example of th \\u200be cat image detector, you can see that there are two  \\ndifferent distributions of data on the x-axis. On the y-axis, we ha \\u200bve three types of error:  \\nhuman level error, error on examples the algorithm has trained on, and error on examples  \\nthe algorithm has not trained on. We can fill in the boxes with the different types of errors we  \\nidentified in the previous chapter.   \\nIf you wish, you can also fill in the remaining two boxes in this table: You can fill in the  \\nupper-right box (Human level performance on Mobile Images) by asking some humans to  \\nlabel your mobile cat images data and measure their error. You can fill in the next box by  \\ntaking the mobile cat images (Distribution B) and putting a small fraction of into the training  \\nset so that the neural network learns on it too. Then you measure the learned model’s error  \\non that subset of data. Filling in these two additional entries may sometimes give additional  \\ninsight about what the algorithm is doing on the two different distributions (Distribution A  \\nand B) of data.  \\nBy understanding which types of error the algorithm suffers from the most, you will be better  \\npositioned to decide whether to focus on reducing bias, reducing variance, or reducing data  \\nmismatch.  \\n  \\nPage 80 Machine Learning Yearning-Draft Andrew Ng  \\n42 Addressing data mismatch  \\n \\nSuppose you have developed a speech recognition system that does very well on the training  \\nset and on the training dev set. However, it does poorly on your dev set: You have a data  \\nmismatch problem. What can you do?  \\nI recommend that you: (i) Try to understand what properties of the data differ between the  \\ntraining and the dev set distributions. (ii) Try to find more training data that better matches  \\nthe dev set examples that your algorithm has trouble with.  14\\nFor example, suppose you carry out an error analysis on the speech recognition dev set: You  \\nmanually go through 100 examples, and try to understand where the algorithm is making  \\nmistakes. You find that your system does poorly because most of the audio clips in the dev  \\nset are taken within a car, whereas most of the training examples were recorded against a  \\nquiet background. The engine and road noise dramatically worsen the performance of your  \\nspeech system. In this case, you might try to acquire more training data comprising audio  \\nclips that were taken in a car. The purpose of the error analysis is to understand the  \\nsignificant differences between the training and the dev set, which is what leads to the data  \\nmismatch.  \\nIf your training and training dev sets include audio recorded within a car, you should also  \\ndouble-check your system’s performance on this subset of data. If it is doing well on the car  \\ndata in the training set but not on car data in the training dev set, then this further validates  \\nthe hypothesis that getting more car data would help. This is why we discussed the  \\npossibility of including in your training set some data drawn from the same distribution as  \\nyour dev/test set in the previous chapter. Doing so allows you to compare your performance  \\non the car data in the training set vs. the dev/test set.   \\nUnfortunately, there are no guarantees in this process. For example, if you don't have any  \\nway to get more training data that better match the dev set data, you might not have a clear  \\npath towards improving performance.  \\n \\n \\xa0\\n14There is also some research on “domain adaptation”—how to tr ain an algorithm on one distribution  \\nand hav e it gener alize\"),\n",
       " Document(page_content=\" you to compare your performance  \\non the car data in the training set vs. the dev/test set.   \\nUnfortunately, there are no guarantees in this process. For example, if you don't have any  \\nway to get more training data that better match the dev set data, you might not have a clear  \\npath towards improving performance.  \\n \\n \\xa0\\n14There is also some research on “domain adaptation”—how to tr ain an algorithm on one distribution  \\nand hav e it gener alize to a differ ent distribution.  These methods ar e typically applicable only in  \\nspecial types of problems and are much less widely used than the ideas descr ibed in this chapter .  \\nPage 81 Machine Learning Yearning-Draft Andrew Ng  \\n43 Artificial data synthesis  \\n \\nYour speech system needs more data that sounds as if it were taken from within a car. Rather  \\nthan collecting a lot of data while driving around, there might be an easier way to get this  \\ndata: By artificially synthesizing it.  \\nSuppose you obtain a large quantity of car/road noise audio clips. You can download this  \\ndata from several websites. Suppose you also have a large training set of people speaking in a  \\nquiet room. If you take an audio clip of a person speaking and “add” to that to an audio clip  \\nof car/road noise, you will obtain an audio clip that sounds as if that person was speaking in  \\na noisy car. Using this process, you can “synthesize” huge amounts of data that sound as if it  \\nwere collected inside a car.   \\nMore generally, there are several circumstances where artificial data synthesis allows you to  \\ncreate a huge dataset that reasonably matches the dev set. Let’s use the cat image detector as  \\na second example. You notice that dev set images have much more motion blur because they  \\ntend to come from cellphone users who are moving their phone slightly while taking the  \\npicture. You can take non-blurry images from the training set of internet images, and add  \\nsimulated motion blur to them, thus making them more similar to the dev set.   \\nKeep in mind that artificial data synthesis has its challenges: it is sometimes easier to create  \\nsynthetic data that appears realistic to a person than it is to create data that appears realistic  \\nto a computer. For example, suppose you have 1,000 hours of speech training data, but only  \\n1 hour of car noise. If you repeatedly use the same 1 hour of car noise with different portions  \\nfrom the original 1,000 hours of training data, you will end up with a synthetic dataset where  \\nthe same car noise is repeated over and over. While a person listening to this audio probably  \\nwould not be able to tell—all car noise sounds the same to most of us—it is possible that a  \\nlearning algorithm would “overfit” to the 1 hour of car noise. Thus, it could generalize poorly  \\nto a new audio clip where the car noise happens to sound different.   \\nAlternatively, suppose you have 1,000 unique hours of car noise, but all of it was taken from  \\njust 10 different cars. In this case, it is possible for an algorithm to “overfit” to these 10 cars  \\nand perform poorly if tested on audio from a different car. Unfortunately, these problems  \\ncan be hard to spot.   \\n \\n \\n \\nPage 82 Machine Learning Yearning-Draft Andrew Ng  \\n \\nTo take one more example, suppose you are building a computer vision system to recognize  \\ncars. Suppose you partner with a video gaming company, which has computer graphics  \\nmodels of several cars. To train your algorithm, you use the models to generate synthetic  \\nimages of cars. Even if the synthesized images look very realistic, this approach (which has  \\nbeen independently proposed by many people) will probably not work well. The video game  \\nmight have ~20 car designs in the entire video game. It is very expensive to build a 3D car  \\nmodel of a car; if you were playing the game, you probably wouldn’t notice that you’re seeing  \\nthe same cars over and over, perhaps only painted differently. I.e., this data looks very  \\nrealistic to you. But compared to the set of all cars out on roads—and therefore what you’re  \\nlikely to see in the dev/test sets—this set of 20 synthesized cars captures only a minuscule  \\nfraction of the world’s distribution of cars. Thus if your 100,000 training examples all come  \\nfrom these 20 cars, your system will “overfit” to these 20 specific car designs, and it will fail  \\nto generalize well to dev/test sets that include other car designs.   \\nWhen synthesizing data, put some thought into whether you’re really synthesizing a  \\nrepresentative set of examples. Try to avoid giving the synthesized data properties that  \\nmakes it possible for a learning algorithm to distinguish\"),\n",
       " Document(page_content=' world’s distribution of cars. Thus if your 100,000 training examples all come  \\nfrom these 20 cars, your system will “overfit” to these 20 specific car designs, and it will fail  \\nto generalize well to dev/test sets that include other car designs.   \\nWhen synthesizing data, put some thought into whether you’re really synthesizing a  \\nrepresentative set of examples. Try to avoid giving the synthesized data properties that  \\nmakes it possible for a learning algorithm to distinguish synthesized from non-synthesized  \\nexamples—such as if all the synthesized data comes from one of 20 car designs, or all the  \\nsynthesized audio comes from only 1 hour of car noise. This advice can be hard to follow.   \\nWhen working on data synthesis, my teams have sometimes taken weeks before we produced  \\ndata with details that are close enough to the actual distribution for the synthesized data to  \\nhave a significant effect. But if you are able to get the details right, you can suddenly access a  \\nfar larger training set than before.   \\n \\n \\nPage 83 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n\\xa0\\n\\xa0\\n\\xa0\\nDebugging  \\ninference  \\nalgorithms \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 84 Machine Learning Yearning-Draft Andrew Ng  \\n44 The Optimization Verification test  \\n \\nSuppose you are building a speech recognition system. Your system works by inputting an  \\naudio clip \\u200bA\\u200b, and computing some Score \\u200bA\\u200b(\\u200bS\\u200b) for each possible  output sentence \\u200bS\\u200b. For  \\nexample, you might try to estimate Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b), the pro bability that the correct  \\noutput transcription is the sentence \\u200bS\\u200b,  given that the input audio was \\u200bA.   \\nGiven a way to compute Score \\u200bA\\u200b(\\u200bS\\u200b), you still have to find the En glish sentence \\u200bS\\u200b that \\nmaximizes it:  \\n \\nHow do you compute the “arg max” above? If the English language has 50,000 words, then  \\nthere are (50,000)\\u200bN \\u200bpossi ble sentences of length \\u200bN\\u200b—far too many to exhaustively enumerate.  \\nSo, you need to apply an approximate search algorithm, to try to find the value of \\u200bS\\u200b that  \\noptimizes (maximizes) Score \\u200bA\\u200b(\\u200bS\\u200b). One example search algorith m is “beam search,” which  \\nkeeps only \\u200bK\\u200b top candidates during the search process. (For the purposes of this chapter, you  \\ndon’t need to understand the details of beam search.) Algorithms like this are not guaranteed  \\nto find the value of \\u200bS \\u200bthat maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nSuppose that an audio clip \\u200bA\\u200b records someone saying “I love machine learning.” But instead  \\nof outputting the correct transcription, your system outputs the incorrect “I love robots.”  \\nThere are now two possibilities for what went wrong:   \\n1.Search algorithm problem \\u200b. The approximate search algorithm (beam search) failed  \\nto find the value of \\u200bS\\u200b that maximizes Score \\u200bA\\u200b(\\u200bS\\u200b).  \\n2.Objective (scoring function) problem. \\u200b Our estimates for Score \\u200bA\\u200b(\\u200bS\\u200b) = P(\\u200bS\\u200b|\\u200bA\\u200b) were \\ninaccurate. In particular, our choice of Score \\u200bA\\u200b(\\u200bS\\u200b) failed to r ecognize that “I love machine  \\nlearning” is the correct transcription.   \\nDepending on which of these was the cause of the failure, you should prioritize your efforts  \\nvery differently. If #1 was the problem, you should work on improving the search algorithm.  \\nIf #2 was the problem, you should work on the learning algorithm that estimates Score \\u200bA\\u200b(\\u200bS\\u200b).  \\nFacing this situation, some researchers will randomly decide to work on the search  \\nalgorithm; others will randomly work on a better way to learn values for Score \\u200bA\\u200b(S). But \\nunless you know which of these is the underlying cause of the error, your efforts could be  \\nwasted. How can you decide more systematically what to work on?   \\nPage 85 Machine Learning Yearning-Draft Andrew Ng  \\nLet S\\u200bout\\u200b be the output tran scription (“I love robots”). Let S* be the correct transcription (“I  \\nlove machine learning”). In order to understand whether #1 or #2 above is the problem, you  \\ncan perform the \\u200bOptimization Verification test \\u200b: First, compute Score \\u200bA\\u200b(\\u200bS\\u200b*) and \\nScore\\u200bA\\u200b(\\u200bS\\u200bout\\u200b). Then check w hether Score \\u200bA\\u200b(\\u200bS\\u200b*) > Score \\u200bA\\u200b(\\u200bS\\u200bout\\u200b). There are two possibilities:   \\nCase 1'),\n",
       " Document(page_content=' S* be the correct transcription (“I  \\nlove machine learning”). In order to understand whether #1 or #2 above is the problem, you  \\ncan perform the \\u200bOptimization Verification test \\u200b: First, compute Score \\u200bA\\u200b(\\u200bS\\u200b*) and \\nScore\\u200bA\\u200b(\\u200bS\\u200bout\\u200b). Then check w hether Score \\u200bA\\u200b(\\u200bS\\u200b*) > Score \\u200bA\\u200b(\\u200bS\\u200bout\\u200b). There are two possibilities:   \\nCase 1: Score\\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, your learning algorithm has correctly given S* a higher score than S \\u200bout\\u200b. \\nNevertheless, our approximate search algorithm chose S \\u200bout \\u200brather than S*. This tells you that  \\nyour approximate search algorithm is failing to choose the value of S that maximizes  \\nScore\\u200bA\\u200b(\\u200bS\\u200b). In this case, the Optimization Verification test tells  you that you have a search  \\nalgorithm problem and should focus on that. For example, you could try increasing the beam  \\nwidth of beam search.   \\nCase 2: Score\\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)  \\nIn this case, you know that the way you’re computing Score \\u200bA\\u200b(.) is at fault: It is failing to give a  \\nstrictly higher score to the correct output \\u200bS\\u200b* than the incorrect \\u200bS\\u200bout\\u200b. The Optimization \\nVerification test tells you that you have an objective (scoring) function problem. Thus, you  \\nshould focus on improving how you learn or approximate Score \\u200bA\\u200b(\\u200bS\\u200b) for different sentences \\u200bS\\u200b.  \\nOur discussion has focused on a single example. To apply the Optimization Verification test  \\nin practice, you should examine the errors in your dev set. For each error, you would test  \\nwhether Score \\u200bA\\u200b(S*) > Score \\u200bA\\u200b(S\\u200bout\\u200b). Each dev example for which this inequality holds will get  \\nmarked as an error caused by the optimization algorithm. Each example for which this does  \\nnot hold (Score \\u200bA\\u200b(S*) ≤ Score \\u200bA\\u200b(S\\u200bout\\u200b)) gets counted as a mistake due to the way you’re  \\ncomputing Score \\u200bA\\u200b(.).  \\nFor example, suppose you find that 95% of the errors were due to the scoring function  \\nScore\\u200bA\\u200b(.), and only 5% due to the optimization algorithm. Now  you know that no matter how  \\nmuch you improve your optimization procedure, you would realistically eliminate only ~5%  \\nof our errors. Thus, you should instead focus on improving how you estimate Score \\u200bA\\u200b(.).  \\n \\n \\n \\n \\xa0\\nPage 86 Machine Learning Yearning-Draft Andrew Ng  \\n45 General form of Optimization Verification  \\ntest \\n \\nYou can apply the Optimization Verification test when, given some input \\u200bx\\u200b, you know how  to  \\ncompute Score \\u200bx\\u200b(\\u200by\\u200b) that indicates how good a response \\u200by\\u200b is to a n input \\u200bx\\u200b. Furthermore, you \\nare using an approximate algorithm to try to find arg max \\u200by\\u200b Score\\u200bx\\u200b(\\u200by\\u200b), but suspect that the  \\nsearch algorithm is sometimes failing to find the maximum. In our previous speech  \\nrecognition example, \\u200bx=A\\u200b was an audio clip, and \\u200by=S\\u200b was the output transcript.   \\nSuppose y* is the “correct” output but the algorithm instead outputs y \\u200bout\\u200b. Then the key test is  \\nto measure whether Score \\u200bx\\u200b(y*) > Score \\u200bx\\u200b(y\\u200bout\\u200b). If this inequality holds, then we blame the  \\noptimization algorithm for the mistake. Refer to the previous chapter to make sure you  \\nunderstand the logic behind this. Otherwise, we blame the computation of Score \\u200bx\\u200b(y).  \\nLet’s look at one more example. Suppose you are building a Chinese-to-English machine  \\ntranslation system. Your system works by inputting a Chinese sentence \\u200bC\\u200b, and computing \\nsome Score \\u200bC\\u200b(\\u200bE\\u200b) for each p ossible translation \\u200bE\\u200b. For example, you might use Score \\u200bC\\u200b(\\u200bE\\u200b) = \\nP(\\u200bE\\u200b|\\u200bC\\u200b), the probability of the translation being E given that the input sentence was \\u200bC\\u200b.  \\nYour algorithm translates sentences by trying to compute:   \\n \\nHowever, the set of all possible English sentences \\u200bE \\u200bis too large, so you rely on a heuristic  \\nsearch algorithm.  \\nSuppose your algorithm outputs an incorrect translation \\u200bE\\u200bout\\u200b rather than some correct  \\ntranslation \\u200bE \\u200b*. Then the Optimization Verification test would ask you to compute whether  \\nScore\\u200bC\\u200b(\\u200bE*\\u200b) > Score \\u200bC\\u200b(\\u200bE\\u200bout\\u200b). If'),\n",
       " Document(page_content='C\\u200b.  \\nYour algorithm translates sentences by trying to compute:   \\n \\nHowever, the set of all possible English sentences \\u200bE \\u200bis too large, so you rely on a heuristic  \\nsearch algorithm.  \\nSuppose your algorithm outputs an incorrect translation \\u200bE\\u200bout\\u200b rather than some correct  \\ntranslation \\u200bE \\u200b*. Then the Optimization Verification test would ask you to compute whether  \\nScore\\u200bC\\u200b(\\u200bE*\\u200b) > Score \\u200bC\\u200b(\\u200bE\\u200bout\\u200b). If this inequality holds, then the Score \\u200bC\\u200b(.) correctly recognized E*  \\nas a superior output to \\u200bE\\u200bout\\u200b; thus, you would attribute this error to the approximate search  \\nalgorithm. Otherwise, you attribute this error to the computation of Score \\u200bC\\u200b(.).  \\nIt is a very common “design pattern” in AI to first learn an approximate scoring function  \\nScore\\u200bx\\u200b(.), then use an approximate maximization algorithm. If you are able to spot this  \\npattern, you will be able to use the Optimization Verification test to understand your source  \\nof errors.  \\n  \\nPage 87 Machine Learning Yearning-Draft Andrew Ng  \\n46 Reinforcement learning example  \\n \\nSuppose you are using machine learning to teach a helicopter to fly complex maneuvers.  \\nHere is a time-lapse photo of a computer-controller helicopter executing a landing with the  \\nengine turned off.  \\nThis is called an “autorotation” maneuver. It allows helicopters to land even if their engine  \\nunexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal  \\nis to use a learning algorithm to fly the helicopter through a trajectory \\u200bT \\u200bthat ends in a safe  \\nlanding.   \\nTo apply reinforcement learning, you have to develop a “Reward function” \\u200bR\\u200b(.) that gives a  \\nscore measuring how good each possible trajectory \\u200bT\\u200b is. For example, if \\u200bT \\u200bresults in the  \\nhelicopter crashing, then perhaps the reward is \\u200bR(T)\\u200b = -1,000—a huge negative reward. A \\ntrajectory \\u200bT\\u200b resulting in a safe landing might result in a positive \\u200bR(T) \\u200bwith the exact value  \\ndepending on how smooth the landing was. The reward function \\u200bR\\u200b(.) is typically chosen by  \\nhand to quantify how desirable different trajectories \\u200bT\\u200b are. It has to trade off how bumpy the  \\nlanding was, whether the helicopter landed in exactly the desired spot, how rough the ride  \\ndown was for passengers, and so on. It is not easy to design good reward functions.   \\nPage 88 Machine Learning Yearning-Draft Andrew Ng \\n \\nGiven a reward function \\u200bR(T), \\u200bthe job of the reinforcement learning algorithm is to control  \\nthe helicopter so that it  achieves max \\u200bT\\u200b R(T). \\u200bHowever, reinforcement learning algorithms  \\nmake many approximations and may not succeed in achieving this maximization.   \\nSuppose you have picked some reward \\u200bR(.)\\u200b and have run your learning algorithm. However, \\nits performance appears far worse than your human pilot—the landings are bumpier and  \\nseem less safe than what a human pilot achieves. How can you tell if the fault is with the  \\nreinforcement learning algorithm—which is trying to carry out a trajectory that achieves  \\nmax\\u200bT\\u200b \\u200bR(T)\\u200b—or if the fault  is with the reward function—which is trying to measure as well as  \\nspecify the ideal tradeoff between ride bumpiness and accuracy of landing spot?   \\nTo apply the Optimization Verification test, let \\u200bT\\u200bhum an\\u200b be the trajectory achieved by the  \\nhuman pilot, and let \\u200bT\\u200bout \\u200bbe the trajectory achieved by the algorithm. According to our  \\ndescription above, \\u200bT\\u200bhum an \\u200bis a superior trajectory to \\u200bT\\u200bout\\u200b. Thus, the key test is the following:  \\nDoes it hold true that \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) > \\u200bR\\u200b(\\u200bT\\u200bout\\u200b)?  \\nCase 1: If this inequality holds, then the reward function \\u200bR\\u200b(.) is correctly rating \\u200bT\\u200bhum an \\u200bas \\nsuperior to \\u200bT\\u200bout\\u200b. But our reinforcement learning algorithm is finding the inferior \\u200bT\\u200bout. \\u200bThis  \\nsuggests that working on improving our reinforcement learning algorithm is worthwhile.   \\nCase 2: The inequality does not hold: \\u200bR\\u200b(\\u200bT\\u200bhum an\\u200b) ≤ \\u200bR\\u200b(\\u200bT\\u200bout\\u200b). This means \\u200bR \\u200b(.) assigns a worse  \\nscore to \\u200bT\\u200bhum an \\u200beven though it is the superior trajectory. You sh ould work on improving \\u200bR \\u200b(.) to \\nbetter capture the tradeoffs that correspond to a good landing.   \\nMany machine learning applications have this “pattern” of optimizing an approximate  \\nscoring function Score \\u200bx\\u200b(.) using an approximate search algorit hm. Sometimes, there is no  \\nspecified input \\u200bx'),\n",
       " Document(page_content='\\u200b(\\u200bT\\u200bout\\u200b). This means \\u200bR \\u200b(.) assigns a worse  \\nscore to \\u200bT\\u200bhum an \\u200beven though it is the superior trajectory. You sh ould work on improving \\u200bR \\u200b(.) to \\nbetter capture the tradeoffs that correspond to a good landing.   \\nMany machine learning applications have this “pattern” of optimizing an approximate  \\nscoring function Score \\u200bx\\u200b(.) using an approximate search algorit hm. Sometimes, there is no  \\nspecified input \\u200bx\\u200b, so this reduces to just Score(.). In our example above, the scoring function  \\nwas the reward function Score( \\u200bT\\u200b)=R(\\u200bT\\u200b), and the optimization algorithm was the  \\nreinforcement learning algorithm trying to execute a good trajectory \\u200bT\\u200b.  \\nOne difference between this and earlier examples is that, rather than comparing to an  \\n“optimal” output, you were instead comparing to human-level performance \\u200bT\\u200bhum an\\u200b.We \\nassumed \\u200bT\\u200bhum an\\u200b is pretty good, even if not optimal. In general, so long as you have some y* (in  \\nthis example, \\u200bT\\u200bhum an\\u200b) that is a superior output to the performance of your current learning  \\nalgorithm—even if it is not the “optimal” output—then the Optimization Verification test can  \\nindicate whether it is more promising to improve the optimization algorithm or  the scoring  \\nfunction.  \\n \\n\\xa0\\nPage 89 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nEnd-to-end  \\ndeep learning  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 90 Machine Learning Yearning-Draft Andrew Ng  \\n47 The rise of end-to-end learning  \\n \\nSuppose you want to build a system to examine online product reviews and automatically tell  \\nyou if the writer liked or disliked that product. For example, you hope to recognize the  \\nfollowing review as highly positive:   \\nThis is a great mop!   \\nand the following as highly negative:  \\nThis mop is low quality--I regret buying it.   \\nThe problem of recognizing positive vs. negative opinions is called “sentiment classification.”  \\nTo build this system, you might build a “pipeline” of two components:  \\n1.Parser: A system that annotates the text with information identifying the most  \\nimportant words.  For example, you might use the parser to label all the adjectives  15\\nand nouns. You would therefore get the following annotated text:   \\nThis is a great \\u200bAdjectiv e\\u200b mop\\u200bNoun\\u200b! \\n2.Sentiment classifier: A learning algorithm that takes as input the annotated text and  \\npredicts the overall sentiment. The parser’s annotation could help this learning  \\nalgorithm greatly: By giving adjectives a higher weight, your algorithm will be able to  \\nquickly hone in on the important words such as “great,” and ignore less important  \\nwords such as “this.”   \\nWe can visualize your “pipeline” of two components as follows:   \\n \\n \\n  \\nThere has been a recent trend toward replacing pipeline systems with a single learning  \\nalgorithm. An \\u200bend-to-end learning algorithm \\u200b for this task would simply take as input  \\nthe raw, original text “This is a great mop!”, and try to directly recognize the sentiment:   \\n \\n15 A parser giv es a much r icher annotation of the text than this,  but this simplified descr iption will  \\nsuffice for explaining end-to-end deep lear ning.   \\nPage 91 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\n \\n \\n \\nNeural networks are commonly used in end-to-end learning systems. The term “end-to-end”  \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to  \\nthe desired output. I.e., the learning algorithm directly connects the “input end” of the  \\nsystem to the “output end.”  \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same'),\n",
       " Document(page_content=' abundant, end-to-end systems have been remarkably successful.  \\nBut they are not always a good choice. The next few chapters will give more examples of  \\nend-to-end systems as well as give advice on when you should and should not use them.  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nPage 92 Machine Learning Yearning-Draft Andrew Ng \\n \\n48 More end-to-end learning examples   \\n \\nSuppose you want to build a speech recognition system. You might build a system with three  \\ncomponents:  \\n \\n \\n \\nThe components work as follows:   \\n1.Compute features: Extract hand-designed features, such as MFCC ( \\u200bMel-frequency  \\ncepstrum coefficients) features, \\u200bwhich try to capture the content of an utterance while  \\ndisregarding less relevant properties, such as the speaker’s pitch.   \\n2.Phoneme recognizer: Some linguists believe that there are basic units of sound called  \\n“phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c”  \\nsound in “cake.” This system tries to recognize the phonemes in the audio clip.  \\n3.Final recognizer: Take the sequence of recognized phonemes, and try to string them  \\ntogether into an output transcript.   \\nIn contrast, an end-to-end system might input an audio clip, and try to directly output the  \\ntranscript:   \\n \\n \\n \\nSo far, we have only described machine learning “pipelines” that are completely linear: the  \\noutput is sequentially passed from one staged to the next. Pipelines can be more complex.  \\nFor example, here is a simple architecture for an autonomous car:   \\n \\n \\n \\n \\nPage 93 Machine Learning Yearning-Draft Andrew Ng \\n \\n \\nIt has three components: One detects other cars using the camera images; one detects  \\npedestrians; then a final component plans a path for our own car that avoids the cars and  \\npedestrians.   \\nNot every component in a pipeline has to be learned. For example, the literature on “robot  \\nmotion planning” has numerous algorithms for the final path planning step for the car. Many  \\nof these algorithms do not involve learning.   \\nIn contrast, and end-to-end approach might try to take in the sensor inputs and directly  \\noutput the steering direction:   \\n \\n \\n \\nEven though end-to-end learning has seen many successes, it is not always the best  \\napproach. For example, end-to-end speech recognition works well. But I’m skeptical about  \\nend-to-end learning for autonomous driving. The next few chapters explain why.   \\n  \\nPage 94 Machine Learning Yearning-Draft Andrew Ng \\n \\n49 Pros and cons of end-to-end learning   \\n \\nConsider the same speech pipeline from our earlier example:  \\nMany parts of this pipeline were “hand-engineered”:  \\n•MFCCs are a set of hand-designed audio features. Although they provide a reasonable  \\nsummary of the audio input, they also simplify the input signal by throwing some  \\ninformation away.  \\n•Phonemes are an invention of linguists. They are an imperfect representation of speech  \\nsounds. To the extent that phonemes are a poor approximation of reality, forcing an  \\nalgorithm to use a phoneme representation will limit the speech system’s performance.  \\nThese hand-engineered components limit the potential performance of the speech system.  \\nHowever, allowing hand-engineered components also has some advantages:  \\n•The MFCC features are robust to some properties of speech that do not affect the content,  \\nsuch as speaker pitch. Thus, they help simplify the problem for the learning algorithm.  \\n•To the extent that phonemes are a reasonable representation of speech, they can also help  \\nthe learning algorithm understand basic sound components and therefore improve its  \\nperformance. \\nHaving more hand-engineered components generally allows a speech system to learn with  \\nless data. The hand-engineered knowledge captured by MFCCs and phonemes  \\n“supplements” the knowledge our algorithm acquires from data. When we don’t have much  \\ndata, this knowledge is useful.   \\nNow, consider the end-to-end system:   \\n \\n \\n \\nPage 95 Machine Learning Yearning-Draft Andrew Ng \\n \\nThis system lacks the hand-engineered knowledge. Thus, when the training set is small, it  \\nmight do worse than the hand-engineered pipeline.   \\nHowever, when the training set is large, then it is not hampered by the limitations of an  \\nMFCC or phoneme-based representation. If the learning algorithm is a large-enough neural  \\nnetwork and if it is trained with enough training data, it has the potential to do very well, and  \\nperhaps even approach the optimal error rate.   \\nEnd-to-end learning systems tend to do well when there is a lot of labeled data for “both  \\nends”—the input end and the output end. In this example, we require a large dataset of  \\n(audio, transcript) pairs. When this type of data is not'),\n",
       " Document(page_content='-based representation. If the learning algorithm is a large-enough neural  \\nnetwork and if it is trained with enough training data, it has the potential to do very well, and  \\nperhaps even approach the optimal error rate.   \\nEnd-to-end learning systems tend to do well when there is a lot of labeled data for “both  \\nends”—the input end and the output end. In this example, we require a large dataset of  \\n(audio, transcript) pairs. When this type of data is not available, approach end-to-end  \\nlearning with great caution.   \\nIf you are working on a machine learning problem where the training set is very small, most  \\nof your algorithm’s knowledge will have to come from your human insight. I.e., from your  \\n“hand engineering” components.   \\nIf you choose not to use an end-to-end system, you will have to decide what are the steps in  \\nyour pipeline, and how they should plug together. In the next few chapters, we’ll give some  \\nsuggestions for designing such pipelines.   \\n \\n \\n \\n  \\nPage 96 Machine Learning Yearning-Draft Andrew Ng  \\n50 Choosing pipeline components: Data  \\navailability \\n \\nWhen building a non-end-to-end pipeline system, what are good candidates for the  \\ncomponents of the pipeline? How you design the pipeline will greatly impact the overall  \\nsystem’s performance. One important factor is whether you can easily collect data to train  \\neach of the components.   \\n \\nFor example, consider this autonomous driving architecture:  \\n \\n \\n \\n \\n \\nYou can use machine learning to detect cars and pedestrians. Further, it is not hard to obtain  \\ndata for these: There are numerous computer vision datasets with large numbers of labeled  \\ncars and pedestrians. You can also use crowdsourcing (such as Amazon Mechanical Turk) to  \\nobtain even larger datasets. It is thus relatively easy to obtain training data to build a car  \\ndetector and a pedestrian detector.   \\nIn contrast, consider a pure end-to-end approach:   \\n \\n \\n \\nTo train this system, we would need a large dataset of (Image, Steering Direction) pairs. It is  \\nvery time-consuming and expensive to have people drive cars around and record their  \\nsteering direction to collect such data. You need a fleet of specially-instrumented cars, and a  \\nhuge amount of driving to cover a wide range of possible scenarios. This makes an  \\nend-to-end system difficult to train. It is much easier to obtain a large dataset of labeled car  \\nor pedestrian images.   \\nMore generally, if there is a lot of data available for training “intermediate modules” of a  \\npipeline (such as a car detector or a pedestrian detector), then you might consider using a  \\nPage 97 Machine Learning Yearning-Draft Andrew Ng \\n \\npipeline with multiple stages. This structure could be superior because you could use all that  \\navailable data to train the intermediate modules.   \\nUntil more end-to-end data becomes available, I believe the non-end-to-end approach is  \\nsignificantly more promising for autonomous driving: Its architecture better matches the  \\navailability of data.  \\n \\n \\n \\n  \\nPage 98 Machine Learning Yearning-Draft Andrew Ng  \\n51 Choosing pipeline components: Task  \\nsimplicity   \\n \\nOther than data availability, you should also consider a second factor when picking  \\ncomponents of a pipeline: How simple are the tasks solved by the individual components?  \\nYou should try to choose pipeline components that are individually easy to build or learn.  \\nBut what does it mean for a component to be “easy” to learn?   \\n \\nConsider these machine learning tasks, listed in order of increasing difficulty:   \\n1.Classifying whether an image is overexposed (like the example above)   \\n2.Classifying whether an image was taken indoor or outdoor  \\n3.Classifying whether an image contains a cat  \\n4.Classifying whether an image contains a cat with both black and white fur  \\n5.Classifying whether an image contains a Siamese cat (a particular breed of cat)  \\n \\nEach of these is a binary image classification task: You have to input an image, and output  \\neither 0 or 1. But the tasks earlier in the list seem much “easier” for a neural network to  \\nlearn. You will be able to learn the easier tasks with fewer training examples.   \\nMachine learning does not yet have a good formal definition of what makes a task easy or  \\nhard.  With the rise of deep learning and multi-layered neural networks, we sometimes say a  16\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a  \\nshallow neural network), and “hard” if it requires more computation steps (requiring a  \\ndeeper neural network). But these are informal definitions.   \\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function  \\nis the length of the shortest computer program that can produce that function.'),\n",
       " Document(page_content=' say a  16\\ntask is “easy” if it can be carried out with fewer computation steps (corresponding to a  \\nshallow neural network), and “hard” if it requires more computation steps (requiring a  \\ndeeper neural network). But these are informal definitions.   \\n16Information theory has the concept of “Kolmogorov Complexity”, which says that the complexity of a learned function  \\nis the length of the shortest computer program that can produce that function. However, this theoretical concept has found  \\nfew practical applications in AI. See also: https://en.wikipedia.org/wiki/Kolmogorov_complexity  \\nPage 99 Machine Learning Yearning-Draft Andrew Ng  \\nIf you are able to take a complex task, and break it down into simpler sub-tasks, then by  \\ncoding in the steps of the sub-tasks explicitly, you are giving the algorithm prior knowledge  \\nthat can help it learn a task more efficiently.   \\n \\nSuppose you are building a Siamese cat detector. This is the pure end-to-end architecture:  \\n \\nIn contrast, you can alternatively use a pipeline with two steps:   \\n \\nThe first step (cat detector) detects all the cats in the image.   \\nPage 100 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThe second step then passes cropped images of each of the detected cats (one at a time) to a  \\ncat species classifier, and finally outputs 1 if any of the cats detected is a Siamese cat.  \\n \\nCompared to training a purely end-to-end classifier using just labels 0/1, each of the two  \\ncomponents in the pipeline--the cat detector and the cat breed classifier--seem much easier  \\nto learn and will require significantly less data.  17\\n \\n17 If you are familiar with practical object detection algorithms, you will recognize that they do not learn just with 0/1 \\nimage labels, but are instead trained with bounding boxes provided as part of the training data. A discussion of them is  \\nbeyond the scope of this chapter. See the Deep Learning specialization on Coursera (\\u200bhttp://deeplearning.ai\\u200b) if you would \\nlike to learn more about such algorithms.   \\nPage 101 Machine Learning Yearning-Draft Andrew Ng  \\nAs one final example, let’s revisit the autonomous driving pipeline.   \\n \\n \\n \\n \\n \\n \\n \\nBy using this pipeline, you are telling the algorithm that there are 3 key steps to driving: (1)  \\nDetect other cars, (2) Detect pedestrians, and (3) Plan a path for your car. Further, each of  \\nthese is a relatively simpler function--and can thus be learned with less data--than the  \\npurely end-to-end approach.  \\n \\nIn summary, when deciding what should be the components of a pipeline, try to build a  \\npipeline where each component is a relatively “simple” function that can therefore be learned  \\nfrom only a modest amount of data.   \\n \\n \\n \\n \\n  \\nPage 102 Machine Learning Yearning-Draft Andrew Ng \\n \\n52 Directly learning rich outputs  \\n \\nAn image classification algorithm will input an image \\u200bx\\u200b, and output an integer indicating the  \\nobject category. Can an algorithm instead output an entire sentence describing the image?   \\nFor example:  \\n \\n \\nx\\u200b =   \\n \\ny\\u200b = “A yellow bus driving down a road with  \\ngreen trees and green grass in the  \\nbackground.” \\nTraditional applications of supervised learning learned a function \\u200bh\\u200b:\\u200bX\\u200b→\\u200bY\\u200b, where the output  \\ny\\u200b was usually an integer or a real number. For example:   \\nProblem \\xa0 X\\xa0 Y\\xa0\\nSpam classification \\xa0 Email\\xa0\\xa0 Spam/Not spam (0/1) \\xa0\\nImage recognition \\xa0 Image\\xa0 Integer label \\xa0\\nHousing price prediction \\xa0Features of house\\xa0 Price in dollars \\xa0\\nProduct recommendation \\xa0Product & user features\\xa0 Chance of purchase \\xa0\\n \\nOne of the most exciting developments in end-to-end deep learning is that it is letting us  \\ndirectly learn \\u200by\\u200b that are much more complex than a number. In the image-captioning  \\nexample above, you can have a neural network input an image ( \\u200bx\\u200b) and directly output a  \\ncaption ( \\u200by\\u200b).  \\n \\n \\n \\n \\n \\nPage 103 Machine Learning Yearning-Draft Andrew Ng  \\nHere are more examples:  \\nProblem \\xa0 X\\xa0 Y\\xa0 Example Citation \\xa0\\nImage captioning \\xa0 Image\\xa0 Text\\xa0 Mao et al., 2014\\xa0\\nMachine translation \\xa0 English text \\xa0 French text\\xa0 Suskever et al., 2014\\xa0\\nQuestion answering \\xa0 (Text,Question) pair \\xa0\\xa0 Answer text \\xa0 Bordes et al., 2015 \\xa0\\nSpeech recognition \\xa0 Audio\\xa0 Transcription \\xa0 Hannun et al., 2015 \\xa0\\nTTS\\xa0 Text features \\xa0 Audio\\xa0 van der Oord et al., 2016\\xa0\\n \\nThis is'),\n",
       " Document(page_content='\\nImage captioning \\xa0 Image\\xa0 Text\\xa0 Mao et al., 2014\\xa0\\nMachine translation \\xa0 English text \\xa0 French text\\xa0 Suskever et al., 2014\\xa0\\nQuestion answering \\xa0 (Text,Question) pair \\xa0\\xa0 Answer text \\xa0 Bordes et al., 2015 \\xa0\\nSpeech recognition \\xa0 Audio\\xa0 Transcription \\xa0 Hannun et al., 2015 \\xa0\\nTTS\\xa0 Text features \\xa0 Audio\\xa0 van der Oord et al., 2016\\xa0\\n \\nThis is an accelerating trend in deep learning: When you have the right (input,output)  \\nlabeled pairs, you can sometimes learn end-to-end even when the output is a sentence, an  \\nimage, audio, or other outputs that are richer than a single number.   \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0  \\nPage 104 Machine Learning Yearning-Draft Andrew Ng  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n\\xa0\\nError analysis \\nby parts \\n\\xa0\\n\\xa0\\n\\xa0\\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 105 Machine Learning Yearning-Draft Andrew Ng  \\n53 Error analysis by parts   \\n \\nSuppose your system is built using a complex machine learning pipeline, and you would like  \\nto improve the system’s performance. Which part of the pipeline should you work on  \\nimproving? By attributing errors to specific parts of the pipeline, you can decide how to  \\nprioritize your work.   \\n \\nLet’s use our Siamese cat classifier example:   \\n \\n \\nThe first part, the cat detector, detects cats and crops them out of the image. The second  \\npart, the cat breed classifier, decides if it is a Siamese cat. It is possible to spend years  \\nworking on improving either of these two pipeline components. How do you decide which  \\ncomponent(s) to focus on?   \\nBy carrying out \\u200berror analysis by parts \\u200b, you can try to attribute each mistake the  \\nalgorithm makes to one (or sometimes both) of the two parts of the pipeline. For example,  \\nthe algorithm misclassifies this image as not containing a Siamese cat (y=0) even though the  \\ncorrect label is y=1.   \\n \\nLet’s manually examine what the two steps of the algorithm did. Suppose the Siamese cat  \\ndetector had detected a cat as follows:   \\nPage 106 Machine Learning Yearning-Draft Andrew Ng  \\n \\nThis means that the cat breed classifier is given the following image:   \\n \\n The cat breed classifier then correctly classifies this image as not containing a Siamese cat.  \\nThus, the cat breed classifier is blameless: It was given of a pile of rocks and outputted a very  \\nreasonable label y=0. Indeed, a human classifying the cropped image above would also have  \\npredicted y=0. Thus, you can clearly attribute this error to the cat detector.   \\nIf, on the other hand, the cat detector had outputted the following bounding box:  \\n \\nthen you would conclude that the cat detector had done its job, and that it was the cat breed  \\nclassifier that is at fault.   \\n \\nSay you go through 100 misclassified dev set images and find that 90 of the errors are  \\nattributable to the cat detector, and only 10 errors are attributable to the cat breed classifier.  \\nYou can safely conclude that you should focus more attention on improving the cat detector.   \\nPage 107 Machine Learning Yearning-Draft Andrew Ng  \\n \\nFurther, you have now also conveniently found 90 examples where the cat detector  \\noutputted incorrect bounding boxes. You can use these 90 examples to carry out a deeper  \\nlevel of error analysis on the cat detector to see how to improve that.   \\n \\nOur description of how you attribute error to one part of the pipeline has been informal so  \\nfar: you look at the output of each of the parts and see if you can decide which one made a  \\nmistake. This informal method could be all you need. But in the next chapter, you’ll also see  \\na more formal way of attributing error.   \\n \\n \\n  \\nPage 108 Machine Learning Yearning-Draft Andrew Ng  \\n54 Attributing error to one part   \\n \\nLet’s continue to use this example:  \\n  \\nSuppose the cat detector outputted this bounding box:   \\n \\n \\n \\nThe cat breed classifier is thus given this cropped image, whereupon it incorrectly outputs  \\ny=0, or that there is no cat in the picture.   \\n \\nThe cat detector did its job poorly. However, a highly skilled human could arguably still  \\nrecognize the Siamese cat from the poorly cropped image. So do we attribute this error to the  \\ncat detector, or the cat breed classifier, or both? It is ambiguous.  \\n \\nIf the number of ambiguous cases like these is small, you can make whatever decision you  \\nwant and get a similar result. But here is a more formal test that lets you more definitively  \\nattribute the error to exactly'),\n",
       " Document(page_content=' detector did its job poorly. However, a highly skilled human could arguably still  \\nrecognize the Siamese cat from the poorly cropped image. So do we attribute this error to the  \\ncat detector, or the cat breed classifier, or both? It is ambiguous.  \\n \\nIf the number of ambiguous cases like these is small, you can make whatever decision you  \\nwant and get a similar result. But here is a more formal test that lets you more definitively  \\nattribute the error to exactly one part:  \\n \\n1.Replace the cat detector output with a hand-labeled bounding box.  \\nPage 109 Machine Learning Yearning-Draft Andrew Ng  \\n \\n2.Run the corresponding cropped image through the cat breed classifier. If the cat breed  \\nclassifier still misclassifies it, attribute the error to the cat breed classifier. Otherwise,  \\nattribute the error to the cat detector.   \\n \\nIn other words, run an experiment in which you give the cat breed classifier a “perfect” input.  \\nThere are two cases:  \\n \\n●Case 1: Even given a “perfect” bounding box, the cat breed classifier still incorrectly  \\noutputs y=0. In this case, clearly the cat breed classifier is at fault.  \\n●Case 2: Given a “perfect” bounding box, the breed classifier now correctly outputs  \\ny=1. This shows that if only the cat detector had given a more perfect bounding box,  \\nthen the overall system’s output would have been correct. Thus, attribute the error to  \\nthe cat detector.   \\n \\nBy carrying out this analysis on the misclassified dev set images, you can now  \\nunambiguously attribute each error to one component. This allows you to estimate the  \\nfraction of errors due to each component of the pipeline, and therefore decide where to focus  \\nyour attention.  \\n \\n \\n \\n \\n  \\nPage 110 Machine Learning Yearning-Draft Andrew Ng  \\n55 General case of error attribution   \\n \\nHere are the general steps for error attribution. Suppose the pipeline has three steps A, B  \\nand C, where A feeds directly into B, and B feeds directly into C.   \\n \\n \\n \\nFor each mistake the system makes on the dev set:  \\n \\n1.Try manually modifying A’s output to be a “perfect” output (e.g.,  the “perfect”  \\nbounding box for the cat), and run the rest of the pipeline B, C on this output. If the  \\nalgorithm now gives a correct output, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been correct; thus, you can  \\nattribute this error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B’s output to be the “perfect” output for B. If the algorithm  \\nnow gives a correct output, then attribute the error to component B. Otherwise, go on  \\nto Step 3. \\n3.Attribute the error to component C.   \\n \\nLet’s look at a more complex example:  \\nYour self-driving car uses this pipeline. How do you use error analysis by parts to decide  \\nwhich component(s) to focus on?   \\n \\nYou can map the three components to A, B, C as follows:  \\nA: Detect cars  \\nB: Detect pedestrians  \\nC: Plan path for car  \\n \\n \\nPage 111 Machine Learning Yearning-Draft Andrew Ng \\n \\nFollowing the procedure described above, suppose you test out your car on a closed track  \\nand find a case where the car chooses a more jarring steering direction than a skilled driver  \\nwould. In the self-driving world, such a case is usually called a \\u200bscenario \\u200b. You would then:  \\n  \\n1.Try manually modifying A (detecting cars)’s output to be a “perfect” output (e.g.,  \\nmanually go in and tell it where the other cars are). Run the rest of the pipeline B, C as  \\nbefore, but allow C (plan path) to use A’s now perfect output. If the algorithm now  \\nplans a much better path for the car, then this shows that, if only A had given a better  \\noutput, the overall algorithm’s output would have been better; Thus, you can attribute  \\nthis error to component A. Otherwise, go on to Step 2.   \\n2.Try manually modifying B (detect pedestrian)’s output to be the “perfect” output for  \\nB. If the algorithm now gives a correct output, then attribute the error to component  \\nB. Otherwise, go on to Step 3.   \\n3.Attribute the error to component C.   \\n \\nThe components of an ML pipeline should be ordered according to a Directed Acyclic Graph  \\n(DAG), meaning that you should be able to compute them in some fixed left-to-right order,  \\nand later components should depend only on earlier components’ outputs. So long as the  \\nmapping of the components to the A->B->'),\n",
       " Document(page_content=' now gives a correct output, then attribute the error to component  \\nB. Otherwise, go on to Step 3.   \\n3.Attribute the error to component C.   \\n \\nThe components of an ML pipeline should be ordered according to a Directed Acyclic Graph  \\n(DAG), meaning that you should be able to compute them in some fixed left-to-right order,  \\nand later components should depend only on earlier components’ outputs. So long as the  \\nmapping of the components to the A->B->C order follows the DAG ordering, then the error  \\nanalysis will be fine. You might get slightly different results if you swap A and B:  \\n \\nA: Detect pedestrians (was previously \\u200bDetect cars \\u200b)  \\nB: Detect cars (was previously \\u200bDetect pedestrians \\u200b) \\nC: Plan path for car  \\n \\nBut the results of this analysis would still be valid and give good guidance for where to focus  \\nyour attention.  \\n \\n \\n  \\nPage 112 Machine Learning Yearning-Draft Andrew Ng  \\n56 Error analysis by parts and comparison to  \\nhuman-level performance   \\n \\nCarrying out error analysis on a learning algorithm is like using data science to analyze an  \\nML system’s mistakes in order to derive insights about what to do next. At its most basic,  \\nerror analysis by parts tells us what component(s) performance is (are) worth the greatest  \\neffort to improve.  \\n \\nSay you have a dataset about customers buying things on a website. A data scientist may  \\nhave many different ways of analyzing the data. She may draw many different conclusions  \\nabout whether the website should raise prices, about the lifetime value of customers acquired  \\nthrough different marketing campaigns, and so on. There is no one “right” way to analyze a  \\ndataset, and there are many possible useful insights one could draw. Similarly, there is no  \\none “right” way to carry out error analysis. Through these chapters you have learned many of  \\nthe most common design patterns for drawing useful insights about your ML system, but you  \\nshould feel free to experiment with other ways of analyzing errors as well.  \\n \\nLet’s return to the self-driving application, where a car detection algorithm outputs the  \\nlocation (and perhaps velocity) of the nearby cars, a pedestrian detection algorithm outputs  \\nthe location of the nearby pedestrians, and these two outputs are finally used to plan a path  \\nfor the car.  \\n \\nTo debug this pipeline, rather than rigorously following the procedure you saw in the  \\nprevious chapter, you could more informally ask:  \\n \\n1.How far is the Detect cars component from human-level performance at detecting  \\ncars?  \\n2.How far is the Detect pedestrians component from human-level performance?  \\nPage 113 Machine Learning Yearning-Draft Andrew Ng \\n \\n3.How far is the overall system’s performance from human-level performance? Here,  \\nhuman-level performance assumes the human has to plan a path for the car given  \\nonly the outputs from the previous two pipeline components (rather than access to  \\nthe camera images). In other words, how does the Plan path component’s  \\nperformance compare to that of a human’s, when the human is given only the same  \\ninput?   \\n \\nIf you find that one of the components is far from human-level performance, you now have a  \\ngood case to focus on improving the performance of that component.   \\n \\nMany error analysis processes work best when we are trying to automate something humans  \\ncan do and can thus benchmark against human-level performance. Most of our preceding  \\nexamples had this implicit assumption. If you are building an ML system where the final  \\noutput or some of the intermediate components are doing things that even humans cannot  \\ndo well, then some of these procedures will not apply.  \\n \\nThis is another advantage of working on problems that humans can solve--you have more  \\npowerful error analysis tools, and thus you can prioritize your team’s work more efficiently.   \\n \\n \\n \\n  \\nPage 114 Machine Learning Yearning-Draft Andrew Ng  \\n57 Spotting a flawed ML pipeline   \\n \\nWhat if each individual component of your ML pipeline is performing at human-level  \\nperformance or near-human-level performance, but the overall pipeline falls far short of  \\nhuman-level? This usually means that the pipeline is flawed and needs to be redesigned.  \\nError analysis can also help you understand if you need to redesign your pipeline.   \\n \\n \\nIn the previous chapter, we posed the question of whether each of the three components’  \\nperformance is at human level. Suppose the answer to all three questions is yes. That is:  \\n \\n1.The Detect cars component is at (roughly) human-level performance for detecting  \\ncars from the camera images.  \\n2.The Detect pedestrians component is at (roughly) human-level performance for  \\ndetecting cars from the camera images.   \\n3.Compared to a human that has to plan a path for the car given only the outputs  \\nfrom the previous two pipeline components (rather than access to the camera  \\n'),\n",
       " Document(page_content=' level. Suppose the answer to all three questions is yes. That is:  \\n \\n1.The Detect cars component is at (roughly) human-level performance for detecting  \\ncars from the camera images.  \\n2.The Detect pedestrians component is at (roughly) human-level performance for  \\ndetecting cars from the camera images.   \\n3.Compared to a human that has to plan a path for the car given only the outputs  \\nfrom the previous two pipeline components (rather than access to the camera  \\nimages), \\u200b the Plan path component’s performance is at a similar level.   \\n \\nHowever, your overall self-driving car is performing significantly below human-level  \\nperformance. I.e., humans given access to the camera images can plan significantly better  \\npaths for the car. What conclusion can you draw?   \\n \\nThe only possible conclusion is that the ML pipeline is flawed. In this case, the Plan path  \\ncomponent is doing as well as it can \\u200bgiven its inputs \\u200b, but the inputs do not contain enough  \\ninformation. You should ask yourself what other information, other than the outputs from  \\nthe two earlier pipeline components, is needed to plan paths very well for a car to drive. In  \\nother words, what other information does a skilled human driver need?   \\n \\nPage 115 Machine Learning Yearning-Draft Andrew Ng  \\nFor example, suppose you realize that a human driver also needs to know the location of the  \\nlane markings. This suggests that you should redesign the pipeline as follows : 18\\n \\n \\n \\n \\nUltimately, if you don’t think your pipeline as a whole will achieve human-level performance,  \\neven if every individual component has human-level performance (remember that you are  \\ncomparing to a human who is given the same input as the component), then the pipeline is  \\nflawed and should be redesigned.  \\n  \\n18 In the self-driv ing example abov e, in theory one could solv e this problem by also feeding the raw camera  \\nimage into the planning component.  Howev er, this would v iolate the design principle of “Task  simplicity”  \\ndescribed in Chapter 51,  because the path planning module now needs to input a raw image and has a v ery \\ncomplex task  to solv e. That’ s why adding a Detect lane markings component  is a better choice--it helps get the \\nimportant and prev iously missing information about lane markings to the path planning module,  but you av oid \\nmak ing any particular module ov erly complex to build/train.   \\n \\nPage 116 Machine Learning Yearning-Draft Andrew Ng  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xa0\\nConclusion \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 \\xa0\\nPage 117 Machine Learning Yearning-Draft Andrew Ng  \\n58 Building a superhero team - Get your  \\nteammates to read this  \\n \\nCongratulations on finishing this book!   \\nIn Chapter 2, we talked about how this book can help you become the superhero of your  \\nteam.  \\n \\nThe only thing better than being a superhero is being part of a superhero team. I hope you’ll  \\ngive copies of this book to your friends and teammates and help create other superheroes!  \\n \\n  \\nPage 118 Machine Learning Yearning-Draft Andrew Ng ')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_answer_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
